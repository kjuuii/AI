# -*- coding: utf-8 -*-
"""
ç‰¹å¾æå–æ¨¡å— - Streamlitç•Œé¢
æä¾›æ•°æ®ä¸Šä¼ ã€ç‰¹å¾æå–ã€å¯è§†åŒ–å’Œå¯¼å‡ºåŠŸèƒ½
"""

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import os
import io
import base64

# å°è¯•å¯¼å…¥å­—ä½“è®¾ç½®
try:
    from font_utils import FONT_PROP, apply_plot_style

    FONT_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: æ— æ³•ä» font_utils å¯¼å…¥ï¼Œå°†ä½¿ç”¨å¤‡ç”¨ç»˜å›¾è®¾ç½®ã€‚")
    FONT_AVAILABLE = False
    FONT_PROP = None


    def apply_plot_style(ax):
        return ax

# å¯¼å…¥ç‰¹å¾æå–å™¨
try:
    from feature_extractor import (
        process_data,
        get_feature_names,
        save_features_to_file,
        export_features_for_analysis,
        create_analysis_script,
        extract_features_from_segment,
        SAMPLE_RATE
    )

    EXTRACTOR_LOADED = True
except ImportError as e:
    st.error(f"æ— æ³•å¯¼å…¥ feature_extractor æ¨¡å—: {e}")
    EXTRACTOR_LOADED = False


def show_feature_extraction_page():
    """æ˜¾ç¤ºç‰¹å¾æå–é¡µé¢çš„ä¸»å‡½æ•°"""
    st.title("ğŸ” ç‰¹å¾æå–")

    if not EXTRACTOR_LOADED:
        st.error("ç‰¹å¾æå–å™¨æ¨¡å—æœªèƒ½æ­£ç¡®åŠ è½½ã€‚è¯·ç¡®ä¿ feature_extractor.py æ–‡ä»¶å­˜åœ¨ã€‚")
        return

    # ä½¿ç”¨æ ‡ç­¾é¡µç»„ç»‡ä¸åŒåŠŸèƒ½
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ“ æ•°æ®ä¸Šä¼ ", "âš™ï¸ ç‰¹å¾æå–", "ğŸ“Š ç‰¹å¾åˆ†æ", "ğŸ’¾ å¯¼å‡ºç»“æœ"])

    with tab1:
        show_data_upload_section()

    with tab2:
        show_feature_extraction_section()

    with tab3:
        show_feature_analysis_section()

    with tab4:
        show_export_section()


def show_data_upload_section():
    """æ•°æ®ä¸Šä¼ éƒ¨åˆ†"""
    st.header("ğŸ“ æ•°æ®ä¸Šä¼ ")

    # æ•°æ®æºé€‰æ‹©
    data_source = st.radio(
        "é€‰æ‹©æ•°æ®æºï¼š",
        ["ä¸Šä¼ æ–‡ä»¶", "ä½¿ç”¨ç¤ºä¾‹æ•°æ®", "ä»å…¶ä»–æ¨¡å—åŠ è½½"],
        key="fe_data_source"
    )

    if data_source == "ä¸Šä¼ æ–‡ä»¶":
        uploaded_file = st.file_uploader(
            "ä¸Šä¼ æ•°æ®æ–‡ä»¶",
            type=['csv', 'xlsx', 'xls'],
            help="æ”¯æŒCSVå’ŒExcelæ ¼å¼ï¼Œéœ€åŒ…å«ç”µå‹å’Œç”µæµæ•°æ®åˆ—",
            key="fe_file_upload"
        )

        if uploaded_file is not None:
            # è¯»å–æ–‡ä»¶
            try:
                if uploaded_file.name.endswith('.csv'):
                    df = pd.read_csv(uploaded_file)
                else:
                    df = pd.read_excel(uploaded_file)

                st.success(f"âœ… æˆåŠŸåŠ è½½æ–‡ä»¶: {uploaded_file.name}")
                st.write(f"æ•°æ®å½¢çŠ¶: {df.shape}")

                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                st.subheader("æ•°æ®é¢„è§ˆ")
                st.dataframe(df.head())

                # ä¿å­˜åˆ°session state
                st.session_state['fe_data'] = df
                st.session_state['fe_filename'] = uploaded_file.name

                # åˆ—é€‰æ‹©
                show_column_selection(df)

            except Exception as e:
                st.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")

    elif data_source == "ä½¿ç”¨ç¤ºä¾‹æ•°æ®":
        if st.button("ç”Ÿæˆç¤ºä¾‹æ•°æ®", key="fe_generate_example"):
            df = generate_example_data()
            st.session_state['fe_data'] = df
            st.session_state['fe_filename'] = "example_data.csv"

            st.success("âœ… å·²ç”Ÿæˆç¤ºä¾‹æ•°æ®")
            st.dataframe(df.head())

            # è‡ªåŠ¨è®¾ç½®åˆ—å
            st.session_state['fe_voltage_col'] = 'Voltage'
            st.session_state['fe_current_col'] = 'Current'

    else:  # ä»å…¶ä»–æ¨¡å—åŠ è½½
        st.info("å¯ä»¥ä»åˆ†ç±»ã€å›å½’æˆ–èšç±»æ¨¡å—åŠ è½½å·²å¤„ç†çš„æ•°æ®")

        # æ£€æŸ¥å…¶ä»–æ¨¡å—çš„æ•°æ®
        available_data = []
        if 'classification_data' in st.session_state and st.session_state.classification_data is not None:
            available_data.append("åˆ†ç±»æ¨¡å—æ•°æ®")
        if 'regression_data' in st.session_state and st.session_state.regression_data is not None:
            available_data.append("å›å½’æ¨¡å—æ•°æ®")
        if 'clustering_data' in st.session_state and st.session_state.clustering_data is not None:
            available_data.append("èšç±»æ¨¡å—æ•°æ®")

        if available_data:
            selected_data = st.selectbox("é€‰æ‹©è¦åŠ è½½çš„æ•°æ®ï¼š", available_data)

            if st.button("åŠ è½½æ•°æ®", key="fe_load_from_module"):
                if selected_data == "åˆ†ç±»æ¨¡å—æ•°æ®":
                    df = st.session_state.classification_data
                elif selected_data == "å›å½’æ¨¡å—æ•°æ®":
                    df = st.session_state.regression_data
                else:
                    df = st.session_state.clustering_data

                st.session_state['fe_data'] = df
                st.session_state['fe_filename'] = f"{selected_data}.csv"

                st.success(f"âœ… å·²åŠ è½½{selected_data}")
                st.dataframe(df.head())

                show_column_selection(df)
        else:
            st.warning("æš‚æ— å¯ç”¨çš„æ¨¡å—æ•°æ®")


def show_column_selection(df):
    """æ˜¾ç¤ºåˆ—é€‰æ‹©ç•Œé¢"""
    st.subheader("åˆ—é€‰æ‹©")

    col1, col2 = st.columns(2)

    with col1:
        voltage_col = st.selectbox(
            "é€‰æ‹©ç”µå‹åˆ—ï¼š",
            df.columns,
            key="fe_voltage_col_select"
        )
        st.session_state['fe_voltage_col'] = voltage_col

    with col2:
        current_col = st.selectbox(
            "é€‰æ‹©ç”µæµåˆ—ï¼š",
            df.columns,
            key="fe_current_col_select"
        )
        st.session_state['fe_current_col'] = current_col


def generate_example_data():
    """ç”Ÿæˆç¤ºä¾‹æ•°æ®"""
    # åˆ›å»º3ç§’çš„æ¨¡æ‹Ÿæ•°æ®
    time = np.linspace(0, 3, 3 * SAMPLE_RATE)

    # æ¨¡æ‹Ÿå¯åŠ¨ç”µæµä¿¡å·
    current_signal = (1 - np.exp(-time * 3)) * 1.5  # æŒ‡æ•°ä¸Šå‡
    current_signal += 0.1 * np.sin(2 * np.pi * 5 * time)  # æ·»åŠ 5HzæŒ¯è¡
    current_signal += np.random.randn(len(time)) * 0.05  # æ·»åŠ å™ªå£°

    # æ¨¡æ‹Ÿç”µå‹ä¿¡å·
    voltage_signal = np.full_like(time, 220)  # åŸºå‡†ç”µå‹220V
    voltage_signal += 2 * np.sin(2 * np.pi * 0.5 * time)  # è½»å¾®æ³¢åŠ¨
    voltage_signal += np.random.randn(len(time)) * 0.5  # æ·»åŠ å™ªå£°

    return pd.DataFrame({
        'Time': time,
        'Voltage': voltage_signal,
        'Current': current_signal
    })


def show_feature_extraction_section():
    """ç‰¹å¾æå–éƒ¨åˆ†"""
    st.header("âš™ï¸ ç‰¹å¾æå–")

    if 'fe_data' not in st.session_state:
        st.warning("è¯·å…ˆä¸Šä¼ æ•°æ®")
        return

    df = st.session_state['fe_data']

    # æå–å‚æ•°è®¾ç½®
    st.subheader("æå–å‚æ•°")

    col1, col2, col3 = st.columns(3)

    with col1:
        window_size = st.number_input(
            "çª—å£å¤§å°ï¼ˆæ ·æœ¬æ•°ï¼‰",
            min_value=100,
            max_value=10000,
            value=1875,
            step=100,
            help="æ•°æ®çª—å£çš„å¤§å°ï¼Œé»˜è®¤ä¸º3ç§’çš„æ•°æ®ï¼ˆ625Hz * 3ï¼‰"
        )

    with col2:
        current_threshold = st.number_input(
            "ç”µæµé˜ˆå€¼",
            min_value=0.0,
            max_value=5.0,
            value=0.5,
            step=0.1,
            help="ç”¨äºæ£€æµ‹ä¿¡å·èµ·å§‹ç‚¹çš„ç”µæµé˜ˆå€¼"
        )

    with col3:
        batch_process = st.checkbox(
            "æ‰¹é‡å¤„ç†",
            help="å¦‚æœæ•°æ®åŒ…å«å¤šä¸ªæ ·æœ¬ï¼Œå¯ä»¥æ‰¹é‡æå–ç‰¹å¾"
        )

    # æå–æŒ‰é’®
    if st.button("ğŸš€ å¼€å§‹æå–ç‰¹å¾", key="fe_extract_button", type="primary"):
        try:
            with st.spinner("æ­£åœ¨æå–ç‰¹å¾..."):
                # è·å–åˆ—å
                voltage_col = st.session_state.get('fe_voltage_col', 'Voltage')
                current_col = st.session_state.get('fe_current_col', 'Current')

                # æå–ç‰¹å¾
                features_df = process_data(
                    df,
                    voltage_col=voltage_col,
                    current_col=current_col,
                    window_size=int(window_size),
                    current_threshold=current_threshold
                )

                # ä¿å­˜ç»“æœ
                st.session_state['fe_features'] = features_df
                st.session_state['fe_feature_names'] = get_feature_names()

                st.success(f"âœ… ç‰¹å¾æå–å®Œæˆï¼æå–äº† {len(features_df.columns)} ä¸ªç‰¹å¾")

                # æ˜¾ç¤ºç‰¹å¾é¢„è§ˆ
                st.subheader("ç‰¹å¾é¢„è§ˆ")
                st.dataframe(features_df)

                # æ˜¾ç¤ºç‰¹å¾ç»Ÿè®¡
                with st.expander("ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯", expanded=True):
                    st.dataframe(features_df.describe())

        except Exception as e:
            st.error(f"ç‰¹å¾æå–å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())


def show_feature_analysis_section():
    """ç‰¹å¾åˆ†æéƒ¨åˆ†"""
    st.header("ğŸ“Š ç‰¹å¾åˆ†æ")

    if 'fe_features' not in st.session_state:
        st.warning("è¯·å…ˆæå–ç‰¹å¾")
        return

    features_df = st.session_state['fe_features']
    feature_names = st.session_state['fe_feature_names']

    # åˆ†æé€‰é¡¹
    analysis_type = st.selectbox(
        "é€‰æ‹©åˆ†æç±»å‹ï¼š",
        ["ç‰¹å¾åˆ†å¸ƒ", "ç‰¹å¾ç›¸å…³æ€§", "ç‰¹å¾é‡è¦æ€§ä¼°è®¡", "ç‰¹å¾è¯¦æƒ…"]
    )

    if analysis_type == "ç‰¹å¾åˆ†å¸ƒ":
        show_feature_distribution(features_df)

    elif analysis_type == "ç‰¹å¾ç›¸å…³æ€§":
        show_feature_correlation(features_df)

    elif analysis_type == "ç‰¹å¾é‡è¦æ€§ä¼°è®¡":
        show_feature_importance(features_df)

    else:  # ç‰¹å¾è¯¦æƒ…
        show_feature_details(features_df, feature_names)


def show_feature_distribution(features_df):
    """æ˜¾ç¤ºç‰¹å¾åˆ†å¸ƒ"""
    st.subheader("ç‰¹å¾åˆ†å¸ƒå›¾")

    # é€‰æ‹©è¦æ˜¾ç¤ºçš„ç‰¹å¾
    selected_features = st.multiselect(
        "é€‰æ‹©è¦æ˜¾ç¤ºçš„ç‰¹å¾ï¼š",
        features_df.columns,
        default=list(features_df.columns[:6])  # é»˜è®¤æ˜¾ç¤ºå‰6ä¸ª
    )

    if selected_features:
        # è®¡ç®—å­å›¾å¸ƒå±€
        n_features = len(selected_features)
        n_cols = min(3, n_features)
        n_rows = (n_features + n_cols - 1) // n_cols

        fig, axes = plt.subplots(n_rows, n_cols, figsize=(5 * n_cols, 4 * n_rows))
        if n_features == 1:
            axes = [axes]
        elif n_rows == 1:
            axes = axes.reshape(1, -1)
        elif n_cols == 1:
            axes = axes.reshape(-1, 1)

        for idx, feature in enumerate(selected_features):
            row = idx // n_cols
            col = idx % n_cols
            ax = axes[row, col] if n_rows > 1 else axes[col]

            # ç»˜åˆ¶ç›´æ–¹å›¾
            ax.hist(features_df[feature], bins=20, alpha=0.7, color='steelblue', edgecolor='black')
            ax.set_title(feature, fontsize=10)
            ax.set_xlabel('Value')
            ax.set_ylabel('Frequency')
            ax.grid(True, alpha=0.3)

            if FONT_AVAILABLE:
                apply_plot_style(ax)

        # éšè—å¤šä½™çš„å­å›¾
        for idx in range(n_features, n_rows * n_cols):
            row = idx // n_cols
            col = idx % n_cols
            axes[row, col].set_visible(False)

        plt.tight_layout()
        st.pyplot(fig)
        plt.close()


def show_feature_correlation(features_df):
    """æ˜¾ç¤ºç‰¹å¾ç›¸å…³æ€§"""
    st.subheader("ç‰¹å¾ç›¸å…³æ€§çŸ©é˜µ")

    # è®¡ç®—ç›¸å…³æ€§çŸ©é˜µ
    corr_matrix = features_df.corr()

    # ç»˜åˆ¶çƒ­åŠ›å›¾
    fig, ax = plt.subplots(figsize=(12, 10))

    # ä½¿ç”¨maskåªæ˜¾ç¤ºä¸‹ä¸‰è§’
    mask = np.triu(np.ones_like(corr_matrix), k=1)

    sns.heatmap(
        corr_matrix,
        mask=mask,
        annot=True,
        fmt='.2f',
        cmap='coolwarm',
        center=0,
        square=True,
        linewidths=0.5,
        cbar_kws={"shrink": 0.8},
        ax=ax
    )

    ax.set_title('Feature Correlation Matrix', fontsize=16, pad=20)

    if FONT_AVAILABLE:
        apply_plot_style(ax)

    plt.tight_layout()
    st.pyplot(fig)
    plt.close()

    # æ˜¾ç¤ºé«˜ç›¸å…³æ€§ç‰¹å¾å¯¹
    st.subheader("é«˜ç›¸å…³æ€§ç‰¹å¾å¯¹")

    # è·å–ç›¸å…³æ€§é˜ˆå€¼
    threshold = st.slider("ç›¸å…³æ€§é˜ˆå€¼ï¼š", 0.5, 1.0, 0.8, 0.05)

    # æ‰¾å‡ºé«˜ç›¸å…³æ€§çš„ç‰¹å¾å¯¹
    high_corr_pairs = []
    for i in range(len(corr_matrix.columns)):
        for j in range(i + 1, len(corr_matrix.columns)):
            if abs(corr_matrix.iloc[i, j]) >= threshold:
                high_corr_pairs.append({
                    'Feature 1': corr_matrix.columns[i],
                    'Feature 2': corr_matrix.columns[j],
                    'Correlation': corr_matrix.iloc[i, j]
                })

    if high_corr_pairs:
        high_corr_df = pd.DataFrame(high_corr_pairs)
        high_corr_df = high_corr_df.sort_values('Correlation', ascending=False)
        st.dataframe(high_corr_df)
    else:
        st.info(f"æ²¡æœ‰å‘ç°ç›¸å…³æ€§é«˜äº {threshold} çš„ç‰¹å¾å¯¹")


def show_feature_importance(features_df):
    """æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§ä¼°è®¡"""
    st.subheader("ç‰¹å¾é‡è¦æ€§ä¼°è®¡")

    st.info("è¿™æ˜¯åŸºäºç‰¹å¾æ–¹å·®çš„ç®€å•é‡è¦æ€§ä¼°è®¡ã€‚å¯¹äºæ›´å‡†ç¡®çš„é‡è¦æ€§è¯„ä¼°ï¼Œè¯·åœ¨å…·ä½“çš„æœºå™¨å­¦ä¹ ä»»åŠ¡ä¸­ä½¿ç”¨ã€‚")

    # è®¡ç®—å„ç§ç»Ÿè®¡é‡ä½œä¸ºé‡è¦æ€§æŒ‡æ ‡
    importance_metrics = pd.DataFrame({
        'Feature': features_df.columns,
        'Variance': features_df.var(),
        'Coefficient of Variation': features_df.std() / (features_df.mean() + 1e-10),
        'Range': features_df.max() - features_df.min(),
        'IQR': features_df.quantile(0.75) - features_df.quantile(0.25)
    })

    # æ ‡å‡†åŒ–å„æŒ‡æ ‡
    for col in ['Variance', 'Coefficient of Variation', 'Range', 'IQR']:
        importance_metrics[f'{col}_normalized'] = (
                importance_metrics[col] / importance_metrics[col].max()
        )

    # è®¡ç®—ç»¼åˆé‡è¦æ€§åˆ†æ•°
    importance_metrics['Importance Score'] = (
            importance_metrics['Variance_normalized'] * 0.3 +
            importance_metrics['Coefficient of Variation_normalized'] * 0.3 +
            importance_metrics['Range_normalized'] * 0.2 +
            importance_metrics['IQR_normalized'] * 0.2
    )

    # æ’åº
    importance_metrics = importance_metrics.sort_values('Importance Score', ascending=False)

    # å¯è§†åŒ–
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # æ¡å½¢å›¾
    top_n = min(15, len(importance_metrics))
    ax1.barh(
        importance_metrics['Feature'].head(top_n),
        importance_metrics['Importance Score'].head(top_n),
        color='steelblue'
    )
    ax1.set_xlabel('Importance Score')
    ax1.set_title(f'Top {top_n} Important Features')
    ax1.grid(True, alpha=0.3)

    # å„æŒ‡æ ‡å¯¹æ¯”
    metrics_to_plot = ['Variance_normalized', 'Coefficient of Variation_normalized',
                       'Range_normalized', 'IQR_normalized']
    importance_metrics[metrics_to_plot].head(top_n).plot(
        kind='bar',
        ax=ax2,
        width=0.8
    )
    ax2.set_xticklabels(importance_metrics['Feature'].head(top_n), rotation=45, ha='right')
    ax2.set_ylabel('Normalized Score')
    ax2.set_title('Feature Importance by Different Metrics')
    ax2.legend(['Variance', 'CV', 'Range', 'IQR'], loc='upper right')
    ax2.grid(True, alpha=0.3)

    if FONT_AVAILABLE:
        apply_plot_style(ax1)
        apply_plot_style(ax2)

    plt.tight_layout()
    st.pyplot(fig)
    plt.close()

    # æ˜¾ç¤ºè¯¦ç»†æ•°æ®
    with st.expander("æŸ¥çœ‹è¯¦ç»†é‡è¦æ€§æŒ‡æ ‡"):
        display_df = importance_metrics[['Feature', 'Variance', 'Coefficient of Variation',
                                         'Range', 'IQR', 'Importance Score']]
        st.dataframe(display_df)


def show_feature_details(features_df, feature_names):
    """æ˜¾ç¤ºç‰¹å¾è¯¦ç»†è¯´æ˜"""
    st.subheader("ç‰¹å¾è¯¦ç»†è¯´æ˜")

    # ç‰¹å¾æè¿°
    feature_descriptions = {
        'startup_peak': "å¯åŠ¨å³°å€¼ç”µæµ - è®¾å¤‡å¯åŠ¨æ—¶çš„æœ€å¤§ç”µæµå€¼",
        'startup_peak_time': "å¯åŠ¨å³°å€¼æ—¶é—´ - è¾¾åˆ°å¯åŠ¨å³°å€¼çš„æ—¶é—´ï¼ˆç§’ï¼‰",
        'startup_acceleration': "å¯åŠ¨åŠ é€Ÿåº¦ - å¯åŠ¨é˜¶æ®µç”µæµå˜åŒ–çš„åŠ é€Ÿåº¦",
        'startup_slope': "å¯åŠ¨æ–œç‡ - å¯åŠ¨é˜¶æ®µç”µæµä¸Šå‡çš„æ–œç‡",
        'steady_mean': "ç¨³æ€å¹³å‡å€¼ - ç¨³å®šè¿è¡Œæ—¶çš„å¹³å‡ç”µæµ",
        'steady_std': "ç¨³æ€æ ‡å‡†å·® - ç¨³å®šè¿è¡Œæ—¶ç”µæµçš„æ ‡å‡†å·®",
        'steady_amplitude': "ç¨³æ€å¹…åº¦ - ç¨³å®šè¿è¡Œæ—¶ç”µæµçš„æœ€å¤§æœ€å°å€¼ä¹‹å·®",
        'steady_iqr': "ç¨³æ€å››åˆ†ä½è· - ç¨³å®šè¿è¡Œæ—¶ç”µæµçš„å››åˆ†ä½è·",
        'steady_skew': "ç¨³æ€ååº¦ - ç¨³å®šè¿è¡Œæ—¶ç”µæµåˆ†å¸ƒçš„ååº¦",
        'steady_kurtosis': "ç¨³æ€å³°åº¦ - ç¨³å®šè¿è¡Œæ—¶ç”µæµåˆ†å¸ƒçš„å³°åº¦",
        'peaks_per_sec': "æ¯ç§’å³°å€¼æ•° - ç¨³å®šè¿è¡Œæ—¶æ¯ç§’å‡ºç°çš„å³°å€¼æ•°é‡",
        'valleys_per_sec': "æ¯ç§’è°·å€¼æ•° - ç¨³å®šè¿è¡Œæ—¶æ¯ç§’å‡ºç°çš„è°·å€¼æ•°é‡",
        'mean_peak_prominence': "å¹³å‡å³°å€¼çªå‡ºåº¦ - å³°å€¼çš„å¹³å‡çªå‡ºç¨‹åº¦",
        'std_peak_prominence': "å³°å€¼çªå‡ºåº¦æ ‡å‡†å·® - å³°å€¼çªå‡ºåº¦çš„æ ‡å‡†å·®",
        'main_freq': "ä¸»é¢‘ç‡ - ä¿¡å·çš„ä¸»è¦é¢‘ç‡æˆåˆ†",
        'main_freq_power': "ä¸»é¢‘ç‡åŠŸç‡ - ä¸»é¢‘ç‡çš„åŠŸç‡å¤§å°",
        'low_freq_ratio': "ä½é¢‘æ¯”ä¾‹ - 0-10Hzé¢‘ç‡æˆåˆ†çš„èƒ½é‡æ¯”ä¾‹",
        'mid_freq_ratio': "ä¸­é¢‘æ¯”ä¾‹ - 10-30Hzé¢‘ç‡æˆåˆ†çš„èƒ½é‡æ¯”ä¾‹",
        'high_freq_ratio': "é«˜é¢‘æ¯”ä¾‹ - 30-50Hzé¢‘ç‡æˆåˆ†çš„èƒ½é‡æ¯”ä¾‹",
        'spectral_centroid': "é¢‘è°±è´¨å¿ƒ - é¢‘è°±çš„é‡å¿ƒä½ç½®",
        'window_mean_std': "çª—å£å‡å€¼æ ‡å‡†å·® - æ»‘åŠ¨çª—å£å‡å€¼çš„æ ‡å‡†å·®",
        'window_mean_range': "çª—å£å‡å€¼èŒƒå›´ - æ»‘åŠ¨çª—å£å‡å€¼çš„èŒƒå›´",
        'mean_window_std': "å¹³å‡çª—å£æ ‡å‡†å·® - æ»‘åŠ¨çª—å£æ ‡å‡†å·®çš„å¹³å‡å€¼",
        'std_window_std': "çª—å£æ ‡å‡†å·®çš„æ ‡å‡†å·® - æ»‘åŠ¨çª—å£æ ‡å‡†å·®çš„æ ‡å‡†å·®",
        'autocorr_lag10': "10æ»åè‡ªç›¸å…³ - ä¿¡å·ä¸å…¶10ä¸ªé‡‡æ ·ç‚¹å»¶è¿Ÿçš„ç›¸å…³æ€§",
        'first_min_time': "ç¬¬ä¸€ä¸ªæœ€å°å€¼æ—¶é—´ - è‡ªç›¸å…³å‡½æ•°ç¬¬ä¸€ä¸ªæœ€å°å€¼çš„æ—¶é—´",
        'voltage_mean': "ç”µå‹å¹³å‡å€¼ - ç”µå‹ä¿¡å·çš„å¹³å‡å€¼",
        'voltage_std': "ç”µå‹æ ‡å‡†å·® - ç”µå‹ä¿¡å·çš„æ ‡å‡†å·®",
        'voltage_amplitude': "ç”µå‹å¹…åº¦ - ç”µå‹ä¿¡å·çš„æœ€å¤§æœ€å°å€¼ä¹‹å·®",
        'power_mean': "åŠŸç‡å¹³å‡å€¼ - ç¬æ—¶åŠŸç‡çš„å¹³å‡å€¼",
        'power_std': "åŠŸç‡æ ‡å‡†å·® - ç¬æ—¶åŠŸç‡çš„æ ‡å‡†å·®",
        'power_max': "åŠŸç‡æœ€å¤§å€¼ - ç¬æ—¶åŠŸç‡çš„æœ€å¤§å€¼",
        'voltage_current_corr': "ç”µå‹ç”µæµç›¸å…³æ€§ - ç”µå‹å’Œç”µæµä¿¡å·çš„ç›¸å…³ç³»æ•°"
    }

    # åˆ›å»ºç‰¹å¾è¯´æ˜è¡¨æ ¼
    feature_info = []
    for i, name in enumerate(feature_names):
        feature_info.append({
            'åºå·': i + 1,
            'ç‰¹å¾åç§°': name,
            'å½“å‰å€¼': features_df[name].iloc[0] if len(features_df) > 0 else 'N/A',
            'è¯´æ˜': feature_descriptions.get(name, 'æš‚æ— è¯´æ˜')
        })

    feature_info_df = pd.DataFrame(feature_info)

    # æ˜¾ç¤ºè¡¨æ ¼
    st.dataframe(
        feature_info_df,
        use_container_width=True,
        hide_index=True
    )

    # ç‰¹å¾åˆ†ç±»å±•ç¤º
    st.subheader("ç‰¹å¾åˆ†ç±»")

    col1, col2, col3 = st.columns(3)

    with col1:
        st.markdown("**ğŸš€ å¯åŠ¨ç‰¹å¾**")
        startup_features = ['startup_peak', 'startup_peak_time', 'startup_acceleration', 'startup_slope']
        for f in startup_features:
            if f in features_df.columns:
                st.write(f"- {f}: {features_df[f].iloc[0]:.4f}")

    with col2:
        st.markdown("**ğŸ“Š ç¨³æ€ç‰¹å¾**")
        steady_features = ['steady_mean', 'steady_std', 'steady_amplitude', 'steady_iqr']
        for f in steady_features:
            if f in features_df.columns:
                st.write(f"- {f}: {features_df[f].iloc[0]:.4f}")

    with col3:
        st.markdown("**ğŸŒŠ é¢‘åŸŸç‰¹å¾**")
        freq_features = ['main_freq', 'main_freq_power', 'spectral_centroid']
        for f in freq_features:
            if f in features_df.columns:
                st.write(f"- {f}: {features_df[f].iloc[0]:.4f}")


def show_export_section():
    """å¯¼å‡ºç»“æœéƒ¨åˆ†"""
    st.header("ğŸ’¾ å¯¼å‡ºç»“æœ")

    if 'fe_features' not in st.session_state:
        st.warning("è¯·å…ˆæå–ç‰¹å¾")
        return

    features_df = st.session_state['fe_features']

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("å¯¼å‡ºç‰¹å¾æ•°æ®")

        # æ–‡ä»¶æ ¼å¼é€‰æ‹©
        file_format = st.selectbox(
            "é€‰æ‹©å¯¼å‡ºæ ¼å¼ï¼š",
            ["CSV", "Excel", "JSON"]
        )

        # æ·»åŠ å…ƒæ•°æ®
        include_metadata = st.checkbox("åŒ…å«å…ƒæ•°æ®", value=True)

        if st.button("å¯¼å‡ºç‰¹å¾æ•°æ®", key="fe_export_features"):
            try:
                if file_format == "CSV":
                    csv = features_df.to_csv(index=False)
                    b64 = base64.b64encode(csv.encode()).decode()
                    href = f'<a href="data:file/csv;base64,{b64}" download="extracted_features.csv">ä¸‹è½½CSVæ–‡ä»¶</a>'
                    st.markdown(href, unsafe_allow_html=True)

                elif file_format == "Excel":
                    output = io.BytesIO()
                    with pd.ExcelWriter(output, engine='xlsxwriter') as writer:
                        features_df.to_excel(writer, sheet_name='Features', index=False)

                        if include_metadata:
                            metadata = {
                                'Extraction Time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                                'Feature Count': len(features_df.columns),
                                'Sample Count': len(features_df),
                                'Source File': st.session_state.get('fe_filename', 'Unknown')
                            }
                            metadata_df = pd.DataFrame(list(metadata.items()), columns=['Property', 'Value'])
                            metadata_df.to_excel(writer, sheet_name='Metadata', index=False)

                    b64 = base64.b64encode(output.getvalue()).decode()
                    href = f'<a href="data:application/vnd.openxmlformats-officedocument.spreadsheetml.sheet;base64,{b64}" download="extracted_features.xlsx">ä¸‹è½½Excelæ–‡ä»¶</a>'
                    st.markdown(href, unsafe_allow_html=True)

                else:  # JSON
                    json_str = features_df.to_json(orient='records', indent=2)
                    b64 = base64.b64encode(json_str.encode()).decode()
                    href = f'<a href="data:file/json;base64,{b64}" download="extracted_features.json">ä¸‹è½½JSONæ–‡ä»¶</a>'
                    st.markdown(href, unsafe_allow_html=True)

                st.success("âœ… å¯¼å‡ºå‡†å¤‡å®Œæˆï¼Œç‚¹å‡»é“¾æ¥ä¸‹è½½")

            except Exception as e:
                st.error(f"å¯¼å‡ºå¤±è´¥: {e}")

    with col2:
        st.subheader("å¯¼å‡ºåˆ°å…¶ä»–æ¨¡å—")

        # é€‰æ‹©ç›®æ ‡æ¨¡å—
        target_module = st.selectbox(
            "é€‰æ‹©ç›®æ ‡æ¨¡å—ï¼š",
            ["åˆ†ç±»æ¨¡å—", "å›å½’æ¨¡å—", "èšç±»æ¨¡å—"]
        )

        # æ·»åŠ æ ‡ç­¾ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if target_module in ["åˆ†ç±»æ¨¡å—", "å›å½’æ¨¡å—"]:
            st.info("åˆ†ç±»å’Œå›å½’ä»»åŠ¡éœ€è¦æ·»åŠ ç›®æ ‡æ ‡ç­¾")

            label_source = st.radio(
                "æ ‡ç­¾æ¥æºï¼š",
                ["æ‰‹åŠ¨è¾“å…¥", "ä»æ–‡ä»¶åŠ è½½"]
            )

            if label_source == "æ‰‹åŠ¨è¾“å…¥":
                if target_module == "åˆ†ç±»æ¨¡å—":
                    label_value = st.text_input("è¾“å…¥ç±»åˆ«æ ‡ç­¾ï¼š", "Class_A")
                else:
                    label_value = st.number_input("è¾“å…¥ç›®æ ‡å€¼ï¼š", value=1.0)

                # ä¸ºæ‰€æœ‰æ ·æœ¬æ·»åŠ ç›¸åŒæ ‡ç­¾
                features_with_label = features_df.copy()
                features_with_label['target'] = label_value
            else:
                st.info("è¯·ç¡®ä¿æ ‡ç­¾æ–‡ä»¶ä¸ç‰¹å¾æ•°æ®çš„æ ·æœ¬æ•°é‡ä¸€è‡´")
        else:
            features_with_label = features_df.copy()

        if st.button("å¯¼å‡ºåˆ°æ¨¡å—", key="fe_export_to_module"):
            if target_module == "åˆ†ç±»æ¨¡å—":
                st.session_state['classification_data'] = features_with_label
                st.success("âœ… ç‰¹å¾æ•°æ®å·²å¯¼å‡ºåˆ°åˆ†ç±»æ¨¡å—")
            elif target_module == "å›å½’æ¨¡å—":
                st.session_state['regression_data'] = features_with_label
                st.success("âœ… ç‰¹å¾æ•°æ®å·²å¯¼å‡ºåˆ°å›å½’æ¨¡å—")
            else:
                st.session_state['clustering_data'] = features_with_label
                st.success("âœ… ç‰¹å¾æ•°æ®å·²å¯¼å‡ºåˆ°èšç±»æ¨¡å—")

            st.info(f"è¯·å‰å¾€{target_module}ç»§ç»­åˆ†æ")

    # ç”Ÿæˆåˆ†æè„šæœ¬
    st.subheader("ç”Ÿæˆåˆ†æè„šæœ¬")

    script_type = st.selectbox(
        "é€‰æ‹©è„šæœ¬ç±»å‹ï¼š",
        ["åˆ†ç±»åˆ†æ", "å›å½’åˆ†æ", "èšç±»åˆ†æ", "é€šç”¨åˆ†æ"]
    )

    if st.button("ç”ŸæˆPythonè„šæœ¬", key="fe_generate_script"):
        try:
            # å…ˆä¿å­˜ç‰¹å¾æ–‡ä»¶
            temp_file = "temp_features.csv"
            features_df.to_csv(temp_file, index=False)

            # åˆ›å»ºè„šæœ¬
            script_types_map = {
                "åˆ†ç±»åˆ†æ": "classification",
                "å›å½’åˆ†æ": "regression",
                "èšç±»åˆ†æ": "clustering",
                "é€šç”¨åˆ†æ": "general"
            }

            script_path = create_analysis_script(
                temp_file,
                script_types_map[script_type],
                output_dir="generated_scripts"
            )

            # è¯»å–å¹¶æ˜¾ç¤ºè„šæœ¬
            with open(script_path, 'r', encoding='utf-8') as f:
                script_content = f.read()

            st.code(script_content, language='python')

            # æä¾›ä¸‹è½½
            b64 = base64.b64encode(script_content.encode()).decode()
            href = f'<a href="data:file/python;base64,{b64}" download="{os.path.basename(script_path)}">ä¸‹è½½Pythonè„šæœ¬</a>'
            st.markdown(href, unsafe_allow_html=True)

            st.success(f"âœ… è„šæœ¬ç”ŸæˆæˆåŠŸ: {script_path}")

            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_file):
                os.remove(temp_file)

        except Exception as e:
            st.error(f"ç”Ÿæˆè„šæœ¬å¤±è´¥: {e}")
            import traceback
            st.code(traceback.format_exc())

# ä¸»å‡½æ•°å·²ç»åœ¨æœ€ä¸Šé¢å®šä¹‰äº† show_feature_extraction_page()