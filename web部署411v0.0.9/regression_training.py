# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import time
from datetime import datetime
import base64
from io import BytesIO
import joblib
import platform
import matplotlib.font_manager as fm
import random
import warnings

# --- æ¨¡å‹åº“å¯¼å…¥ ---
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler

# å›å½’æ¨¡å‹
from catboost import CatBoostRegressor
from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor,
                              ExtraTreesRegressor, AdaBoostRegressor)
from sklearn.svm import SVR
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet

# æ·±åº¦å­¦ä¹ 
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.optimizers import Adam, SGD, RMSprop
    from tensorflow.keras.callbacks import EarlyStopping

    TENSORFLOW_AVAILABLE = True
except ImportError:
    TENSORFLOW_AVAILABLE = False
    st.warning("TensorFlowæœªå®‰è£…ï¼Œæ·±åº¦å­¦ä¹ æ¨¡å‹å°†ä¸å¯ç”¨")

# --- å…¨å±€è®¾ç½® ---
warnings.filterwarnings("ignore", category=UserWarning, message="Glyph.*? missing from current font")
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=RuntimeWarning, message="invalid value encountered in scalar divide")
warnings.filterwarnings("ignore", category=UserWarning, message="`use_label_encoder` is deprecated")

# --- å›å½’æ¨¡å‹ä¿¡æ¯é…ç½® ---
REGRESSOR_INFO = {
    "decision_tree": {
        "name": "å†³ç­–æ ‘å›å½’ (Decision Tree)",
        "description": "é€šè¿‡é€’å½’åœ°å°†æ•°æ®åˆ†å‰²æˆå­é›†æ¥æ„å»ºé¢„æµ‹æ¨¡å‹çš„æ ‘å½¢ç®—æ³•",
        "advantages": [
            "æ˜“äºç†è§£å’Œè§£é‡Šï¼Œå¯ä»¥å¯è§†åŒ–",
            "ä¸éœ€è¦æ•°æ®é¢„å¤„ç†ï¼ˆå¦‚æ ‡å‡†åŒ–ï¼‰",
            "å¯ä»¥å¤„ç†æ•°å€¼å‹å’Œç±»åˆ«å‹ç‰¹å¾",
            "èƒ½å¤Ÿæ•æ‰éçº¿æ€§å…³ç³»"
        ],
        "disadvantages": [
            "å®¹æ˜“è¿‡æ‹Ÿåˆï¼Œç‰¹åˆ«æ˜¯æ·±å±‚æ ‘",
            "å¯¹å™ªå£°æ•°æ®æ•æ„Ÿ",
            "å†³ç­–è¾¹ç•Œæ˜¯è½´å¹³è¡Œçš„",
            "å¯¹è¿ç»­å€¼é¢„æµ‹å¯èƒ½ä¸å¤Ÿå¹³æ»‘"
        ],
        "suitable_for": "ä¸­å°å‹æ•°æ®é›†ï¼Œéœ€è¦æ¨¡å‹å¯è§£é‡Šæ€§çš„å›å½’ä»»åŠ¡"
    },
    "random_forest": {
        "name": "éšæœºæ£®æ—å›å½’ (Random Forest)",
        "description": "é›†æˆå¤šä¸ªå†³ç­–æ ‘ï¼Œé€šè¿‡å¹³å‡é¢„æµ‹å€¼æ¥æé«˜å‡†ç¡®æ€§å’Œç¨³å®šæ€§",
        "advantages": [
            "å‡†ç¡®ç‡é«˜ï¼Œä¸å®¹æ˜“è¿‡æ‹Ÿåˆ",
            "å¯ä»¥è¯„ä¼°ç‰¹å¾é‡è¦æ€§",
            "èƒ½å¤Ÿå¤„ç†é«˜ç»´æ•°æ®",
            "å¯¹ç¼ºå¤±æ•°æ®å’Œå¼‚å¸¸å€¼ä¸æ•æ„Ÿ"
        ],
        "disadvantages": [
            "æ¨¡å‹è¾ƒå¤§ï¼Œé¢„æµ‹é€Ÿåº¦ç›¸å¯¹è¾ƒæ…¢",
            "åœ¨æŸäº›çº¿æ€§å…³ç³»å¼ºçš„æ•°æ®ä¸Šå¯èƒ½è¡¨ç°ä¸å¦‚çº¿æ€§æ¨¡å‹",
            "éš¾ä»¥è§£é‡Šå•ä¸ªé¢„æµ‹"
        ],
        "suitable_for": "å¤§å¤šæ•°å›å½’ä»»åŠ¡ï¼Œç‰¹åˆ«é€‚åˆéœ€è¦ç‰¹å¾é‡è¦æ€§åˆ†æçš„åœºæ™¯"
    },
    "gradient_boosting": {
        "name": "æ¢¯åº¦æå‡å›å½’ (GBDT)",
        "description": "é€šè¿‡é€æ­¥æ”¹è¿›çš„æ–¹å¼æ„å»ºé›†æˆæ¨¡å‹ï¼Œæ¯ä¸ªæ–°æ¨¡å‹éƒ½å°è¯•çº æ­£å‰ä¸€ä¸ªæ¨¡å‹çš„é”™è¯¯",
        "advantages": [
            "é¢„æµ‹å‡†ç¡®ç‡é«˜",
            "èƒ½å¤Ÿå¤„ç†éçº¿æ€§å…³ç³»",
            "å¯¹å¼‚å¸¸å€¼çš„é²æ£’æ€§è¾ƒå¥½",
            "å¯ä»¥å¤„ç†ä¸åŒç±»å‹çš„ç‰¹å¾"
        ],
        "disadvantages": [
            "è®­ç»ƒæ—¶é—´é•¿",
            "éœ€è¦ä»”ç»†è°ƒå‚ä»¥é¿å…è¿‡æ‹Ÿåˆ",
            "éš¾ä»¥å¹¶è¡ŒåŒ–è®­ç»ƒè¿‡ç¨‹"
        ],
        "suitable_for": "ç«èµ›å’Œéœ€è¦é«˜ç²¾åº¦çš„å›å½’åœºæ™¯"
    },
    "catboost": {
        "name": "CatBoostå›å½’",
        "description": "ä¸“é—¨å¤„ç†ç±»åˆ«ç‰¹å¾çš„æ¢¯åº¦æå‡ç®—æ³•ï¼Œè‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾ç¼–ç ",
        "advantages": [
            "è‡ªåŠ¨å¤„ç†ç±»åˆ«ç‰¹å¾ï¼Œæ— éœ€é¢„å¤„ç†",
            "å‡å°‘è¿‡æ‹Ÿåˆï¼Œå†…ç½®æ­£åˆ™åŒ–",
            "è®­ç»ƒé€Ÿåº¦å¿«ï¼Œæ”¯æŒGPUåŠ é€Ÿ",
            "é»˜è®¤å‚æ•°è¡¨ç°è‰¯å¥½"
        ],
        "disadvantages": [
            "æ¨¡å‹æ–‡ä»¶è¾ƒå¤§",
            "å¯¹äºçº¯æ•°å€¼ç‰¹å¾å¯èƒ½ä¸å¦‚å…¶ä»–ä¸“é—¨ç®—æ³•",
            "ç›¸å¯¹è¾ƒæ–°ï¼Œç¤¾åŒºæ”¯æŒè¾ƒå°‘"
        ],
        "suitable_for": "åŒ…å«å¤§é‡ç±»åˆ«ç‰¹å¾çš„å›å½’æ•°æ®é›†"
    },
    "xgboost": {
        "name": "æé™æ¢¯åº¦æå‡å›å½’ (XGBoost)",
        "description": "é«˜æ•ˆçš„æ¢¯åº¦æå‡å®ç°ï¼Œåœ¨è®¸å¤šæœºå™¨å­¦ä¹ ç«èµ›ä¸­è¡¨ç°ä¼˜å¼‚",
        "advantages": [
            "é€Ÿåº¦å¿«ï¼Œæ€§èƒ½é«˜",
            "å†…ç½®æ­£åˆ™åŒ–å‡å°‘è¿‡æ‹Ÿåˆ",
            "å¯ä»¥å¤„ç†ç¼ºå¤±å€¼",
            "æ”¯æŒå¹¶è¡Œè®¡ç®—å’ŒGPUåŠ é€Ÿ"
        ],
        "disadvantages": [
            "å‚æ•°è¾ƒå¤šï¼Œè°ƒå‚å¤æ‚",
            "å¯¹ç±»åˆ«ç‰¹å¾éœ€è¦é¢„å¤„ç†",
            "å†…å­˜æ¶ˆè€—è¾ƒå¤§"
        ],
        "suitable_for": "å¤§è§„æ¨¡æ•°æ®é›†ï¼Œç«èµ›å¸¸ç”¨ï¼Œå¯¹æ€§èƒ½è¦æ±‚é«˜çš„åœºæ™¯"
    },
    "lightgbm": {
        "name": "è½»é‡çº§æ¢¯åº¦æå‡å›å½’ (LightGBM)",
        "description": "åŸºäºç›´æ–¹å›¾çš„é«˜æ•ˆæ¢¯åº¦æå‡ç®—æ³•ï¼Œå¾®è½¯å¼€å‘",
        "advantages": [
            "è®­ç»ƒé€Ÿåº¦æå¿«",
            "å†…å­˜ä½¿ç”¨å°‘",
            "å‡†ç¡®ç‡é«˜",
            "æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒ"
        ],
        "disadvantages": [
            "å¯èƒ½åœ¨å°æ•°æ®é›†ä¸Šè¿‡æ‹Ÿåˆ",
            "å¯¹å™ªå£°æ•æ„Ÿ",
            "éœ€è¦è¾ƒæ–°ç‰ˆæœ¬çš„ä¾èµ–åº“"
        ],
        "suitable_for": "å¤§è§„æ¨¡æ•°æ®é›†ï¼Œéœ€è¦å¿«é€Ÿè®­ç»ƒçš„å›å½’åœºæ™¯"
    },
    "extra_trees": {
        "name": "æç«¯éšæœºæ ‘å›å½’ (Extra Trees)",
        "description": "æ¯”éšæœºæ£®æ—æ›´éšæœºçš„é›†æˆæ–¹æ³•ï¼Œåœ¨é€‰æ‹©åˆ†å‰²ç‚¹æ—¶å¢åŠ æ›´å¤šéšæœºæ€§",
        "advantages": [
            "è®­ç»ƒé€Ÿåº¦æ¯”éšæœºæ£®æ—å¿«",
            "å‡å°‘è¿‡æ‹Ÿåˆé£é™©",
            "åœ¨æŸäº›æ•°æ®é›†ä¸Šè¡¨ç°ä¼˜äºéšæœºæ£®æ—",
            "æ–¹å·®æ›´å°"
        ],
        "disadvantages": [
            "å¯èƒ½éœ€è¦æ›´å¤šçš„æ ‘æ¥è¾¾åˆ°ç›¸åŒç²¾åº¦",
            "æ¨¡å‹è§£é‡Šæ€§è¾ƒå·®",
            "å¯¹å‚æ•°è®¾ç½®æ•æ„Ÿ"
        ],
        "suitable_for": "é«˜ç»´æ•°æ®ï¼Œéœ€è¦å¿«é€Ÿè®­ç»ƒçš„å›å½’åœºæ™¯"
    },
    "adaboost": {
        "name": "è‡ªé€‚åº”å¢å¼ºå›å½’ (AdaBoost)",
        "description": "é€šè¿‡ç»„åˆå¤šä¸ªå¼±å­¦ä¹ å™¨æ„å»ºå¼ºå­¦ä¹ å™¨çš„é›†æˆæ–¹æ³•",
        "advantages": [
            "èƒ½å¤Ÿæé«˜å¼±å­¦ä¹ å™¨çš„æ€§èƒ½",
            "ä¸å®¹æ˜“è¿‡æ‹Ÿåˆ",
            "å¯ä»¥ä½¿ç”¨å„ç§ç±»å‹çš„å›å½’å™¨ä½œä¸ºåŸºå­¦ä¹ å™¨",
            "ç†è®ºåŸºç¡€æ‰å®"
        ],
        "disadvantages": [
            "å¯¹å™ªå£°å’Œå¼‚å¸¸å€¼æ•æ„Ÿ",
            "è®­ç»ƒæ—¶é—´å¯èƒ½è¾ƒé•¿",
            "åœ¨å¤æ‚æ•°æ®ä¸Šè¡¨ç°å¯èƒ½ä¸å¦‚å…¶ä»–é›†æˆæ–¹æ³•"
        ],
        "suitable_for": "æ•°æ®è´¨é‡è¾ƒé«˜çš„å›å½’é—®é¢˜ï¼ŒåŸºå­¦ä¹ å™¨ç›¸å¯¹ç®€å•çš„åœºæ™¯"
    },
    "linear": {
        "name": "çº¿æ€§å›å½’ (Linear Regression)",
        "description": "æœ€åŸºæœ¬çš„çº¿æ€§å›å½’æ¨¡å‹ï¼Œå‡è®¾ç‰¹å¾å’Œç›®æ ‡ä¹‹é—´å­˜åœ¨çº¿æ€§å…³ç³»",
        "advantages": [
            "ç®€å•å¿«é€Ÿï¼Œæ˜“äºç†è§£",
            "å¯è§£é‡Šæ€§å¼º",
            "ä¸éœ€è¦è°ƒæ•´è¶…å‚æ•°",
            "é€‚åˆçº¿æ€§å…³ç³»æ˜æ˜¾çš„æ•°æ®"
        ],
        "disadvantages": [
            "åªèƒ½å¤„ç†çº¿æ€§å…³ç³»",
            "å¯¹å¼‚å¸¸å€¼æ•æ„Ÿ",
            "å½“ç‰¹å¾æ•°é‡æ¥è¿‘æ ·æœ¬æ•°é‡æ—¶å¯èƒ½ä¸ç¨³å®š"
        ],
        "suitable_for": "çº¿æ€§å…³ç³»æ˜æ˜¾çš„å›å½’é—®é¢˜ï¼Œéœ€è¦å¿«é€Ÿå»ºæ¨¡å’Œå¼ºå¯è§£é‡Šæ€§çš„åœºæ™¯"
    },
    "ridge": {
        "name": "å²­å›å½’ (Ridge Regression)",
        "description": "å¸¦L2æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ï¼Œå¯ä»¥å¤„ç†å¤šé‡å…±çº¿æ€§é—®é¢˜",
        "advantages": [
            "è§£å†³å¤šé‡å…±çº¿æ€§é—®é¢˜",
            "é˜²æ­¢è¿‡æ‹Ÿåˆ",
            "ç³»æ•°æ”¶ç¼©ä½¿æ¨¡å‹æ›´ç¨³å®š",
            "æœ‰é—­å¼è§£"
        ],
        "disadvantages": [
            "ä»ç„¶å‡è®¾çº¿æ€§å…³ç³»",
            "ä¸èƒ½è¿›è¡Œç‰¹å¾é€‰æ‹©",
            "éœ€è¦é€‰æ‹©æ­£åˆ™åŒ–å‚æ•°"
        ],
        "suitable_for": "ç‰¹å¾é—´å­˜åœ¨å¤šé‡å…±çº¿æ€§çš„çº¿æ€§å›å½’é—®é¢˜"
    },
    "lasso": {
        "name": "å¥—ç´¢å›å½’ (Lasso Regression)",
        "description": "å¸¦L1æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ï¼Œå¯ä»¥è¿›è¡Œè‡ªåŠ¨ç‰¹å¾é€‰æ‹©",
        "advantages": [
            "è‡ªåŠ¨è¿›è¡Œç‰¹å¾é€‰æ‹©",
            "äº§ç”Ÿç¨€ç–æ¨¡å‹",
            "é˜²æ­¢è¿‡æ‹Ÿåˆ",
            "å¯è§£é‡Šæ€§å¼º"
        ],
        "disadvantages": [
            "ä»ç„¶å‡è®¾çº¿æ€§å…³ç³»",
            "å½“ç‰¹å¾æ•°å¤§äºæ ·æœ¬æ•°æ—¶é€‰æ‹©æœ‰é™",
            "å¯¹ç›¸å…³ç‰¹å¾å¯èƒ½éšæœºé€‰æ‹©ä¸€ä¸ª"
        ],
        "suitable_for": "é«˜ç»´ç¨€ç–æ•°æ®ï¼Œéœ€è¦ç‰¹å¾é€‰æ‹©çš„çº¿æ€§å›å½’é—®é¢˜"
    },
    "elastic_net": {
        "name": "å¼¹æ€§ç½‘ç»œå›å½’ (ElasticNet)",
        "description": "ç»“åˆL1å’ŒL2æ­£åˆ™åŒ–çš„çº¿æ€§å›å½’ï¼Œå…¼å…·Ridgeå’ŒLassoçš„ä¼˜ç‚¹",
        "advantages": [
            "å…¼å…·Ridgeå’ŒLassoçš„ä¼˜ç‚¹",
            "å¯ä»¥é€‰æ‹©ç›¸å…³ç‰¹å¾ç»„",
            "åœ¨ç‰¹å¾æ•°å¤§äºæ ·æœ¬æ•°æ—¶è¡¨ç°è‰¯å¥½",
            "å‚æ•°è°ƒèŠ‚çµæ´»"
        ],
        "disadvantages": [
            "éœ€è¦è°ƒèŠ‚ä¸¤ä¸ªæ­£åˆ™åŒ–å‚æ•°",
            "ä»ç„¶å‡è®¾çº¿æ€§å…³ç³»",
            "è®¡ç®—å¤æ‚åº¦ç›¸å¯¹è¾ƒé«˜"
        ],
        "suitable_for": "é«˜ç»´æ•°æ®ï¼Œç‰¹å¾é—´å­˜åœ¨ç›¸å…³æ€§ä¸”éœ€è¦ç‰¹å¾é€‰æ‹©çš„åœºæ™¯"
    },
    "svm": {
        "name": "æ”¯æŒå‘é‡å›å½’ (SVR)",
        "description": "é€šè¿‡å¯»æ‰¾æœ€ä¼˜è¶…å¹³é¢æ¥è¿›è¡Œå›å½’é¢„æµ‹ï¼Œå¯ä»¥å¤„ç†éçº¿æ€§å…³ç³»",
        "advantages": [
            "åœ¨é«˜ç»´ç©ºé—´è¡¨ç°è‰¯å¥½",
            "ä½¿ç”¨æ ¸æŠ€å·§å¯å¤„ç†éçº¿æ€§é—®é¢˜",
            "å¯¹å¼‚å¸¸å€¼ç›¸å¯¹é²æ£’",
            "æ³›åŒ–èƒ½åŠ›å¼º"
        ],
        "disadvantages": [
            "å¤§è§„æ¨¡æ•°æ®è®­ç»ƒæ…¢",
            "å¯¹å‚æ•°å’Œæ ¸å‡½æ•°é€‰æ‹©æ•æ„Ÿ",
            "éš¾ä»¥å¤„ç†å¤§è§„æ¨¡æ•°æ®",
            "æ¨¡å‹è§£é‡Šæ€§å·®"
        ],
        "suitable_for": "ä¸­å°å‹æ•°æ®é›†ï¼Œç‰¹åˆ«æ˜¯é«˜ç»´æ•°æ®æˆ–éçº¿æ€§å…³ç³»æ˜æ˜¾çš„åœºæ™¯"
    },
    "knn": {
        "name": "Kè¿‘é‚»å›å½’ (KNN Regression)",
        "description": "åŸºäºå®ä¾‹çš„æ‡’æƒ°å­¦ä¹ ç®—æ³•ï¼Œé¢„æµ‹å€¼æ˜¯kä¸ªæœ€è¿‘é‚»çš„å¹³å‡å€¼",
        "advantages": [
            "ç®€å•ç›´è§‚ï¼Œæ˜“äºç†è§£",
            "å¯¹éçº¿æ€§æ•°æ®æ•ˆæœå¥½",
            "ä¸éœ€è¦è®­ç»ƒè¿‡ç¨‹",
            "å¯ä»¥å¤„ç†å¤šè¾“å‡ºå›å½’"
        ],
        "disadvantages": [
            "è®¡ç®—æˆæœ¬é«˜ï¼ˆé¢„æµ‹æ…¢ï¼‰",
            "å¯¹é«˜ç»´æ•°æ®æ•ˆæœå·®ï¼ˆç»´åº¦ç¾éš¾ï¼‰",
            "å¯¹å±€éƒ¨å™ªå£°æ•æ„Ÿ",
            "éœ€è¦å¤§é‡å­˜å‚¨ç©ºé—´"
        ],
        "suitable_for": "å°å‹æ•°æ®é›†ï¼Œç‰¹å¾ç»´åº¦ä¸é«˜ï¼Œå±€éƒ¨æ¨¡å¼æ˜æ˜¾çš„å›å½’åœºæ™¯"
    },
    "neural_network": {
        "name": "ç¥ç»ç½‘ç»œå›å½’ (MLP)",
        "description": "å¤šå±‚æ„ŸçŸ¥å™¨ï¼Œå¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»",
        "advantages": [
            "å¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»",
            "é€‚åº”æ€§å¼º",
            "å¯ä»¥å¤„ç†å¤§è§„æ¨¡ç‰¹å¾",
            "ç†è®ºä¸Šå¯ä»¥é€¼è¿‘ä»»æ„å‡½æ•°"
        ],
        "disadvantages": [
            "éœ€è¦å¤§é‡æ•°æ®",
            "è®­ç»ƒæ—¶é—´é•¿",
            "éš¾ä»¥è§£é‡Š",
            "éœ€è¦è°ƒæ•´å¤šä¸ªè¶…å‚æ•°",
            "å®¹æ˜“è¿‡æ‹Ÿåˆ"
        ],
        "suitable_for": "å¤§å‹å¤æ‚æ•°æ®é›†ï¼Œéçº¿æ€§å…³ç³»å¼ºçš„å›å½’åœºæ™¯"
    },
    "bp_neural_network": {
        "name": "BPç¥ç»ç½‘ç»œå›å½’",
        "description": "ä½¿ç”¨åå‘ä¼ æ’­ç®—æ³•çš„æ·±åº¦ç¥ç»ç½‘ç»œï¼Œé€‚åˆå¤æ‚å›å½’ä»»åŠ¡",
        "advantages": [
            "å¯ä»¥å­¦ä¹ å¤æ‚çš„éçº¿æ€§å…³ç³»",
            "é€‚ç”¨äºå„ç§å›å½’é—®é¢˜",
            "å¯ä»¥è‡ªåŠ¨å­¦ä¹ ç‰¹å¾è¡¨ç¤º",
            "æ”¯æŒå¤šå±‚ç»“æ„å’Œä¸åŒæ¿€æ´»å‡½æ•°"
        ],
        "disadvantages": [
            "å®¹æ˜“è¿‡æ‹Ÿåˆ",
            "éœ€è¦å¤§é‡æ•°æ®",
            "è®­ç»ƒæ—¶é—´è¾ƒé•¿",
            "éœ€è¦è°ƒæ•´å¾ˆå¤šè¶…å‚æ•°",
            "éœ€è¦GPUåŠ é€Ÿ"
        ],
        "suitable_for": "ä¸­å¤§å‹æ•°æ®é›†ï¼Œå¤æ‚çš„éçº¿æ€§å›å½’é—®é¢˜"
    },
    "rnn": {
        "name": "å¾ªç¯ç¥ç»ç½‘ç»œå›å½’ (RNN)",
        "description": "ä¸“é—¨å¤„ç†åºåˆ—æ•°æ®çš„ç¥ç»ç½‘ç»œï¼Œé€‚åˆæ—¶åºå›å½’ä»»åŠ¡",
        "advantages": [
            "å¯ä»¥å¤„ç†å˜é•¿åºåˆ—",
            "å…·æœ‰è®°å¿†åŠŸèƒ½",
            "é€‚åˆæ—¶åºæ•°æ®",
            "å‚æ•°å…±äº«æ•ˆç‡é«˜"
        ],
        "disadvantages": [
            "æ¢¯åº¦æ¶ˆå¤±é—®é¢˜",
            "è®­ç»ƒé€Ÿåº¦æ…¢",
            "éš¾ä»¥å¤„ç†é•¿åºåˆ—",
            "å¯¹æ•°æ®æ ¼å¼è¦æ±‚é«˜"
        ],
        "suitable_for": "æ—¶åºæ•°æ®å›å½’ã€åºåˆ—é¢„æµ‹é—®é¢˜"
    },
    "lstm": {
        "name": "é•¿çŸ­æœŸè®°å¿†ç½‘ç»œå›å½’ (LSTM)",
        "description": "è§£å†³RNNæ¢¯åº¦æ¶ˆå¤±é—®é¢˜çš„æ”¹è¿›ç‰ˆå¾ªç¯ç¥ç»ç½‘ç»œ",
        "advantages": [
            "è§£å†³é•¿æœŸä¾èµ–é—®é¢˜",
            "é¿å…æ¢¯åº¦æ¶ˆå¤±",
            "é€‚åˆé•¿åºåˆ—",
            "è®°å¿†èƒ½åŠ›å¼º"
        ],
        "disadvantages": [
            "è®¡ç®—å¤æ‚åº¦é«˜",
            "å‚æ•°è¾ƒå¤š",
            "è®­ç»ƒæ—¶é—´é•¿",
            "å¯¹æ•°æ®é¢„å¤„ç†è¦æ±‚é«˜"
        ],
        "suitable_for": "é•¿æ—¶åºæ•°æ®å›å½’ã€éœ€è¦é•¿æœŸè®°å¿†çš„åºåˆ—é¢„æµ‹é—®é¢˜"
    },
    "gru": {
        "name": "é—¨æ§å¾ªç¯å•å…ƒå›å½’ (GRU)",
        "description": "LSTMçš„ç®€åŒ–ç‰ˆæœ¬ï¼Œè®¡ç®—æ•ˆç‡æ›´é«˜çš„å¾ªç¯ç¥ç»ç½‘ç»œ",
        "advantages": [
            "æ¯”LSTMå‚æ•°å°‘",
            "è®­ç»ƒé€Ÿåº¦è¾ƒå¿«",
            "æ€§èƒ½æ¥è¿‘LSTM",
            "é¿å…æ¢¯åº¦æ¶ˆå¤±"
        ],
        "disadvantages": [
            "ä»éœ€è¦åºåˆ—æ•°æ®",
            "è¶…å‚æ•°è°ƒèŠ‚å¤æ‚",
            "å¯¹çŸ­åºåˆ—æ•ˆæœä¸€èˆ¬",
            "å¯è§£é‡Šæ€§å·®"
        ],
        "suitable_for": "ä¸­é•¿åºåˆ—æ•°æ®å›å½’ã€è®¡ç®—èµ„æºæœ‰é™çš„åºåˆ—é¢„æµ‹"
    },
    "cnn": {
        "name": "å·ç§¯ç¥ç»ç½‘ç»œå›å½’ (CNN)",
        "description": "ä½¿ç”¨å·ç§¯æ“ä½œæå–ç‰¹å¾çš„ç¥ç»ç½‘ç»œï¼Œé€‚åˆæœ‰ç©ºé—´ç»“æ„çš„å›å½’æ•°æ®",
        "advantages": [
            "å¹³ç§»ä¸å˜æ€§",
            "å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›å¼º",
            "å‚æ•°å…±äº«å‡å°‘è¿‡æ‹Ÿåˆ",
            "è®¡ç®—æ•ˆç‡é«˜"
        ],
        "disadvantages": [
            "éœ€è¦è¾ƒå¤šæ•°æ®",
            "å¯¹æ•°æ®ç»´åº¦æœ‰è¦æ±‚",
            "è¶…å‚æ•°æ•æ„Ÿ",
            "å¯è§£é‡Šæ€§å·®"
        ],
        "suitable_for": "å›¾åƒæ•°æ®å›å½’ã€ç©ºé—´ç»“æ„æ•°æ®ã€ç‰¹å¾å…·æœ‰å±€éƒ¨ç›¸å…³æ€§çš„å›å½’é—®é¢˜"
    }
}


# [ä¿ç•™åŸæœ‰çš„å­—ä½“è®¾ç½®å’Œæ·±åº¦å­¦ä¹ æ¨¡å‹æ„å»ºå‡½æ•°...]
def setup_chinese_font():
    """è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ"""
    system = platform.system()
    font_candidates = []
    if system == 'Windows':
        font_candidates = ['Microsoft YaHei', 'SimHei', 'SimSun', 'Arial Unicode MS']
    elif system == 'Darwin':  # macOS
        font_candidates = ['PingFang SC', 'Heiti SC', 'STHeiti', 'Apple LiGothic Medium']
    else:  # Linux and other systems
        font_candidates = ['Noto Sans CJK SC', 'WenQuanYi Micro Hei', 'Droid Sans Fallback']

    font_candidates.extend(['DejaVu Sans', 'Arial', 'Helvetica'])

    font_prop = None
    for font_name in font_candidates:
        try:
            font_path = fm.findfont(fm.FontProperties(family=font_name))
            if os.path.exists(font_path) and 'DejaVuSans' not in font_path:
                print(f"å­—ä½“æ—¥å¿—: ä½¿ç”¨å­—ä½“ '{font_name}' åœ¨è·¯å¾„: {font_path}")
                plt.rcParams['font.family'] = 'sans-serif'
                plt.rcParams['font.sans-serif'] = [font_name] + plt.rcParams['font.sans-serif']
                font_prop = fm.FontProperties(family=font_name)
                break
        except Exception as e:
            print(f"å­—ä½“æ—¥å¿—: å°è¯•å­—ä½“ {font_name} å¤±è´¥: {e}")

    if not font_prop:
        print("å­—ä½“æ—¥å¿—: æœªæ‰¾åˆ°åˆé€‚çš„ä¸­æ–‡å­—ä½“ï¼Œç»˜å›¾ä¸­çš„ä¸­æ–‡å¯èƒ½æ— æ³•æ­£å¸¸æ˜¾ç¤ºã€‚")

    plt.rcParams['axes.unicode_minus'] = False
    return font_prop


FONT_PROP = setup_chinese_font()


# [è¿™é‡Œä¿ç•™åŸæœ‰çš„æ·±åº¦å­¦ä¹ æ¨¡å‹æ„å»ºå‡½æ•°ï¼Œå› ä¸ºå®ƒä»¬å·²ç»å¾ˆå®Œå–„äº†]
def build_bp_neural_network_regressor(input_shape, params):
    """æ„å»ºBPç¥ç»ç½‘ç»œå›å½’æ¨¡å‹"""
    if not TENSORFLOW_AVAILABLE:
        raise ImportError("TensorFlowä¸å¯ç”¨ï¼Œæ— æ³•æ„å»ºæ·±åº¦å­¦ä¹ æ¨¡å‹")

    model = Sequential()

    # è§£æéšè—å±‚ç»“æ„
    try:
        hidden_units = [int(x.strip()) for x in params['hidden_layers'].split(',') if x.strip()]
    except:
        hidden_units = [128, 64, 32]

    # è¾“å…¥å±‚
    model.add(layers.Dense(hidden_units[0], activation=params['activation'], input_shape=(input_shape,)))
    if params['dropout_rate'] > 0:
        model.add(layers.Dropout(params['dropout_rate']))

    # éšè—å±‚
    for units in hidden_units[1:]:
        model.add(layers.Dense(units, activation=params['activation']))
        if params['dropout_rate'] > 0:
            model.add(layers.Dropout(params['dropout_rate']))

    # è¾“å‡ºå±‚
    model.add(layers.Dense(1))

    # ç¼–è¯‘æ¨¡å‹
    optimizer_map = {
        'adam': Adam(learning_rate=params['learning_rate']),
        'sgd': SGD(learning_rate=params['learning_rate']),
        'rmsprop': RMSprop(learning_rate=params['learning_rate'])
    }

    model.compile(optimizer=optimizer_map[params['optimizer']], loss='mse', metrics=['mae'])
    return model


# [ä¿ç•™å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹æ„å»ºå‡½æ•°...]

# --- æ–°å¢ï¼šå›å½’æ¨¡å‹è®­ç»ƒç®¡ç†å™¨ ---
class MultiRegressionTrainer:
    """å¤šæ¨¡å‹å›å½’è®­ç»ƒç®¡ç†å™¨"""

    def __init__(self):
        self.trainers = {}
        self.results = {}
        self.scaler = None

    def get_default_params(self, model_type):
        """è·å–æ¨¡å‹çš„é»˜è®¤å‚æ•°"""
        defaults = {
            'decision_tree': {'max_depth': 5, 'min_samples_split': 2, 'min_samples_leaf': 1,
                              'criterion': 'squared_error'},
            'random_forest': {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2},
            'gradient_boosting': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 3, 'subsample': 1.0},
            'catboost': {'iterations': 100, 'learning_rate': 0.1, 'depth': 6},
            'xgboost': {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 6, 'subsample': 1.0,
                        'colsample_bytree': 1.0},
            'lightgbm': {'n_estimators': 100, 'learning_rate': 0.1, 'num_leaves': 31, 'max_depth': -1},
            'extra_trees': {'n_estimators': 100, 'max_depth': None, 'min_samples_split': 2},
            'adaboost': {'n_estimators': 50, 'learning_rate': 1.0, 'loss': 'linear'},
            'linear': {},
            'ridge': {'alpha': 1.0},
            'lasso': {'alpha': 1.0},
            'elastic_net': {'alpha': 1.0, 'l1_ratio': 0.5},
            'svm': {'C': 1.0, 'kernel': 'rbf', 'gamma': 'scale', 'epsilon': 0.1},
            'knn': {'n_neighbors': 5, 'weights': 'uniform', 'algorithm': 'auto', 'p': 2},
            'neural_network': {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam', 'alpha': 0.0001,
                               'learning_rate_init': 0.001, 'max_iter': 200},
            'bp_neural_network': {
                'hidden_layers': '128,64,32', 'activation': 'relu', 'dropout_rate': 0.2,
                'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100, 'optimizer': 'adam',
                'early_stopping': True, 'patience': 10
            },
            'rnn': {
                'rnn_units': 64, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.001,
                'batch_size': 32, 'epochs': 100, 'optimizer': 'adam', 'sequence_length': 10,
                'early_stopping': True, 'patience': 10
            },
            'lstm': {
                'lstm_units': 64, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.001,
                'batch_size': 32, 'epochs': 100, 'optimizer': 'adam', 'sequence_length': 10,
                'early_stopping': True, 'patience': 10
            },
            'gru': {
                'gru_units': 64, 'num_layers': 2, 'dropout_rate': 0.2, 'learning_rate': 0.001,
                'batch_size': 32, 'epochs': 100, 'optimizer': 'adam', 'sequence_length': 10,
                'early_stopping': True, 'patience': 10
            },
            'cnn': {
                'conv_layers': '32,64,128', 'kernel_size': 3, 'pool_size': 2, 'dense_units': 128,
                'dropout_rate': 0.2, 'learning_rate': 0.001, 'batch_size': 32, 'epochs': 100,
                'optimizer': 'adam', 'early_stopping': True, 'patience': 10
            }
        }
        return defaults.get(model_type, {})

    def train_models(self, X, y, selected_models, model_params, groups=None,
                     normalize_features=True, test_size=0.2, progress_callback=None):
        """è®­ç»ƒå¤šä¸ªæ¨¡å‹"""
        # æ•°æ®é¢„å¤„ç† - æ›´ä¸¥æ ¼çš„æ•°æ®éªŒè¯
        print(f"åŸå§‹æ•°æ®å½¢çŠ¶: X={X.shape}, y={y.shape}")

        # è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œä½†ä¿ç•™åŸå§‹æ•°æ®ç»“æ„
        X_clean = X.copy()
        y_clean = y.copy()

        # æ›´å®‰å…¨çš„æ•°å€¼è½¬æ¢
        for col in X_clean.columns:
            if X_clean[col].dtype == 'object':
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ï¼Œå¤±è´¥çš„ä¿ç•™ä¸ºNaN
                X_clean[col] = pd.to_numeric(X_clean[col], errors='coerce')

        if y_clean.dtype == 'object':
            y_clean = pd.to_numeric(y_clean, errors='coerce')

        # æ£€æŸ¥è½¬æ¢åçš„æ•°æ®
        print(f"è½¬æ¢åæ•°æ®å½¢çŠ¶: X_clean={X_clean.shape}, y_clean={y_clean.shape}")
        print(f"X_cleanæ•°æ®ç±»å‹: {X_clean.dtypes}")
        print(f"y_cleanæ•°æ®ç±»å‹: {y_clean.dtype}")
        print(f"y_cleanç»Ÿè®¡: min={y_clean.min()}, max={y_clean.max()}, mean={y_clean.mean()}")

        # åˆå¹¶æ•°æ®ä»¥ä¾¿ä¸€èµ·åˆ é™¤NaNè¡Œ
        combined = pd.concat([X_clean, y_clean], axis=1)
        if groups is not None:
            combined = pd.concat([combined, groups], axis=1)

        # åˆ é™¤åŒ…å«NaNçš„è¡Œ
        initial_rows = len(combined)
        combined.dropna(inplace=True)
        final_rows = len(combined)

        print(f"æ•°æ®æ¸…æ´—: {initial_rows} -> {final_rows} è¡Œ (åˆ é™¤äº† {initial_rows - final_rows} è¡Œ)")

        if combined.empty:
            raise ValueError("æ¸…æ´—åæ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥æ•°æ®è´¨é‡")

        # åˆ†ç¦»æ¸…æ´—åçš„æ•°æ®
        X_processed = combined[X.columns]
        y_processed = combined[y.name]
        groups_processed = combined[groups.name] if groups is not None else None

        # å†æ¬¡éªŒè¯æ•°æ®
        if len(X_processed) == 0 or len(y_processed) == 0:
            raise ValueError("å¤„ç†åçš„æ•°æ®ä¸ºç©º")

        if y_processed.std() == 0:
            raise ValueError("ç›®æ ‡å˜é‡æ— å˜åŒ–ï¼ˆæ ‡å‡†å·®ä¸º0ï¼‰ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆçš„å›å½’åˆ†æ")

        # æ ‡å‡†åŒ–
        if normalize_features:
            self.scaler = StandardScaler()
            X_scaled = pd.DataFrame(
                self.scaler.fit_transform(X_processed),
                columns=X_processed.columns,
                index=X_processed.index
            )
            print("å·²åº”ç”¨ç‰¹å¾æ ‡å‡†åŒ–")
        else:
            X_scaled = X_processed
            self.scaler = None
            print("æœªä½¿ç”¨ç‰¹å¾æ ‡å‡†åŒ–")

        # æ•°æ®åˆ†å‰²
        X_train, X_test, y_train, y_test = train_test_split(
            X_scaled, y_processed, test_size=test_size, random_state=42
        )

        print(f"æ•°æ®åˆ†å‰²: è®­ç»ƒé›†{X_train.shape}, æµ‹è¯•é›†{X_test.shape}")

        # è®­ç»ƒæ¨¡å‹
        total_models = len(selected_models)
        for i, model_type in enumerate(selected_models):
            if progress_callback:
                progress_callback(int((i / total_models) * 100))

            try:
                # è·å–å‚æ•°
                params = model_params.get(model_type, self.get_default_params(model_type))

                # è®­ç»ƒæ¨¡å‹
                result = self._train_single_model(
                    X_train, X_test, y_train, y_test,
                    model_type, params, X_processed, y_processed, groups_processed
                )

                self.results[model_type] = result
                print(f"æ¨¡å‹ {model_type} è®­ç»ƒå®Œæˆï¼Œæµ‹è¯•RÂ²: {result.get('test_r2', 0):.4f}")

            except Exception as e:
                st.error(f"è®­ç»ƒæ¨¡å‹ {REGRESSOR_INFO[model_type]['name']} æ—¶å‡ºé”™: {str(e)}")
                print(f"æ¨¡å‹ {model_type} è®­ç»ƒå¤±è´¥: {str(e)}")
                self.results[model_type] = {'error': str(e)}

        if progress_callback:
            progress_callback(100)

        return self.results

    def _train_single_model(self, X_train, X_test, y_train, y_test,
                            model_type, params, X_full, y_full, groups):
        """è®­ç»ƒå•ä¸ªæ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        if model_type == 'decision_tree':
            model = DecisionTreeRegressor(**params, random_state=42)
        elif model_type == 'random_forest':
            model = RandomForestRegressor(**params, random_state=42, n_jobs=-1)
        elif model_type == 'gradient_boosting':
            model = GradientBoostingRegressor(**params, random_state=42)
        elif model_type == 'catboost':
            model = CatBoostRegressor(**params, verbose=0, random_state=42)
        elif model_type == 'xgboost':
            model = XGBRegressor(**params, random_state=42)
        elif model_type == 'lightgbm':
            model = LGBMRegressor(**params, random_state=42)
        elif model_type == 'extra_trees':
            model = ExtraTreesRegressor(**params, random_state=42, n_jobs=-1)
        elif model_type == 'adaboost':
            model = AdaBoostRegressor(**params, random_state=42)
        elif model_type == 'linear':
            model = LinearRegression()
        elif model_type == 'ridge':
            model = Ridge(**params, random_state=42)
        elif model_type == 'lasso':
            model = Lasso(**params, random_state=42)
        elif model_type == 'elastic_net':
            model = ElasticNet(**params, random_state=42)
        elif model_type == 'svm':
            model = SVR(**params)
        elif model_type == 'knn':
            model = KNeighborsRegressor(**params)
        elif model_type == 'neural_network':
            model = MLPRegressor(**params, random_state=42)
        elif model_type in ['bp_neural_network', 'rnn', 'lstm', 'gru', 'cnn']:
            return self._train_deep_learning_model(
                X_train, X_test, y_train, y_test, model_type, params, X_full, y_full
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {model_type}")

        # è®­ç»ƒæ¨¡å‹
        model.fit(X_train, y_train)

        # é¢„æµ‹
        train_pred = model.predict(X_train)
        test_pred = model.predict(X_test)

        # ä¿®å¤ï¼šæ­£ç¡®å¤„ç†å…¨æ•°æ®é›†é¢„æµ‹æ—¶çš„æ ‡å‡†åŒ–
        if self.scaler is not None:
            # å¦‚æœä½¿ç”¨äº†æ ‡å‡†åŒ–ï¼Œéœ€è¦å¯¹X_fullè¿›è¡Œæ ‡å‡†åŒ–åå†é¢„æµ‹
            X_full_scaled = pd.DataFrame(
                self.scaler.transform(X_full),
                columns=X_full.columns,
                index=X_full.index
            )
            full_pred = model.predict(X_full_scaled)
        else:
            # å¦‚æœæ²¡æœ‰ä½¿ç”¨æ ‡å‡†åŒ–ï¼Œç›´æ¥é¢„æµ‹
            full_pred = model.predict(X_full)

        # è®¡ç®—æŒ‡æ ‡
        train_mse = mean_squared_error(y_train, train_pred)
        test_mse = mean_squared_error(y_test, test_pred)
        train_r2 = r2_score(y_train, train_pred)
        test_r2 = r2_score(y_test, test_pred)
        train_mae = mean_absolute_error(y_train, train_pred)
        test_mae = mean_absolute_error(y_test, test_pred)

        # ç‰¹å¾é‡è¦æ€§
        feature_importance = {}
        try:
            if hasattr(model, 'feature_importances_'):
                feature_importance = dict(zip(X_train.columns, model.feature_importances_))
            elif hasattr(model, 'coef_'):
                feature_importance = dict(zip(X_train.columns, np.abs(model.coef_)))
        except:
            pass

        return {
            'model': model,
            'model_type': model_type,
            'params': params,
            'train_mse': train_mse,
            'test_mse': test_mse,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'feature_importance': feature_importance,
            'y_full': y_full,
            'pred_full': full_pred,
            'index_full': X_full.index
        }

    def _train_deep_learning_model(self, X_train, X_test, y_train, y_test,
                                   model_type, params, X_full, y_full):
        """è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹"""
        if not TENSORFLOW_AVAILABLE:
            raise ImportError("TensorFlowä¸å¯ç”¨")

        # æ•°æ®é¢„å¤„ç†ï¼ˆæ ¹æ®æ¨¡å‹ç±»å‹ï¼‰
        if model_type == 'bp_neural_network':
            X_train_processed = X_train.values.astype(np.float32)
            X_test_processed = X_test.values.astype(np.float32)
            input_shape = X_train_processed.shape[1]
            model = build_bp_neural_network_regressor(input_shape, params)
        # è¿™é‡Œå¯ä»¥æ·»åŠ å…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹çš„å¤„ç†é€»è¾‘

        y_train_processed = y_train.values.astype(np.float32)
        y_test_processed = y_test.values.astype(np.float32)

        # è®¾ç½®å›è°ƒ
        callbacks = []
        if params.get('early_stopping', True):
            early_stopping = EarlyStopping(
                monitor='val_loss',
                patience=params.get('patience', 10),
                restore_best_weights=True
            )
            callbacks.append(early_stopping)

        # è®­ç»ƒ
        model.fit(
            X_train_processed, y_train_processed,
            validation_data=(X_test_processed, y_test_processed),
            epochs=params.get('epochs', 100),
            batch_size=params.get('batch_size', 32),
            callbacks=callbacks,
            verbose=0
        )

        # é¢„æµ‹
        train_pred = model.predict(X_train_processed, verbose=0).flatten()
        test_pred = model.predict(X_test_processed, verbose=0).flatten()

        X_full_processed = X_full.values.astype(np.float32)
        if self.scaler is not None:
            X_full_processed = self.scaler.transform(X_full).astype(np.float32)
        full_pred = model.predict(X_full_processed, verbose=0).flatten()

        # è®¡ç®—æŒ‡æ ‡
        train_mse = mean_squared_error(y_train, train_pred)
        test_mse = mean_squared_error(y_test, test_pred)
        train_r2 = r2_score(y_train, train_pred)
        test_r2 = r2_score(y_test, test_pred)
        train_mae = mean_absolute_error(y_train, train_pred)
        test_mae = mean_absolute_error(y_test, test_pred)

        return {
            'model': model,
            'model_type': model_type,
            'params': params,
            'train_mse': train_mse,
            'test_mse': test_mse,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'train_mae': train_mae,
            'test_mae': test_mae,
            'feature_importance': {},  # æ·±åº¦å­¦ä¹ æ¨¡å‹æš‚ä¸æä¾›ç‰¹å¾é‡è¦æ€§
            'y_full': y_full,
            'pred_full': full_pred,
            'index_full': X_full.index
        }


# --- æ–°å¢ï¼šåˆå§‹åŒ–ä¼šè¯çŠ¶æ€ ---
def initialize_regression_session_state():
    """åˆå§‹åŒ–å›å½’é¡µé¢çš„ä¼šè¯çŠ¶æ€å˜é‡"""
    defaults = {
        'regression_data': None,
        'column_names': [],
        'selected_input_columns': [],
        'selected_output_column': None,
        'data_source_type': 'file',
        'file_names': None,
        'training_results_dict': {},
        'model_trained_flag': False,
        'normalize_features': True,
        'test_size': 0.2,
        'scaler': None,
        'selected_models': [],
        'model_params': {},
        'has_group_column': False,
        'selected_group_column': None,
        'multi_trainer': MultiRegressionTrainer(),
        'use_cv': False,
        'cv_folds': 5,
    }

    # æ·»åŠ æ‰€æœ‰æ¨¡å‹çš„é»˜è®¤å‚æ•°
    for model_type in REGRESSOR_INFO.keys():
        defaults[f'{model_type}_params'] = MultiRegressionTrainer().get_default_params(model_type)

    for key, value in defaults.items():
        if key not in st.session_state:
            st.session_state[key] = value


# --- æ–°å¢ï¼šä¸»é¡µé¢å‡½æ•° ---
def show_regression_training_page():
    """æ˜¾ç¤ºå›å½’è®­ç»ƒé¡µé¢"""
    initialize_regression_session_state()

    st.title("å›å½’æ¨¡å‹è®­ç»ƒ")
    st.markdown("---")

    # åˆ›å»ºé€‰é¡¹å¡
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ“ 1. æ•°æ®å¯¼å…¥",
        "ğŸ“Š 2. ç‰¹å¾é€‰æ‹©",
        "âš™ï¸ 3. æ¨¡å‹è®­ç»ƒ",
        "ğŸ“ˆ 4. ç»“æœå±•ç¤º"
    ])

    with tab1:
        create_data_import_section()

    with tab2:
        create_column_selection_section()

    with tab3:
        create_model_training_section()

    with tab4:
        create_results_section()


# --- æ–°å¢ï¼šUIåˆ›å»ºå‡½æ•° ---
def create_data_import_section():
    """åˆ›å»ºæ•°æ®å¯¼å…¥éƒ¨åˆ†UI"""
    st.header("æ•°æ®æºé€‰æ‹©")
    st.info("æ‚¨å¯ä»¥ä¸Šä¼ å•ä¸ªåŒ…å«ç‰¹å¾å’Œç›®æ ‡åˆ—çš„æ–‡ä»¶ï¼Œæˆ–è€…ä¸Šä¼ ä¸€ä¸ªæ–‡ä»¶å¤¹ï¼Œå…¶ä¸­æ¯ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ ·æœ¬ç»„ã€‚")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("ä¸Šä¼ æ–‡ä»¶")
        uploaded_file = st.file_uploader("é€‰æ‹© CSV æˆ– Excel æ–‡ä»¶", type=["csv", "xlsx", "xls"], key="reg_file_uploader")

        if uploaded_file:
            if st.button("åŠ è½½æ–‡ä»¶æ•°æ®", key="load_file_btn_reg"):
                with st.spinner(f"æ­£åœ¨åŠ è½½ {uploaded_file.name}..."):
                    try:
                        data = pd.read_csv(uploaded_file) if uploaded_file.name.lower().endswith(
                            '.csv') else pd.read_excel(uploaded_file)

                        # åŸºæœ¬æ¸…ç†
                        data.dropna(axis=1, how='all', inplace=True)

                        if data.empty:
                            st.error("ä¸Šä¼ çš„æ–‡ä»¶ä¸ºç©ºæˆ–ä¸åŒ…å«æœ‰æ•ˆæ•°æ®ã€‚")
                        else:
                            st.session_state.regression_data = data
                            st.session_state.column_names = list(data.columns)
                            st.session_state.data_source_type = "file"
                            st.session_state.file_names = None
                            # æ¸…é™¤æ—§ç»“æœ
                            st.session_state.training_results_dict = {}
                            st.session_state.model_trained_flag = False
                            st.session_state.selected_input_columns = []
                            st.session_state.selected_output_column = None

                            st.success(
                                f"å·²æˆåŠŸåŠ è½½: {uploaded_file.name} (åŒ…å« {len(data)} è¡Œ, {len(data.columns)} åˆ—)")
                            st.rerun()

                    except Exception as e:
                        st.error(f"åŠ è½½æ•°æ®æ—¶å‡ºé”™: {str(e)}")
                        st.session_state.regression_data = None

    with col2:
        st.subheader("ä¸Šä¼ æ–‡ä»¶å¤¹")
        folder_path = st.text_input("è¾“å…¥åŒ…å«æ•°æ®æ–‡ä»¶çš„æ–‡ä»¶å¤¹è·¯å¾„", key="reg_folder_path")

        if folder_path and os.path.isdir(folder_path):
            if st.button("åŠ è½½æ–‡ä»¶å¤¹æ•°æ®", key="load_folder_btn_reg"):
                folder_progress = st.progress(0)
                status_text = st.empty()

                def update_folder_progress(p):
                    folder_progress.progress(p / 100)
                    status_text.text(f"æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹... {p}%")

                with st.spinner("æ­£åœ¨å¤„ç†æ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶..."):
                    results, error_msg = process_folder_data(folder_path, progress_callback=update_folder_progress)
                    folder_progress.progress(100)
                    status_text.text("æ–‡ä»¶å¤¹å¤„ç†å®Œæˆã€‚")

                    if results:
                        st.session_state.regression_data = results
                        st.session_state.column_names = list(results['X'].columns)
                        st.session_state.data_source_type = "folder"
                        st.session_state.file_names = results['groups']
                        # æ¸…é™¤æ—§ç»“æœ
                        st.session_state.training_results_dict = {}
                        st.session_state.model_trained_flag = False
                        st.session_state.selected_input_columns = list(results['X'].columns)
                        st.session_state.selected_output_column = None

                        st.success(
                            f"æˆåŠŸåŠ è½½æ–‡ä»¶å¤¹æ•°æ®: {len(results['X'])} è¡Œ, {len(results['groups'].unique())} ä¸ªæ–‡ä»¶ç»„ã€‚")
                        st.rerun()
                    else:
                        st.error(f"å¤„ç†æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {error_msg}")

        elif folder_path:
            st.warning("è¾“å…¥çš„è·¯å¾„ä¸æ˜¯æœ‰æ•ˆçš„æ–‡ä»¶å¤¹ã€‚")

    # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
    if st.session_state.regression_data is not None:
        st.markdown("---")
        st.subheader("æ•°æ®é¢„è§ˆ (å‰5è¡Œ)")
        try:
            if st.session_state.data_source_type == "file":
                st.dataframe(st.session_state.regression_data.head())
            else:
                st.dataframe(st.session_state.regression_data['X'].head())
        except Exception as e:
            st.error(f"é¢„è§ˆæ•°æ®æ—¶å‡ºé”™: {e}")


def create_column_selection_section():
    """åˆ›å»ºåˆ—é€‰æ‹©éƒ¨åˆ†UI"""
    st.header("ç‰¹å¾å’Œç›®æ ‡åˆ—é€‰æ‹©")

    if st.session_state.regression_data is None:
        st.info("è¯·å…ˆåœ¨æ•°æ®å¯¼å…¥é€‰é¡¹å¡ä¸­åŠ è½½æ•°æ®ã€‚")
        return

    all_columns = st.session_state.column_names
    if not all_columns:
        st.warning("æœªèƒ½ä»åŠ è½½çš„æ•°æ®ä¸­è·å–åˆ—åã€‚")
        return

    st.info("è¯·é€‰æ‹©ç”¨äºæ¨¡å‹è®­ç»ƒçš„è¾“å…¥ç‰¹å¾åˆ—å’Œè¦é¢„æµ‹çš„ç›®æ ‡åˆ—ã€‚")

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("è¾“å…¥ç‰¹å¾ (X)")
        default_inputs = [col for col in st.session_state.selected_input_columns if col in all_columns]

        selected_inputs = st.multiselect(
            "é€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªè¾“å…¥ç‰¹å¾åˆ—",
            all_columns,
            default=default_inputs,
            key="input_col_multi_reg"
        )

        if selected_inputs:
            st.write(f"å·²é€‰æ‹© {len(selected_inputs)} ä¸ªè¾“å…¥ç‰¹å¾ã€‚")
        else:
            st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¾“å…¥ç‰¹å¾ã€‚")

    with col2:
        st.subheader("ç›®æ ‡åˆ— (Y)")
        output_options = [col for col in all_columns if col not in selected_inputs]
        current_output_index = 0
        if st.session_state.selected_output_column in output_options:
            current_output_index = output_options.index(st.session_state.selected_output_column) + 1

        selected_output = st.selectbox(
            "é€‰æ‹©ä¸€ä¸ªç›®æ ‡ï¼ˆé¢„æµ‹ï¼‰åˆ—",
            [None] + output_options,
            index=current_output_index,
            key="output_col_select_reg"
        )

        if selected_output:
            st.write(f"å·²é€‰æ‹© '{selected_output}' ä½œä¸ºç›®æ ‡åˆ—ã€‚")
        else:
            st.warning("è¯·é€‰æ‹©ä¸€ä¸ªç›®æ ‡åˆ—ã€‚")

    # ç¡®è®¤æŒ‰é’®
    if st.button("âœ… ç¡®è®¤ç‰¹å¾é€‰æ‹©", key="confirm_columns_reg_btn", use_container_width=True):
        st.session_state.selected_input_columns = selected_inputs
        st.session_state.selected_output_column = selected_output
        st.success("ç‰¹å¾å’Œç›®æ ‡åˆ—å·²ç¡®è®¤ï¼")
        time.sleep(0.5)

    st.markdown("---")
    st.subheader("æ•°æ®é¢„å¤„ç†é€‰é¡¹")
    col_prep1, col_prep2 = st.columns(2)

    with col_prep1:
        normalize_features = st.checkbox(
            "æ ‡å‡†åŒ–è¾“å…¥ç‰¹å¾ (StandardScaler, æ¨è)",
            value=st.session_state.normalize_features,
            key="normalize_cb_reg"
        )

    with col_prep2:
        test_size = st.slider(
            "æµ‹è¯•é›†æ¯”ä¾‹",
            min_value=0.1,
            max_value=0.5,
            value=st.session_state.test_size,
            step=0.05,
            help="ç”¨äºæ¨¡å‹è¯„ä¼°çš„æ•°æ®æ¯”ä¾‹ã€‚",
            key="test_size_slider_reg"
        )

    # ç¡®è®¤é¢„å¤„ç†è®¾ç½®
    if st.button("âœ… ç¡®è®¤é¢„å¤„ç†è®¾ç½®", key="confirm_preproc_reg_btn", use_container_width=True):
        st.session_state.normalize_features = normalize_features
        st.session_state.test_size = test_size
        st.success("é¢„å¤„ç†è®¾ç½®å·²ç¡®è®¤ï¼")
        time.sleep(0.5)


def create_model_training_section():
    """åˆ›å»ºæ¨¡å‹è®­ç»ƒé€‰é¡¹éƒ¨åˆ†UI"""
    st.header("æ¨¡å‹è®­ç»ƒé…ç½®")

    # å‰ç½®æ£€æŸ¥
    data_loaded = st.session_state.regression_data is not None
    features_selected = bool(st.session_state.selected_input_columns)
    target_selected = bool(st.session_state.selected_output_column)

    if not data_loaded:
        st.info("è¯·å…ˆåœ¨æ•°æ®å¯¼å…¥é€‰é¡¹å¡ä¸­åŠ è½½æ•°æ®ã€‚")
        return
    if not features_selected or not target_selected:
        st.warning("è¯·å…ˆåœ¨ç‰¹å¾é€‰æ‹©é€‰é¡¹å¡ä¸­é€‰æ‹©è¾“å…¥ç‰¹å¾å’Œç›®æ ‡åˆ—ã€‚")
        return

    # åˆå§‹åŒ–session state
    if 'selected_models' not in st.session_state:
        st.session_state.selected_models = []
    if 'model_params' not in st.session_state:
        st.session_state.model_params = {}
    if 'training_results_dict' not in st.session_state:
        st.session_state.training_results_dict = {}

    # æ¨¡å‹é€‰æ‹©å’Œé…ç½®éƒ¨åˆ†
    st.subheader("é€‰æ‹©å›å½’ç®—æ³•")

    # ä½¿ç”¨columnså¸ƒå±€
    col1, col2 = st.columns([1, 2])

    with col1:
        st.markdown("### å¯ç”¨ç®—æ³•åˆ—è¡¨")
        # æ˜¾ç¤ºæ‰€æœ‰å¯ç”¨çš„å›å½’å™¨
        for model_key, model_info in REGRESSOR_INFO.items():
            if st.checkbox(model_info['name'], key=f"check_{model_key}"):
                if model_key not in st.session_state.selected_models:
                    st.session_state.selected_models.append(model_key)
                    # åˆå§‹åŒ–è¯¥æ¨¡å‹çš„å‚æ•°
                    if model_key not in st.session_state.model_params:
                        st.session_state.model_params[model_key] = st.session_state.multi_trainer.get_default_params(
                            model_key)
            else:
                if model_key in st.session_state.selected_models:
                    st.session_state.selected_models.remove(model_key)
                    # æ¸…é™¤è¯¥æ¨¡å‹çš„å‚æ•°å’Œç»“æœ
                    if model_key in st.session_state.model_params:
                        del st.session_state.model_params[model_key]
                    if model_key in st.session_state.training_results_dict:
                        del st.session_state.training_results_dict[model_key]

    with col2:
        st.markdown("### æ¨¡å‹é…ç½®å’Œè¯´æ˜")

        if st.session_state.selected_models:
            # ä½¿ç”¨tabsæ˜¾ç¤ºæ¯ä¸ªé€‰ä¸­æ¨¡å‹çš„é…ç½®
            model_tabs = st.tabs([REGRESSOR_INFO[m]['name'] for m in st.session_state.selected_models])

            for i, (model_key, tab) in enumerate(zip(st.session_state.selected_models, model_tabs)):
                with tab:
                    # æ˜¾ç¤ºæ¨¡å‹è¯´æ˜
                    with st.expander("ç®—æ³•è¯´æ˜", expanded=True):
                        info = REGRESSOR_INFO[model_key]
                        st.markdown(f"**æè¿°ï¼š** {info['description']}")

                        col_adv, col_dis = st.columns(2)
                        with col_adv:
                            st.markdown("**ä¼˜ç‚¹ï¼š**")
                            for adv in info['advantages']:
                                st.markdown(f"â€¢ {adv}")

                        with col_dis:
                            st.markdown("**ç¼ºç‚¹ï¼š**")
                            for dis in info['disadvantages']:
                                st.markdown(f"â€¢ {dis}")

                        st.markdown(f"**é€‚ç”¨åœºæ™¯ï¼š** {info['suitable_for']}")

                    # æ˜¾ç¤ºå‚æ•°é…ç½®
                    st.markdown("#### å‚æ•°è®¾ç½®")
                    params = create_param_widgets(model_key, f"{model_key}_params")
                    st.session_state.model_params[model_key] = params
        else:
            st.info("è¯·ä»å·¦ä¾§é€‰æ‹©è‡³å°‘ä¸€ä¸ªå›å½’ç®—æ³•")

    # è®­ç»ƒé€‰é¡¹
    st.markdown("---")
    st.subheader("è®­ç»ƒé€‰é¡¹")

    col_opt1, col_opt2 = st.columns(2)
    with col_opt1:
        use_cv = st.checkbox("ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ€§èƒ½", value=st.session_state.get('use_cv', False))
        st.session_state.use_cv = use_cv

    with col_opt2:
        if use_cv:
            cv_folds = st.slider("äº¤å‰éªŒè¯æŠ˜æ•°", 2, 10, st.session_state.get('cv_folds', 5))
            st.session_state.cv_folds = cv_folds

    # è®­ç»ƒæŒ‰é’®
    st.markdown("---")
    if st.button("ğŸš€ å¼€å§‹è®­ç»ƒæ‰€é€‰æ¨¡å‹", type="primary", use_container_width=True):
        if not st.session_state.selected_models:
            st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªå›å½’ç®—æ³•")
            return

        # è®­ç»ƒæ‰€æœ‰é€‰ä¸­çš„æ¨¡å‹
        train_selected_models()

    # æ˜¾ç¤ºè®­ç»ƒè¿›åº¦å’Œç»“æœæ‘˜è¦
    if st.session_state.training_results_dict:
        st.markdown("---")
        st.subheader("è®­ç»ƒç»“æœæ‘˜è¦")
        display_results_summary()


def create_param_widgets(model_type, key_prefix):
    """ä¸ºä¸åŒæ¨¡å‹åˆ›å»ºå‚æ•°è¾“å…¥æ§ä»¶"""
    params = {}

    if model_type == "decision_tree":
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            params['max_depth'] = st.slider("æœ€å¤§æ·±åº¦ (0=æ— é™åˆ¶)", 0, 50, 5, 1, key=f"{key_prefix}_max_depth")
            if params['max_depth'] == 0:
                params['max_depth'] = None
        with col2:
            params['min_samples_split'] = st.slider("æœ€å°åˆ†è£‚æ ·æœ¬æ•°", 2, 20, 2, 1, key=f"{key_prefix}_min_split")
        with col3:
            params['min_samples_leaf'] = st.slider("å¶èŠ‚ç‚¹æœ€å°æ ·æœ¬æ•°", 1, 20, 1, 1, key=f"{key_prefix}_min_leaf")
        with col4:
            params['criterion'] = st.selectbox("åˆ†è£‚æ ‡å‡†", ["squared_error", "friedman_mse", "absolute_error"],
                                               key=f"{key_prefix}_criterion")

    elif model_type == "random_forest":
        col1, col2, col3 = st.columns(3)
        with col1:
            params['n_estimators'] = st.slider("æ ‘çš„æ•°é‡", 10, 1000, 100, 10, key=f"{key_prefix}_n_est")
        with col2:
            max_depth = st.slider("æœ€å¤§æ·±åº¦ (0=æ— é™åˆ¶)", 0, 50, 0, 1, key=f"{key_prefix}_max_depth")
            params['max_depth'] = None if max_depth == 0 else max_depth
        with col3:
            params['min_samples_split'] = st.slider("æœ€å°åˆ†è£‚æ ·æœ¬æ•°", 2, 20, 2, 1, key=f"{key_prefix}_min_split")

    elif model_type == "gradient_boosting":
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            params['n_estimators'] = st.slider("æ ‘çš„æ•°é‡", 50, 1000, 100, 10, key=f"{key_prefix}_n_est")
        with col2:
            params['learning_rate'] = st.slider("å­¦ä¹ ç‡", 0.001, 0.5, 0.1, 0.01, format="%.3f", key=f"{key_prefix}_lr")
        with col3:
            params['max_depth'] = st.slider("æœ€å¤§æ·±åº¦", 1, 20, 3, 1, key=f"{key_prefix}_max_depth")
        with col4:
            params['subsample'] = st.slider("å­æ ·æœ¬æ¯”ä¾‹", 0.1, 1.0, 1.0, 0.1, format="%.1f",
                                            key=f"{key_prefix}_subsample")

    elif model_type == "catboost":
        col1, col2, col3 = st.columns(3)
        with col1:
            params['iterations'] = st.slider("è¿­ä»£æ¬¡æ•°", 50, 2000, 100, 50, key=f"{key_prefix}_iter")
        with col2:
            params['learning_rate'] = st.slider("å­¦ä¹ ç‡", 0.001, 0.5, 0.1, 0.01, format="%.3f", key=f"{key_prefix}_lr")
        with col3:
            params['depth'] = st.slider("æ ‘æ·±åº¦", 1, 16, 6, 1, key=f"{key_prefix}_depth")

    elif model_type == "xgboost":
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            params['n_estimators'] = st.slider("æ ‘çš„æ•°é‡", 50, 1000, 100, 10, key=f"{key_prefix}_n_est")
        with col2:
            params['learning_rate'] = st.slider("å­¦ä¹ ç‡", 0.001, 0.5, 0.1, 0.01, format="%.3f", key=f"{key_prefix}_lr")
        with col3:
            params['max_depth'] = st.slider("æœ€å¤§æ·±åº¦", 1, 20, 6, 1, key=f"{key_prefix}_max_depth")
        with col4:
            params['subsample'] = st.slider("å­æ ·æœ¬æ¯”ä¾‹", 0.1, 1.0, 1.0, 0.1, format="%.1f",
                                            key=f"{key_prefix}_subsample")

    elif model_type == "lightgbm":
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            params['n_estimators'] = st.slider("æ ‘çš„æ•°é‡", 50, 1000, 100, 10, key=f"{key_prefix}_n_est")
        with col2:
            params['learning_rate'] = st.slider("å­¦ä¹ ç‡", 0.001, 0.5, 0.1, 0.01, format="%.3f", key=f"{key_prefix}_lr")
        with col3:
            params['num_leaves'] = st.slider("å¶å­æ•°é‡", 10, 300, 31, 5, key=f"{key_prefix}_num_leaves")
        with col4:
            max_depth = st.slider("æœ€å¤§æ·±åº¦ (-1=æ— é™åˆ¶)", -1, 20, -1, 1, key=f"{key_prefix}_max_depth")
            params['max_depth'] = max_depth

    elif model_type == "extra_trees":
        col1, col2, col3 = st.columns(3)
        with col1:
            params['n_estimators'] = st.slider("æ ‘çš„æ•°é‡", 10, 1000, 100, 10, key=f"{key_prefix}_n_est")
        with col2:
            max_depth = st.slider("æœ€å¤§æ·±åº¦ (0=æ— é™åˆ¶)", 0, 50, 0, 1, key=f"{key_prefix}_max_depth")
            params['max_depth'] = None if max_depth == 0 else max_depth
        with col3:
            params['min_samples_split'] = st.slider("æœ€å°åˆ†è£‚æ ·æœ¬æ•°", 2, 20, 2, 1, key=f"{key_prefix}_min_split")

    elif model_type == "adaboost":
        col1, col2, col3 = st.columns(3)
        with col1:
            params['n_estimators'] = st.slider("å¼±å­¦ä¹ å™¨æ•°é‡", 10, 500, 50, 10, key=f"{key_prefix}_n_est")
        with col2:
            params['learning_rate'] = st.slider("å­¦ä¹ ç‡", 0.1, 2.0, 1.0, 0.1, format="%.1f", key=f"{key_prefix}_lr")
        with col3:
            params['loss'] = st.selectbox("æŸå¤±å‡½æ•°", ["linear", "square", "exponential"], key=f"{key_prefix}_loss")

    elif model_type == "linear":
        st.info("çº¿æ€§å›å½’æ²¡æœ‰éœ€è¦è°ƒæ•´çš„è¶…å‚æ•°")
        params = {}

    elif model_type == "ridge":
        col1, col2 = st.columns(2)
        with col1:
            params['alpha'] = st.slider("æ­£åˆ™åŒ–å‚æ•° Alpha", 0.1, 100.0, 1.0, 0.1, format="%.1f",
                                        key=f"{key_prefix}_alpha")
        with col2:
            params['solver'] = st.selectbox("æ±‚è§£å™¨", ["auto", "svd", "cholesky", "lsqr", "sparse_cg", "sag", "saga"],
                                            key=f"{key_prefix}_solver")

    elif model_type == "lasso":
        col1, col2 = st.columns(2)
        with col1:
            params['alpha'] = st.slider("æ­£åˆ™åŒ–å‚æ•° Alpha", 0.1, 100.0, 1.0, 0.1, format="%.1f",
                                        key=f"{key_prefix}_alpha")
        with col2:
            params['max_iter'] = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 100, 5000, 1000, 100, key=f"{key_prefix}_max_iter")

    elif model_type == "elastic_net":
        col1, col2, col3 = st.columns(3)
        with col1:
            params['alpha'] = st.slider("æ­£åˆ™åŒ–å‚æ•° Alpha", 0.1, 100.0, 1.0, 0.1, format="%.1f",
                                        key=f"{key_prefix}_alpha")
        with col2:
            params['l1_ratio'] = st.slider("L1æ¯”ä¾‹", 0.0, 1.0, 0.5, 0.1, format="%.1f", key=f"{key_prefix}_l1_ratio")
        with col3:
            params['max_iter'] = st.slider("æœ€å¤§è¿­ä»£æ¬¡æ•°", 100, 5000, 1000, 100, key=f"{key_prefix}_max_iter")

    elif model_type == "svm":
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            params['C'] = st.slider("æ­£åˆ™åŒ–å‚æ•° C", 0.1, 100.0, 1.0, 0.1, format="%.1f", key=f"{key_prefix}_c")
        with col2:
            params['kernel'] = st.selectbox("æ ¸å‡½æ•°", ["rbf", "linear", "poly", "sigmoid"], key=f"{key_prefix}_kernel")
        with col3:
            gamma_options = ["scale", "auto", 0.001, 0.01, 0.1, 1.0]
            params['gamma'] = st.selectbox("Gamma", gamma_options, key=f"{key_prefix}_gamma")
        with col4:
            params['epsilon'] = st.slider("Epsilon", 0.01, 1.0, 0.1, 0.01, format="%.2f", key=f"{key_prefix}_epsilon")

    elif model_type == "knn":
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            params['n_neighbors'] = st.slider("é‚»å±…æ•°é‡ K", 1, 50, 5, 1, key=f"{key_prefix}_k")
        with col2:
            params['weights'] = st.selectbox("æƒé‡", ["uniform", "distance"], key=f"{key_prefix}_weights")
        with col3:
            params['algorithm'] = st.selectbox("ç®—æ³•", ["auto", "ball_tree", "kd_tree", "brute"],
                                               key=f"{key_prefix}_algorithm")
        with col4:
            params['p'] = st.slider("è·ç¦»åº¦é‡å‚æ•°", 1, 5, 2, 1, key=f"{key_prefix}_p")

    elif model_type == "neural_network":
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            hidden_size = st.slider("éšè—å±‚å¤§å°", 10, 500, 100, 10, key=f"{key_prefix}_hidden_size")
            params['hidden_layer_sizes'] = (hidden_size,)
        with col2:
            params['activation'] = st.selectbox("æ¿€æ´»å‡½æ•°", ["relu", "tanh", "logistic"],
                                                key=f"{key_prefix}_activation")
        with col3:
            params['solver'] = st.selectbox("æ±‚è§£å™¨", ["adam", "lbfgs", "sgd"], key=f"{key_prefix}_solver")
        with col4:
            params['alpha'] = st.slider("æ­£åˆ™åŒ–å‚æ•°", 0.0001, 0.1, 0.0001, 0.0001, format="%.4f",
                                        key=f"{key_prefix}_alpha")

    elif model_type == "bp_neural_network":
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            params['hidden_layers'] = st.text_input("éšè—å±‚ç»“æ„ (é€—å·åˆ†éš”)", "128,64,32",
                                                    key=f"{key_prefix}_hidden_layers")
        with col2:
            params['activation'] = st.selectbox("æ¿€æ´»å‡½æ•°", ["relu", "tanh", "sigmoid"], key=f"{key_prefix}_activation")
        with col3:
            params['dropout_rate'] = st.slider("Dropoutç‡", 0.0, 0.8, 0.2, 0.1, format="%.1f",
                                               key=f"{key_prefix}_dropout")
        with col4:
            params['learning_rate'] = st.slider("å­¦ä¹ ç‡", 0.0001, 0.1, 0.001, 0.0001, format="%.4f",
                                                key=f"{key_prefix}_lr")

        col5, col6, col7, col8 = st.columns(4)
        with col5:
            params['batch_size'] = st.slider("æ‰¹æ¬¡å¤§å°", 8, 256, 32, 8, key=f"{key_prefix}_batch_size")
        with col6:
            params['epochs'] = st.slider("è®­ç»ƒè½®æ•°", 10, 500, 100, 10, key=f"{key_prefix}_epochs")
        with col7:
            params['optimizer'] = st.selectbox("ä¼˜åŒ–å™¨", ["adam", "sgd", "rmsprop"], key=f"{key_prefix}_optimizer")
        with col8:
            params['early_stopping'] = st.checkbox("æ—©åœ", True, key=f"{key_prefix}_early_stopping")

    # å¯¹äºå…¶ä»–æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œå¯ä»¥ç±»ä¼¼å¤„ç†
    elif model_type in ["rnn", "lstm", "gru", "cnn"]:
        st.info(f"{REGRESSOR_INFO[model_type]['name']} ä½¿ç”¨é»˜è®¤å‚æ•°")
        params = {}

    else:
        st.warning(f"æœªå®šä¹‰å‚æ•°æ§ä»¶çš„æ¨¡å‹ç±»å‹: {model_type}")
        params = {}

    return params


def train_selected_models():
    """è®­ç»ƒæ‰€æœ‰é€‰ä¸­çš„æ¨¡å‹"""
    # å‡†å¤‡æ•°æ®
    if st.session_state.data_source_type == "file":
        data_source = st.session_state.regression_data
        X = data_source[st.session_state.selected_input_columns].copy()
        y = data_source[st.session_state.selected_output_column].copy()
        groups = None
        if st.session_state.has_group_column and st.session_state.selected_group_column:
            groups = data_source[st.session_state.selected_group_column].copy()
    else:
        # ä¿®å¤ï¼šæ–‡ä»¶å¤¹æ•°æ®å¤„ç†æ—¶æ­£ç¡®è·å–ç›®æ ‡å˜é‡
        data_dict = st.session_state.regression_data
        X_data = data_dict['X']

        # æ£€æŸ¥ç›®æ ‡åˆ—æ˜¯å¦å­˜åœ¨
        if st.session_state.selected_output_column not in X_data.columns:
            st.error(f"ç›®æ ‡åˆ— '{st.session_state.selected_output_column}' ä¸å­˜åœ¨äºæ•°æ®ä¸­")
            return

        X = X_data[st.session_state.selected_input_columns].copy()
        y = X_data[st.session_state.selected_output_column].copy()  # ä¿®å¤ï¼šä»X_dataè·å–y
        groups = data_dict['groups'].copy()

    # æ•°æ®éªŒè¯
    if X.empty or y.empty:
        st.error("è¾“å…¥æ•°æ®æˆ–ç›®æ ‡æ•°æ®ä¸ºç©º")
        return

    if len(X) != len(y):
        st.error(f"è¾“å…¥æ•°æ®é•¿åº¦({len(X)})ä¸ç›®æ ‡æ•°æ®é•¿åº¦({len(y)})ä¸åŒ¹é…")
        return

    # åˆ›å»ºè¿›åº¦æ¡
    progress_bar = st.progress(0)
    status_text = st.empty()

    def update_progress(p):
        progress_bar.progress(p / 100)
        status_text.text(f"æ­£åœ¨è®­ç»ƒæ¨¡å‹... {p}%")

    try:
        # è®­ç»ƒæ‰€æœ‰é€‰ä¸­çš„æ¨¡å‹
        results = st.session_state.multi_trainer.train_models(
            X, y,
            st.session_state.selected_models,
            st.session_state.model_params,
            groups=groups,
            normalize_features=st.session_state.normalize_features,
            test_size=st.session_state.test_size,
            progress_callback=update_progress
        )

        st.session_state.training_results_dict = results
        st.session_state.model_trained_flag = True

        progress_bar.progress(100)
        status_text.text("æ‰€æœ‰æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        st.success("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")

    except Exception as e:
        st.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {str(e)}")
        progress_bar.progress(0)
        status_text.text("è®­ç»ƒå¤±è´¥")


def display_results_summary():
    """æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çš„ç»“æœæ‘˜è¦"""
    # åˆ›å»ºç»“æœæ¯”è¾ƒè¡¨æ ¼
    results_data = []
    for model_type, results in st.session_state.training_results_dict.items():
        if 'error' not in results:
            row = {
                'æ¨¡å‹': REGRESSOR_INFO[model_type]['name'],
                'è®­ç»ƒRÂ²': f"{results.get('train_r2', 0):.4f}",
                'æµ‹è¯•RÂ²': f"{results.get('test_r2', 0):.4f}",
                'è®­ç»ƒMSE': f"{results.get('train_mse', 0):.4f}",
                'æµ‹è¯•MSE': f"{results.get('test_mse', 0):.4f}",
                'æµ‹è¯•MAE': f"{results.get('test_mae', 0):.4f}"
            }
            results_data.append(row)

    if results_data:
        df = pd.DataFrame(results_data)
        st.dataframe(df, use_container_width=True, hide_index=True)

        # æ‰¾å‡ºæœ€ä½³æ¨¡å‹
        best_model_idx = df['æµ‹è¯•RÂ²'].apply(lambda x: float(x)).idxmax()
        best_model = df.iloc[best_model_idx]['æ¨¡å‹']
        st.success(f"ğŸ† æœ€ä½³æ¨¡å‹ï¼ˆæŒ‰æµ‹è¯•RÂ²ï¼‰ï¼š{best_model}")


def create_results_section():
    """åˆ›å»ºç»“æœå±•ç¤ºéƒ¨åˆ†UI"""
    st.header("ç»“æœå±•ç¤ºä¸åˆ†æ")

    if not st.session_state.model_trained_flag or not st.session_state.training_results_dict:
        st.info("è¯·å…ˆåœ¨æ¨¡å‹è®­ç»ƒé€‰é¡¹å¡ä¸­è®­ç»ƒæ¨¡å‹ä»¥æŸ¥çœ‹ç»“æœã€‚")
        return

    # åˆ›å»ºå­é€‰é¡¹å¡
    tab1, tab2, tab3, tab4, tab5 = st.tabs([
        "ğŸ“Š æ¨¡å‹æ¯”è¾ƒ", "ğŸ“ˆ è¯¦ç»†ç»“æœ", "ğŸ” ç‰¹å¾åˆ†æ", "ğŸ“‰ å¯è§†åŒ–", "ğŸ’¾ å¯¼å‡º"
    ])

    with tab1:
        create_model_comparison_tab()

    with tab2:
        create_detailed_results_tab()

    with tab3:
        create_feature_analysis_tab()

    with tab4:
        create_visualization_tab()

    with tab5:
        create_export_tab()


def create_model_comparison_tab():
    """åˆ›å»ºæ¨¡å‹æ¯”è¾ƒé€‰é¡¹å¡"""
    st.subheader("æ¨¡å‹æ€§èƒ½æ¯”è¾ƒ")

    # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„æŒ‡æ ‡
    comparison_data = []
    for model_type, results in st.session_state.training_results_dict.items():
        if 'error' not in results:
            metrics = {
                'æ¨¡å‹': REGRESSOR_INFO[model_type]['name'],
                'è®­ç»ƒRÂ²': results.get('train_r2', 0),
                'æµ‹è¯•RÂ²': results.get('test_r2', 0),
                'è®­ç»ƒMSE': results.get('train_mse', 0),
                'æµ‹è¯•MSE': results.get('test_mse', 0),
                'è®­ç»ƒMAE': results.get('train_mae', 0),
                'æµ‹è¯•MAE': results.get('test_mae', 0)
            }
            comparison_data.append(metrics)

    if comparison_data:
        df = pd.DataFrame(comparison_data)

        # åˆ›å»ºæ¯”è¾ƒå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # RÂ²æ¯”è¾ƒ
        ax = axes[0, 0]
        x = range(len(df))
        width = 0.35
        ax.bar([i - width / 2 for i in x], df['è®­ç»ƒRÂ²'], width, label='è®­ç»ƒé›†', alpha=0.8, color='#3498db')
        ax.bar([i + width / 2 for i in x], df['æµ‹è¯•RÂ²'], width, label='æµ‹è¯•é›†', alpha=0.8, color='#e74c3c')
        ax.set_xlabel('æ¨¡å‹')
        ax.set_ylabel('RÂ² åˆ†æ•°')
        ax.set_title('RÂ² åˆ†æ•°æ¯”è¾ƒ')
        ax.set_xticks(x)
        ax.set_xticklabels(df['æ¨¡å‹'], rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # MSEæ¯”è¾ƒ
        ax = axes[0, 1]
        ax.bar([i - width / 2 for i in x], df['è®­ç»ƒMSE'], width, label='è®­ç»ƒé›†', alpha=0.8, color='#3498db')
        ax.bar([i + width / 2 for i in x], df['æµ‹è¯•MSE'], width, label='æµ‹è¯•é›†', alpha=0.8, color='#e74c3c')
        ax.set_xlabel('æ¨¡å‹')
        ax.set_ylabel('å‡æ–¹è¯¯å·® (MSE)')
        ax.set_title('MSEæ¯”è¾ƒ')
        ax.set_xticks(x)
        ax.set_xticklabels(df['æ¨¡å‹'], rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)

        # é›·è¾¾å›¾ - æµ‹è¯•é›†æŒ‡æ ‡
        ax = axes[1, 0]
        categories = ['RÂ²', 'MSE', 'MAE']
        angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()
        angles += angles[:1]

        ax = plt.subplot(2, 2, 3, projection='polar')
        for idx, row in df.iterrows():
            # æ ‡å‡†åŒ–æŒ‡æ ‡ç”¨äºé›·è¾¾å›¾
            r2_norm = row['æµ‹è¯•RÂ²']
            mse_norm = 1 / (1 + row['æµ‹è¯•MSE'])  # MSEè¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºè¶Šå¤§è¶Šå¥½
            mae_norm = 1 / (1 + row['æµ‹è¯•MAE'])  # MAEè¶Šå°è¶Šå¥½ï¼Œè½¬æ¢ä¸ºè¶Šå¤§è¶Šå¥½

            values = [r2_norm, mse_norm, mae_norm]
            values += values[:1]
            ax.plot(angles, values, 'o-', linewidth=2, label=row['æ¨¡å‹'])
            ax.fill(angles, values, alpha=0.15)

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(categories)
        ax.set_title('æµ‹è¯•é›†æ€§èƒ½é›·è¾¾å›¾')
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))
        ax.grid(True)

        # æ•£ç‚¹å›¾ - è®­ç»ƒvsæµ‹è¯•RÂ²
        ax = axes[1, 1]
        colors = plt.cm.Set3(np.linspace(0, 1, len(df)))
        for idx, row in df.iterrows():
            ax.scatter(row['è®­ç»ƒRÂ²'], row['æµ‹è¯•RÂ²'],
                       s=200, c=[colors[idx]], alpha=0.6, edgecolors='black', linewidth=2)
            ax.annotate(row['æ¨¡å‹'], (row['è®­ç»ƒRÂ²'], row['æµ‹è¯•RÂ²']),
                        xytext=(5, 5), textcoords='offset points', fontsize=8)

        # æ·»åŠ å¯¹è§’çº¿
        max_r2 = max(df['è®­ç»ƒRÂ²'].max(), df['æµ‹è¯•RÂ²'].max())
        min_r2 = min(df['è®­ç»ƒRÂ²'].min(), df['æµ‹è¯•RÂ²'].min())
        ax.plot([min_r2, max_r2], [min_r2, max_r2], 'k--', alpha=0.5)
        ax.set_xlabel('è®­ç»ƒRÂ²')
        ax.set_ylabel('æµ‹è¯•RÂ²')
        ax.set_title('è®­ç»ƒvsæµ‹è¯•RÂ²')
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        st.pyplot(fig)

        # æ˜¾ç¤ºè¯¦ç»†æ¯”è¾ƒè¡¨æ ¼
        st.markdown("### è¯¦ç»†æŒ‡æ ‡å¯¹æ¯”")
        st.dataframe(df.style.highlight_max(axis=0, subset=[col for col in df.columns if col != 'æ¨¡å‹']),
                     use_container_width=True)


def create_detailed_results_tab():
    """åˆ›å»ºè¯¦ç»†ç»“æœé€‰é¡¹å¡"""
    st.subheader("æ¨¡å‹è¯¦ç»†ç»“æœ")

    # é€‰æ‹©è¦æŸ¥çœ‹çš„æ¨¡å‹
    available_models = [model_type for model_type, results in st.session_state.training_results_dict.items()
                        if 'error' not in results]

    if not available_models:
        st.warning("æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒç»“æœ")
        return

    selected_model = st.selectbox(
        "é€‰æ‹©è¦æŸ¥çœ‹è¯¦ç»†ç»“æœçš„æ¨¡å‹",
        available_models,
        format_func=lambda x: REGRESSOR_INFO[x]['name']
    )

    if selected_model:
        results = st.session_state.training_results_dict[selected_model]

        # æ˜¾ç¤ºæ€§èƒ½æŒ‡æ ‡
        col1, col2 = st.columns(2)

        with col1:
            st.markdown("### è®­ç»ƒé›†æŒ‡æ ‡")
            metrics_train = {
                "RÂ² åˆ†æ•°": f"{results.get('train_r2', 0):.4f}",
                "å‡æ–¹è¯¯å·® (MSE)": f"{results.get('train_mse', 0):.4f}",
                "å¹³å‡ç»å¯¹è¯¯å·® (MAE)": f"{results.get('train_mae', 0):.4f}"
            }
            for metric, value in metrics_train.items():
                st.metric(metric, value)

        with col2:
            st.markdown("### æµ‹è¯•é›†æŒ‡æ ‡")
            metrics_test = {
                "RÂ² åˆ†æ•°": f"{results.get('test_r2', 0):.4f}",
                "å‡æ–¹è¯¯å·® (MSE)": f"{results.get('test_mse', 0):.4f}",
                "å¹³å‡ç»å¯¹è¯¯å·® (MAE)": f"{results.get('test_mae', 0):.4f}"
            }
            for metric, value in metrics_test.items():
                st.metric(metric, value)

        # æ˜¾ç¤ºæ¨¡å‹å‚æ•° - æ”¹è¿›çš„å¤„ç†
        st.markdown("### æ¨¡å‹å‚æ•°")
        if 'params' in results and results['params']:
            # å°†å‚æ•°è½¬æ¢ä¸ºæ›´å¥½çš„æ˜¾ç¤ºæ ¼å¼
            params_to_display = {}
            for key, value in results['params'].items():
                if value is None:
                    params_to_display[key] = "None"
                elif isinstance(value, float):
                    params_to_display[key] = f"{value:.4f}"
                else:
                    params_to_display[key] = str(value)

            params_df = pd.DataFrame(list(params_to_display.items()), columns=['å‚æ•°', 'å€¼'])
            st.dataframe(params_df, hide_index=True, use_container_width=True)
        else:
            # å¦‚æœæ²¡æœ‰è®­ç»ƒæ—¶çš„å‚æ•°ï¼Œæ˜¾ç¤ºé»˜è®¤å‚æ•°
            default_params = st.session_state.multi_trainer.get_default_params(selected_model)
            if default_params:
                st.info("æ˜¾ç¤ºé»˜è®¤å‚æ•°ï¼ˆè®­ç»ƒæ—¶å¯èƒ½ä½¿ç”¨äº†è‡ªå®šä¹‰å‚æ•°ï¼‰")
                params_to_display = {}
                for key, value in default_params.items():
                    if value is None:
                        params_to_display[key] = "None"
                    elif isinstance(value, float):
                        params_to_display[key] = f"{value:.4f}"
                    else:
                        params_to_display[key] = str(value)

                params_df = pd.DataFrame(list(params_to_display.items()), columns=['å‚æ•°', 'å€¼'])
                st.dataframe(params_df, hide_index=True, use_container_width=True)
            else:
                st.info("è¯¥æ¨¡å‹æ²¡æœ‰å¯è°ƒå‚æ•°æˆ–ä½¿ç”¨é»˜è®¤è®¾ç½®")

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        model_info = REGRESSOR_INFO.get(selected_model, {})
        if model_info:
            with st.expander("æ¨¡å‹è¯´æ˜", expanded=False):
                st.markdown(f"**æè¿°ï¼š** {model_info.get('description', 'æ— æè¿°')}")
                st.markdown(f"**é€‚ç”¨åœºæ™¯ï¼š** {model_info.get('suitable_for', 'æ— è¯´æ˜')}")


def create_feature_analysis_tab():
    """åˆ›å»ºç‰¹å¾åˆ†æé€‰é¡¹å¡"""
    st.subheader("ç‰¹å¾é‡è¦æ€§åˆ†æ")

    # æ”¶é›†æœ‰ç‰¹å¾é‡è¦æ€§çš„æ¨¡å‹
    models_with_importance = []
    for model_type, results in st.session_state.training_results_dict.items():
        if 'error' not in results and results.get('feature_importance'):
            models_with_importance.append(model_type)

    if not models_with_importance:
        st.info("å½“å‰è®­ç»ƒçš„æ¨¡å‹ä¸­æ²¡æœ‰æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æçš„æ¨¡å‹")
        st.warning("å†³ç­–æ ‘ã€éšæœºæ£®æ—ã€æ¢¯åº¦æå‡ç­‰æ¨¡å‹æ”¯æŒç‰¹å¾é‡è¦æ€§åˆ†æ")
        return

    # é€‰æ‹©æ¨¡å‹
    selected_model = st.selectbox(
        "é€‰æ‹©æ¨¡å‹æŸ¥çœ‹ç‰¹å¾é‡è¦æ€§",
        models_with_importance,
        format_func=lambda x: REGRESSOR_INFO[x]['name']
    )

    if selected_model:
        results = st.session_state.training_results_dict[selected_model]
        feature_importance = results['feature_importance']

        # ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾
        fig = plot_feature_importance(feature_importance, top_n=20)
        st.pyplot(fig)

        # æ˜¾ç¤ºç‰¹å¾é‡è¦æ€§è¡¨æ ¼
        st.markdown("### ç‰¹å¾é‡è¦æ€§è¯¦ç»†æ•°æ®")
        importance_df = pd.DataFrame({
            'ç‰¹å¾': list(feature_importance.keys()),
            'é‡è¦æ€§': list(feature_importance.values())
        }).sort_values(by='é‡è¦æ€§', ascending=False)

        st.dataframe(importance_df, use_container_width=True, hide_index=True)

        # ä¸‹è½½ç‰¹å¾é‡è¦æ€§æ•°æ®
        csv = importance_df.to_csv(index=False)
        st.download_button(
            label="ä¸‹è½½ç‰¹å¾é‡è¦æ€§æ•°æ® (CSV)",
            data=csv,
            file_name=f"feature_importance_{selected_model}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
            mime='text/csv'
        )


def create_visualization_tab():
    """åˆ›å»ºå¯è§†åŒ–é€‰é¡¹å¡"""
    st.subheader("æ¨¡å‹å¯è§†åŒ–åˆ†æ")

    # é€‰æ‹©æ¨¡å‹
    available_models = [model_type for model_type, results in st.session_state.training_results_dict.items()
                        if 'error' not in results]

    if not available_models:
        st.warning("æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹ç»“æœ")
        return

    selected_model = st.selectbox(
        "é€‰æ‹©è¦å¯è§†åŒ–çš„æ¨¡å‹",
        available_models,
        format_func=lambda x: REGRESSOR_INFO[x]['name'],
        key="viz_model_select"
    )

    if selected_model:
        results = st.session_state.training_results_dict[selected_model]

        # è·å–æ•°æ®
        y_full = results.get('y_full')
        pred_full = results.get('pred_full')
        index_full = results.get('index_full')

        # é€‰æ‹©å¯è§†åŒ–ç±»å‹
        viz_options = ["é¢„æµ‹å€¼vsçœŸå®å€¼", "æ®‹å·®åˆ†æ"]

        viz_type = st.selectbox("é€‰æ‹©å¯è§†åŒ–ç±»å‹", viz_options, key="viz_type_select")

        # ç”Ÿæˆå¯è§†åŒ–
        try:
            if viz_type == "é¢„æµ‹å€¼vsçœŸå®å€¼":
                fig = plot_training_results(y_full, pred_full, index_full)
            elif viz_type == "æ®‹å·®åˆ†æ":
                fig = plot_residuals(y_full, pred_full, index_full)

            st.pyplot(fig)

            # æä¾›ä¸‹è½½é€‰é¡¹
            buf = BytesIO()
            fig.savefig(buf, format='png', dpi=150, bbox_inches='tight')
            buf.seek(0)
            st.download_button(
                label=f"ä¸‹è½½{viz_type}å›¾ç‰‡",
                data=buf,
                file_name=f"{viz_type}_{selected_model}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png",
                mime='image/png'
            )
            plt.close(fig)

        except Exception as e:
            st.error(f"ç”Ÿæˆå¯è§†åŒ–æ—¶å‡ºé”™: {str(e)}")


def create_export_tab():
    """åˆ›å»ºæ¨¡å‹å¯¼å‡ºé€‰é¡¹å¡"""
    st.subheader("æ¨¡å‹å¯¼å‡ºä¸éƒ¨ç½²")

    # é€‰æ‹©è¦å¯¼å‡ºçš„æ¨¡å‹
    available_models = [model_type for model_type, results in st.session_state.training_results_dict.items()
                        if 'error' not in results]

    if not available_models:
        st.warning("æ²¡æœ‰å¯å¯¼å‡ºçš„æ¨¡å‹")
        return

    selected_model = st.selectbox(
        "é€‰æ‹©è¦å¯¼å‡ºçš„æ¨¡å‹",
        available_models,
        format_func=lambda x: REGRESSOR_INFO[x]['name'],
        key="export_model_select"
    )

    if selected_model:
        results = st.session_state.training_results_dict[selected_model]

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        st.info(f"""
        **æ¨¡å‹ç±»å‹**: {REGRESSOR_INFO[selected_model]['name']}  
        **æµ‹è¯•RÂ²**: {results.get('test_r2', 0):.4f}  
        **æµ‹è¯•MSE**: {results.get('test_mse', 0):.4f}
        """)

        # å¯¼å‡ºé€‰é¡¹
        col1, col2 = st.columns(2)

        with col1:
            # å¯¼å‡ºæ¨¡å‹æ–‡ä»¶
            if st.button("å¯¼å‡ºæ¨¡å‹æ–‡ä»¶ (.joblib)", key="export_joblib"):
                try:
                    # å‡†å¤‡å¯¼å‡ºæ•°æ®
                    export_data = {
                        'model': results['model'],
                        'model_type': selected_model,
                        'feature_names': st.session_state.selected_input_columns,
                        'scaler': st.session_state.multi_trainer.scaler,
                        'params': results.get('params'),
                        'metrics': {
                            'test_r2': results.get('test_r2'),
                            'test_mse': results.get('test_mse'),
                            'test_mae': results.get('test_mae')
                        },
                        'export_time': datetime.now().isoformat()
                    }

                    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
                    temp_file = BytesIO()
                    joblib.dump(export_data, temp_file)
                    temp_file.seek(0)

                    # æä¾›ä¸‹è½½
                    st.download_button(
                        label="ä¸‹è½½æ¨¡å‹æ–‡ä»¶",
                        data=temp_file,
                        file_name=f"model_{selected_model}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.joblib",
                        mime='application/octet-stream'
                    )
                    st.success("æ¨¡å‹å·²å‡†å¤‡å¥½ä¸‹è½½ï¼")

                except Exception as e:
                    st.error(f"å¯¼å‡ºæ¨¡å‹æ—¶å‡ºé”™: {str(e)}")

        with col2:
            # å¯¼å‡ºé¢„æµ‹ç»“æœ
            if st.button("å¯¼å‡ºé¢„æµ‹ç»“æœ (.csv)", key="export_predictions"):
                try:
                    # åˆ›å»ºé¢„æµ‹ç»“æœDataFrame
                    pred_df = pd.DataFrame({
                        'çœŸå®å€¼': results['y_full'],
                        'é¢„æµ‹å€¼': results['pred_full']
                    })

                    # è½¬æ¢ä¸ºCSV
                    csv = pred_df.to_csv(index=False)

                    st.download_button(
                        label="ä¸‹è½½é¢„æµ‹ç»“æœ",
                        data=csv,
                        file_name=f"predictions_{selected_model}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime='text/csv'
                    )
                    st.success("é¢„æµ‹ç»“æœå·²å‡†å¤‡å¥½ä¸‹è½½ï¼")

                except Exception as e:
                    st.error(f"å¯¼å‡ºé¢„æµ‹ç»“æœæ—¶å‡ºé”™: {str(e)}")


# --- ç»˜å›¾å‡½æ•° ---
def apply_plot_style(ax):
    """åº”ç”¨ç»Ÿä¸€çš„ç»˜å›¾æ ·å¼"""
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.grid(True, linestyle='--', alpha=0.6, color='#bdc3c7')
    ax.tick_params(axis='both', which='major', labelsize=9, colors='#34495e')
    ax.xaxis.label.set_fontsize(10)
    ax.yaxis.label.set_fontsize(10)
    ax.title.set_fontsize(12)
    ax.title.set_fontweight('bold')
    ax.xaxis.label.set_fontweight('bold')
    ax.yaxis.label.set_fontweight('bold')
    return ax


def plot_training_results(y_true, predictions, indices=None):
    """ç»˜åˆ¶è®­ç»ƒç»“æœå¯¹æ¯”å›¾"""
    fig, ax = plt.subplots(figsize=(10, 6), dpi=80)
    apply_plot_style(ax)
    font_kwargs = {'fontproperties': FONT_PROP} if FONT_PROP else {}

    try:
        if indices is None:
            indices = np.arange(len(y_true))

        # æ›´å®‰å…¨çš„æ•°æ®è½¬æ¢
        if isinstance(y_true, pd.Series):
            y_true_np = y_true.values
            y_true_name = y_true.name
        else:
            y_true_np = np.asarray(y_true)
            y_true_name = "çœŸå®å€¼"

        if isinstance(predictions, pd.Series):
            predictions_np = predictions.values
        else:
            predictions_np = np.asarray(predictions)

        if isinstance(indices, pd.Series):
            indices_np = indices.values
        else:
            indices_np = np.asarray(indices)

        # æ•°æ®éªŒè¯
        print(f"ç»˜å›¾æ•°æ®ç»Ÿè®¡:")
        print(f"y_true: min={y_true_np.min()}, max={y_true_np.max()}, mean={y_true_np.mean()}")
        print(f"predictions: min={predictions_np.min()}, max={predictions_np.max()}, mean={predictions_np.mean()}")

        if len(y_true_np) != len(predictions_np) or len(y_true_np) != len(indices_np):
            raise ValueError(
                f"ç»˜å›¾æ•°æ®é•¿åº¦ä¸åŒ¹é…: y_true={len(y_true_np)}, pred={len(predictions_np)}, indices={len(indices_np)}")

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å€¼éƒ½ä¸º0
        if np.all(y_true_np == 0) and np.all(predictions_np == 0):
            ax.text(0.5, 0.5, 'è­¦å‘Šï¼šæ‰€æœ‰æ•°æ®å€¼éƒ½ä¸º0\nè¯·æ£€æŸ¥æ•°æ®å¤„ç†è¿‡ç¨‹',
                    ha='center', va='center', color='red', fontsize=12, **font_kwargs)
            return fig

        sort_order = np.argsort(indices_np)
        sorted_indices = indices_np[sort_order]
        sorted_true = y_true_np[sort_order]
        sorted_pred = predictions_np[sort_order]

        ax.plot(sorted_indices, sorted_true, color='#2ecc71', label='çœŸå®å€¼', lw=1.5, marker='o', ms=3, alpha=0.8)
        ax.plot(sorted_indices, sorted_pred, color='#e74c3c', label='é¢„æµ‹å€¼', lw=1.5, ls='--', marker='x', ms=4,
                alpha=0.8)

        ax.set_xlabel('æ ·æœ¬ç´¢å¼• (åŸå§‹é¡ºåº)', **font_kwargs)
        ax.set_ylabel('å€¼', **font_kwargs)
        ax.set_title('æ¨¡å‹é¢„æµ‹å€¼ vs çœŸå®å€¼', **font_kwargs)
        legend = ax.legend(prop=FONT_PROP, fontsize=9)
        plt.tight_layout()

    except Exception as e:
        print(f"ç»˜åˆ¶è®­ç»ƒç»“æœå›¾æ—¶å‡ºé”™: {e}")
        ax.text(0.5, 0.5, f'ç»˜å›¾é”™è¯¯: {e}', ha='center', va='center', color='red', **font_kwargs)

    return fig


def plot_residuals(y_true, predictions, indices=None):
    """ç»˜åˆ¶æ®‹å·®å›¾"""
    fig, ax = plt.subplots(figsize=(10, 6), dpi=80)
    apply_plot_style(ax)
    font_kwargs = {'fontproperties': FONT_PROP} if FONT_PROP else {}

    try:
        if indices is None:
            indices = np.arange(len(y_true))

        y_true_np = y_true.values if isinstance(y_true, pd.Series) else np.asarray(y_true)
        predictions_np = predictions.values if isinstance(predictions, pd.Series) else np.asarray(predictions)
        indices_np = indices.values if isinstance(indices, pd.Series) else np.asarray(indices)

        if len(y_true_np) != len(predictions_np) or len(y_true_np) != len(indices_np):
            raise ValueError("ç»˜å›¾æ•°æ®é•¿åº¦ä¸åŒ¹é…")

        residuals = y_true_np - predictions_np
        sort_order = np.argsort(indices_np)
        sorted_indices = indices_np[sort_order]
        sorted_residuals = residuals[sort_order]

        ax.plot(sorted_indices, sorted_residuals, color='#3498db', label='æ®‹å·®', lw=1.5, marker='.', ms=3, alpha=0.8)
        ax.axhline(y=0, color='#e74c3c', linestyle='--', lw=1.0, alpha=0.7)

        ax.set_xlabel('æ ·æœ¬ç´¢å¼• (åŸå§‹é¡ºåº)', **font_kwargs)
        ax.set_ylabel('æ®‹å·® (çœŸå®å€¼ - é¢„æµ‹å€¼)', **font_kwargs)
        ax.set_title('æ®‹å·®åˆ†æå›¾', **font_kwargs)
        legend = ax.legend(prop=FONT_PROP, fontsize=9)
        plt.tight_layout()

    except Exception as e:
        print(f"ç»˜åˆ¶æ®‹å·®å›¾æ—¶å‡ºé”™: {e}")
        ax.text(0.5, 0.5, f'ç»˜å›¾é”™è¯¯: {e}', ha='center', va='center', color='red', **font_kwargs)

    return fig


def plot_feature_importance(feature_importance_dict, top_n=20):
    """ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾"""
    fig, ax = plt.subplots(figsize=(10, 8), dpi=80)
    apply_plot_style(ax)
    font_kwargs = {'fontproperties': FONT_PROP} if FONT_PROP else {}

    if not feature_importance_dict:
        ax.text(0.5, 0.5, 'æ— ç‰¹å¾é‡è¦æ€§æ•°æ®', ha='center', va='center', color='#7f8c8d', **font_kwargs)
        return fig

    try:
        sorted_importance = sorted(feature_importance_dict.items(), key=lambda x: x[1], reverse=True)
        top_features = sorted_importance[:top_n]
        features = [x[0] for x in top_features]
        importances = [x[1] for x in top_features]

        y_pos = np.arange(len(features))
        bars = ax.barh(y_pos, importances, align='center', color='#3498db', alpha=0.8)

        ax.set_yticks(y_pos)
        ax.set_yticklabels(features, **font_kwargs)
        ax.invert_yaxis()
        ax.set_xlabel('é‡è¦æ€§å€¼', **font_kwargs)
        ax.set_title(f'ç‰¹å¾é‡è¦æ€§ (Top {min(top_n, len(features))})', **font_kwargs)

        for bar in bars:
            width = bar.get_width()
            ax.text(width * 1.01, bar.get_y() + bar.get_height() / 2., f'{width:.4f}',
                    va='center', ha='left', fontsize=8)

        ax.set_xlim(right=max(importances) * 1.15)
        plt.tight_layout()

    except Exception as e:
        print(f"ç»˜åˆ¶ç‰¹å¾é‡è¦æ€§å›¾æ—¶å‡ºé”™: {e}")
        ax.text(0.5, 0.5, f'ç»˜å›¾é”™è¯¯: {e}', ha='center', va='center', color='red', **font_kwargs)

    return fig


# --- æ•°æ®å¤„ç†å‡½æ•° ---
def process_folder_data(folder_path, progress_callback=None):
    """å¤„ç†æ–‡ä»¶å¤¹æ•°æ®ï¼Œæ•´åˆå¤šä¸ªCSV/Excelæ–‡ä»¶"""
    try:
        file_paths = []
        for root, _, files in os.walk(folder_path):
            for file in files:
                if file.lower().endswith(('.csv', '.xlsx', '.xls')):
                    file_paths.append(os.path.join(root, file))

        if not file_paths:
            return None, "æœªåœ¨æ–‡ä»¶å¤¹ä¸­æ‰¾åˆ°CSVæˆ–Excelæ–‡ä»¶ã€‚"

        all_data, group_labels = [], []
        total_files = len(file_paths)

        for i, file_path in enumerate(file_paths):
            if progress_callback:
                progress_callback(int((i + 1) / total_files * 100))

            try:
                df = pd.read_csv(file_path) if file_path.lower().endswith('.csv') else pd.read_excel(file_path)
                if df.empty:
                    continue

                for col in df.select_dtypes(include=['object']).columns:
                    try:
                        df[col] = pd.to_numeric(df[col], errors='ignore')
                    except:
                        pass

                df.dropna(axis=1, how='all', inplace=True)
                df.dropna(inplace=True)

                if df.empty:
                    continue

                numeric_df = df.select_dtypes(include=np.number)
                if numeric_df.empty:
                    continue

                all_data.append(numeric_df)
                group_label = os.path.splitext(os.path.basename(file_path))[0]
                group_labels.extend([group_label] * len(numeric_df))

            except Exception as e:
                st.warning(f"å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶è·³è¿‡ï¼Œé”™è¯¯: {e}")

        if not all_data:
            return None, "æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®æ–‡ä»¶æˆ–æ‰€æœ‰æ–‡ä»¶å¤„ç†å¤±è´¥ã€‚"

        common_columns = set(all_data[0].columns)
        for df in all_data[1:]:
            common_columns.intersection_update(set(df.columns))

        if not common_columns:
            return None, "æ–‡ä»¶ä¹‹é—´æ²¡æœ‰å…±åŒçš„æ•°å€¼åˆ—ï¼Œæ— æ³•åˆå¹¶æ•°æ®ã€‚"

        common_columns = list(common_columns)
        filtered_data = [df[common_columns] for df in all_data]
        X = pd.concat(filtered_data, ignore_index=True)
        groups = pd.Series(group_labels, name='group', index=X.index)

        return {'X': X, 'groups': groups}, None

    except Exception as e:
        import traceback
        return None, f"å¤„ç†æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}\n{traceback.format_exc()}"


# --- ä¸»å‡½æ•°å…¥å£ ---
if __name__ == "__main__":
    show_regression_training_page()