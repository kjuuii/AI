# data_evaluator.py
import time

import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import traceback
from scipy import stats

# --- æ–°å¢ï¼šæ·»åŠ ç¼ºå¤±çš„ Sklearn å¯¼å…¥ ---
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, accuracy_score # æ·»åŠ äº† accuracy_score
from sklearn.model_selection import train_test_split
# --- ç»“æŸæ–°å¢ ---

# --- å°è¯•å¯¼å…¥å­—ä½“å·¥å…· ---
try:
    # å‡è®¾ font_utils.py åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹
    from font_utils import apply_plot_style, FONT_PROP, create_figure_with_safe_dimensions
    print("å­—ä½“å·¥å…·ä» font_utils æˆåŠŸåŠ è½½ (in data_evaluator)")
except ImportError:
    print("è­¦å‘Š: æ— æ³•ä» font_utils å¯¼å…¥ï¼Œå°†åœ¨ data_evaluator ä¸­ä½¿ç”¨å¤‡ç”¨ç»˜å›¾è®¾ç½®ã€‚")
    FONT_PROP = None
    # å®šä¹‰å¤‡ç”¨å‡½æ•°ï¼Œé¿å…åç»­ä»£ç å‡ºé”™
    def apply_plot_style(ax):
        ax.grid(True, linestyle='--', alpha=0.6)
        return ax
    def create_figure_with_safe_dimensions(w, h, dpi=80):
        # ç®€å•çš„å¤‡ç”¨å®ç°
        fig, ax = plt.subplots(figsize=(w, h), dpi=dpi)
        return fig, ax
# --- ç»“æŸå­—ä½“å¯¼å…¥ ---

# --- å¸¸é‡å®šä¹‰ ---
MIN_SAMPLES_PER_CLASS = 10 # åˆ†ç±»ä»»åŠ¡æ¯ç±»æœ€å°‘æ ·æœ¬é˜ˆå€¼ (ç¤ºä¾‹)
MIN_TOTAL_SAMPLES = 50     # æ•´ä½“æœ€å°‘æ ·æœ¬é˜ˆå€¼ (ç¤ºä¾‹)
HIGH_CORRELATION_THRESHOLD = 0.90 # é«˜ç›¸å…³æ€§é˜ˆå€¼ (ç¤ºä¾‹)
LOW_VARIANCE_THRESHOLD = 0.01    # ä½æ–¹å·®é˜ˆå€¼ (ç¤ºä¾‹)
HIGH_SKEWNESS_THRESHOLD = 1.0 # ååº¦é˜ˆå€¼
IQR_MULTIPLIER = 1.5      # IQR æ–¹æ³•çš„ä¹˜æ•°
ZSCORE_THRESHOLD = 3        # Z-score æ–¹æ³•çš„é˜ˆå€¼
HIGH_CARDINALITY_THRESHOLD = 50 # é«˜åŸºæ•°é˜ˆå€¼
RARE_CATEGORY_THRESHOLD = 0.01  # ç¨€æœ‰ç±»åˆ«é¢‘ç‡é˜ˆå€¼
LEAKAGE_CORR_THRESHOLD = 0.98   # æ½œåœ¨æ•°æ®æ³„éœ²çš„ç›¸å…³æ€§é˜ˆå€¼ (å›å½’)
LEAKAGE_AUC_THRESHOLD = 0.99    # æ½œåœ¨æ•°æ®æ³„éœ²çš„AUCé˜ˆå€¼ (åˆ†ç±»ï¼ŒåŸºäºç®€å•æ¨¡å‹)
# --- ç»“æŸå¸¸é‡å®šä¹‰ ---


# ==============================================================
#               è¯„ä¼°æ£€æŸ¥çš„è¾…åŠ©å‡½æ•°å®šä¹‰
# ==============================================================

def check_data_size(df):
    """æ£€æŸ¥æ•°æ®é›†å¤§å°æ˜¯å¦å¯èƒ½è¶³å¤Ÿ"""
    findings = []
    n_samples = len(df)
    if n_samples < MIN_TOTAL_SAMPLES:
        findings.append({
            'type': 'warning',
            'message': f"æ•°æ®é‡è¾ƒå° ({n_samples} ä¸ªæ ·æœ¬)ï¼Œå¯èƒ½ä¸è¶³ä»¥è®­ç»ƒå‡ºæ³›åŒ–èƒ½åŠ›å¼ºçš„æ¨¡å‹ã€‚"
        })
    else:
         findings.append({
            'type': 'info',
            'message': f"æ•°æ®é›†åŒ…å« {n_samples} ä¸ªæ ·æœ¬ã€‚"
        })
    return findings

def check_class_balance(y, task_type):
    """æ£€æŸ¥åˆ†ç±»ä»»åŠ¡çš„ç±»åˆ«å¹³è¡¡æ€§"""
    findings = []
    class_counts = None # åˆå§‹åŒ–
    if task_type != 'Classification':
        return findings, class_counts # åªå¯¹åˆ†ç±»ä»»åŠ¡æœ‰æ•ˆ

    if y is None or y.empty:
        findings.append({'type': 'error', 'message': 'ç›®æ ‡å˜é‡æ— æ•ˆæˆ–ä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥ç±»åˆ«å¹³è¡¡ã€‚'})
        return findings, class_counts

    try:
        class_counts = y.value_counts()
        n_classes = len(class_counts)
        findings.append({'type': 'info', 'message': f"ç›®æ ‡å˜é‡åŒ…å« {n_classes} ä¸ªç±»åˆ«ã€‚"})

        if n_classes > 1:
            min_count = class_counts.min()
            max_count = class_counts.max()
            imbalance_ratio = min_count / max_count if max_count > 0 else 1

            if min_count < MIN_SAMPLES_PER_CLASS:
                 minority_classes = class_counts[class_counts < MIN_SAMPLES_PER_CLASS].index.tolist()
                 findings.append({
                     'type': 'warning',
                     'message': f"ä»¥ä¸‹ç±»åˆ«æ ·æœ¬è¿‡å°‘ (å°‘äº {MIN_SAMPLES_PER_CLASS} ä¸ª): {minority_classes}ï¼Œæœ€å°ç±»åˆ«æ ·æœ¬æ•°ä¸º {min_count}ã€‚è¿™å¯èƒ½å½±å“æ¨¡å‹å­¦ä¹ è¿™äº›ç±»åˆ«ã€‚"
                 })
            if imbalance_ratio < 0.1: # ç¤ºä¾‹ï¼šä¸¥é‡ä¸å¹³è¡¡é˜ˆå€¼
                findings.append({
                    'type': 'warning',
                    'message': f"æ•°æ®å­˜åœ¨ä¸¥é‡çš„ç±»åˆ«ä¸å¹³è¡¡ (æœ€å°/æœ€å¤§æ¯”ä¾‹: {imbalance_ratio:.2f})ã€‚è€ƒè™‘ä½¿ç”¨æ•°æ®å¹³è¡¡æŠ€æœ¯ï¼ˆå¦‚è¿‡é‡‡æ ·/æ¬ é‡‡æ ·ï¼‰æˆ–è°ƒæ•´æ¨¡å‹è¯„ä¼°æŒ‡æ ‡ï¼ˆå¦‚å…³æ³¨F1åˆ†æ•°ã€AUCï¼‰ã€‚"
                })
            elif imbalance_ratio < 0.5: # ç¤ºä¾‹ï¼šä¸­åº¦ä¸å¹³è¡¡
                 findings.append({
                    'type': 'info',
                    'message': f"æ•°æ®å­˜åœ¨ä¸€å®šçš„ç±»åˆ«ä¸å¹³è¡¡ (æœ€å°/æœ€å¤§æ¯”ä¾‹: {imbalance_ratio:.2f})ã€‚å»ºè®®å…³æ³¨æ¨¡å‹åœ¨å°‘æ•°ç±»ä¸Šçš„æ€§èƒ½ã€‚"
                })
            else:
                findings.append({'type': 'info', 'message': "ç±»åˆ«åˆ†å¸ƒç›¸å¯¹å‡è¡¡ã€‚"})
        elif n_classes == 1:
            findings.append({'type': 'warning', 'message': "ç›®æ ‡å˜é‡åªåŒ…å«ä¸€ä¸ªç±»åˆ«ï¼Œæ— æ³•è¿›è¡Œåˆ†ç±»ä»»åŠ¡ã€‚"})
        else: # n_classes == 0
             findings.append({'type': 'error', 'message': "ç›®æ ‡å˜é‡ä¸­æœªæ‰¾åˆ°æœ‰æ•ˆç±»åˆ«ã€‚"})


    except Exception as e:
        findings.append({'type': 'error', 'message': f'æ£€æŸ¥ç±»åˆ«å¹³è¡¡æ—¶å‡ºé”™: {e}'})
        class_counts = None # å‡ºé”™æ—¶é‡ç½®

    return findings, class_counts

def check_missing_values(df):
    """æ£€æŸ¥æ•´ä¸ªæ•°æ®æ¡†ä¸­çš„ç¼ºå¤±å€¼"""
    findings = []
    if df is None or df.empty:
        findings.append({'type': 'error', 'message': 'è¾“å…¥æ•°æ®æ¡†ä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥ç¼ºå¤±å€¼ã€‚'})
        return findings
    try:
        missing_counts = df.isnull().sum()
        missing_cols = missing_counts[missing_counts > 0]
        if not missing_cols.empty:
            total_missing = int(missing_cols.sum()) # ç¡®ä¿æ˜¯æ•´æ•°
            percent_missing = (total_missing / df.size) * 100 # df.size æ˜¯æ€»å…ƒç´ æ•°
            findings.append({
                'type': 'warning',
                'message': f"æ•°æ®ä¸­å‘ç° {total_missing} ä¸ªç¼ºå¤±å€¼ ({percent_missing:.2f}% of total cells)ã€‚"
                           f" å­˜åœ¨ç¼ºå¤±å€¼çš„åˆ—: {missing_cols.index.tolist()}ã€‚"
                           f" ç¼ºå¤±å€¼ä¼šå½±å“å¤§å¤šæ•°æœºå™¨å­¦ä¹ æ¨¡å‹ï¼Œå»ºè®®ä½¿ç”¨ 'ç¼ºå¤±å€¼å¤„ç†' åŠŸèƒ½è¿›è¡Œå¡«å……æˆ–ç§»é™¤ã€‚"
            })
        else:
            findings.append({'type': 'info', 'message': "æ•°æ®ä¸­æœªå‘ç°ç¼ºå¤±å€¼ã€‚"})
    except Exception as e:
        findings.append({'type': 'error', 'message': f"æ£€æŸ¥ç¼ºå¤±å€¼æ—¶å‡ºé”™: {e}"})
    return findings

def check_feature_variance(X):
    """æ£€æŸ¥æ•°å€¼ç‰¹å¾çš„æ–¹å·®æ˜¯å¦è¿‡ä½"""
    findings = []
    if X is None or X.empty:
        findings.append({'type': 'error', 'message': 'ç‰¹å¾æ•°æ®Xä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥æ–¹å·®ã€‚'})
        return findings
    try:
        numeric_X = X.select_dtypes(include=np.number)
        if numeric_X.empty:
            findings.append({'type': 'info', 'message': 'æœªå‘ç°æ•°å€¼ç‰¹å¾ï¼Œè·³è¿‡æ–¹å·®æ£€æŸ¥ã€‚'})
            return findings

        variances = numeric_X.var()
        # æ’é™¤æ–¹å·®ä¸ºNaNæˆ–Infçš„æƒ…å†µ (ä¾‹å¦‚ï¼Œå¦‚æœåˆ—åªæœ‰ä¸€ä¸ªå€¼æˆ–å…¨æ˜¯NaN)
        variances = variances.replace([np.inf, -np.inf], np.nan).dropna()
        low_variance_cols = variances[variances < LOW_VARIANCE_THRESHOLD].index.tolist()

        if low_variance_cols:
            findings.append({
                'type': 'warning',
                'message': f"ä»¥ä¸‹ç‰¹å¾æ–¹å·®è¾ƒä½ (ä½äº {LOW_VARIANCE_THRESHOLD})ï¼Œå¯èƒ½æä¾›çš„ä¿¡æ¯æœ‰é™: {low_variance_cols}ã€‚è€ƒè™‘æ˜¯å¦ç§»é™¤è¿™äº›ç‰¹å¾ï¼Œå› ä¸ºå®ƒä»¬å¯¹æ¨¡å‹é¢„æµ‹çš„è´¡çŒ®å¯èƒ½å¾ˆå°ã€‚"
            })
        else:
            findings.append({'type': 'info', 'message': "æœªæ£€æµ‹åˆ°æ•°å€¼ç‰¹å¾æ–¹å·®è¿‡ä½çš„æƒ…å†µã€‚"})

    except Exception as e:
        findings.append({'type': 'error', 'message': f"æ£€æŸ¥ç‰¹å¾æ–¹å·®æ—¶å‡ºé”™: {e}"})
    return findings


def check_feature_distribution(X):
    """æ£€æŸ¥æ•°å€¼ç‰¹å¾çš„åˆ†å¸ƒï¼Œè¯†åˆ«åæ€ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–"""
    findings = []
    visualizations = []
    if X is None or X.empty:
        findings.append({'type': 'error', 'message': 'ç‰¹å¾æ•°æ®Xä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥åˆ†å¸ƒã€‚'})
        return findings, visualizations

    numeric_cols = X.select_dtypes(include=np.number).columns
    if numeric_cols.empty:
        findings.append({'type': 'info', 'message': 'æœªå‘ç°æ•°å€¼ç‰¹å¾ï¼Œè·³è¿‡åˆ†å¸ƒæ£€æŸ¥ã€‚'})
        return findings, visualizations

    highly_skewed_cols = []
    for col in numeric_cols:
        # è·³è¿‡æ–¹å·®è¿‡ä½çš„åˆ—
        try:
             # æ£€æŸ¥æ–¹å·®å‰å…ˆç§»é™¤NaN
            var_check = X[col].dropna().var()
            if pd.isna(var_check) or var_check < LOW_VARIANCE_THRESHOLD:
                 continue
        except Exception:
             continue # è®¡ç®—æ–¹å·®å‡ºé”™ä¹Ÿè·³è¿‡

        fig = None # åˆå§‹åŒ– fig
        try:
            # è®¡ç®—ååº¦ï¼Œå¤„ç†NaNå€¼
            skewness = X[col].dropna().skew()
            if pd.isna(skewness):
                 continue
            if abs(skewness) > HIGH_SKEWNESS_THRESHOLD:
                highly_skewed_cols.append(f"{col} ({skewness:.2f})")

            # --- å¯è§†åŒ– ---
            fig, ax = create_figure_with_safe_dimensions(6, 4, dpi=70)
            sns.histplot(X[col].dropna(), kde=True, ax=ax, bins=min(30, X[col].nunique()))
            apply_plot_style(ax)
            ax.set_title(f"ç‰¹å¾ '{col}' åˆ†å¸ƒ (ååº¦: {skewness:.2f})", fontproperties=FONT_PROP if FONT_PROP else None, fontsize=10)
            ax.set_xlabel(col, fontproperties=FONT_PROP if FONT_PROP else None, fontsize=9)
            ax.set_ylabel("é¢‘ç‡", fontproperties=FONT_PROP if FONT_PROP else None, fontsize=9)
            plt.xticks(fontsize=8)
            plt.yticks(fontsize=8)
            plt.tight_layout()
            visualizations.append(fig)
            # --- ç»“æŸå¯è§†åŒ– ---

        except Exception as e:
            findings.append({'type': 'warning', 'message': f"åˆ†æç‰¹å¾ '{col}' çš„åˆ†å¸ƒæ—¶å‡ºé”™: {e}"})
            if fig: plt.close(fig) # å¦‚æœç»˜å›¾å‡ºé”™ï¼Œå°è¯•å…³é—­

    if highly_skewed_cols:
        findings.append({
            'type': 'warning',
            'message': f"ä»¥ä¸‹æ•°å€¼ç‰¹å¾å­˜åœ¨æ˜æ˜¾åæ€ (ååº¦ç»å¯¹å€¼ > {HIGH_SKEWNESS_THRESHOLD}): {', '.join(highly_skewed_cols)}ã€‚"
                       f" åæ€åˆ†å¸ƒå¯èƒ½è¿åæŸäº›æ¨¡å‹ï¼ˆå¦‚çº¿æ€§å›å½’ã€LDAï¼‰çš„å‡è®¾ï¼Œå½±å“æ€§èƒ½ã€‚è€ƒè™‘è¿›è¡Œæ•°æ®è½¬æ¢ï¼ˆå¦‚å¯¹æ•°ã€å¹³æ–¹æ ¹ã€Box-Coxå˜æ¢ï¼‰æ¥ä½¿å…¶æ›´æ¥è¿‘æ­£æ€åˆ†å¸ƒã€‚"
        })
    else:
        findings.append({'type': 'info', 'message': 'æ•°å€¼ç‰¹å¾æœªå‘ç°æ˜æ˜¾åæ€ã€‚'})

    return findings, visualizations

def check_categorical_features(X):
    """æ£€æŸ¥åˆ†ç±»ç‰¹å¾çš„åŸºæ•°å’Œç¨€æœ‰ç±»åˆ«ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–"""
    findings = []
    visualizations = []
    if X is None or X.empty:
        findings.append({'type': 'error', 'message': 'ç‰¹å¾æ•°æ®Xä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥åˆ†ç±»ç‰¹å¾ã€‚'})
        return findings, visualizations

    # é€‰æ‹©å¯¹è±¡ç±»å‹å’Œå”¯ä¸€å€¼è¾ƒå°‘çš„æ•°å€¼ç±»å‹ä½œä¸ºæ½œåœ¨åˆ†ç±»ç‰¹å¾
    categorical_cols = X.select_dtypes(include='object').columns.tolist()
    potential_cat_numeric = [col for col in X.select_dtypes(include=np.number).columns
                             if X[col].nunique() < HIGH_CARDINALITY_THRESHOLD / 2]
    categorical_cols.extend(potential_cat_numeric)
    # æ’é™¤å¸ƒå°”ç±»å‹è¢«è¯¯åˆ¤ä¸ºåˆ†ç±»
    categorical_cols = [col for col in categorical_cols if X[col].dtype != 'bool']
    categorical_cols = sorted(list(set(categorical_cols)))


    if not categorical_cols:
        findings.append({'type': 'info', 'message': 'æœªå‘ç°æ˜æ˜¾çš„åˆ†ç±»ç‰¹å¾ï¼Œè·³è¿‡ç›¸å…³æ£€æŸ¥ã€‚'})
        return findings, visualizations

    high_cardinality_cols = []
    rare_category_cols = {}

    for col in categorical_cols:
        fig = None # åˆå§‹åŒ– fig
        try:
            # è®¡ç®—å”¯ä¸€å€¼æ•°é‡ï¼Œå¤„ç†NaN
            col_data_clean = X[col].dropna()
            if col_data_clean.empty: continue # å¦‚æœåˆ—å…¨æ˜¯NaN
            n_unique = col_data_clean.nunique()

            if n_unique > HIGH_CARDINALITY_THRESHOLD:
                high_cardinality_cols.append(f"{col} ({n_unique})")

            # è®¡ç®—é¢‘ç‡
            value_counts = col_data_clean.value_counts(normalize=True)
            rare_categories = value_counts[value_counts < RARE_CATEGORY_THRESHOLD].index.tolist()
            if rare_categories:
                rare_category_cols[col] = rare_categories

            # --- å¯è§†åŒ– (é™åˆ¶ç±»åˆ«æ•°é‡) ---
            if 1 < n_unique <= 30:
                 fig, ax = create_figure_with_safe_dimensions(6, 4, dpi=70)
                 top_n = 20
                 plot_counts = col_data_clean.value_counts().nlargest(top_n)
                 sns.barplot(x=plot_counts.index.astype(str), y=plot_counts.values, ax=ax, palette="viridis")
                 apply_plot_style(ax)
                 ax.set_title(f"ç‰¹å¾ '{col}' é¢‘ç‡ (Top {min(top_n, n_unique)})", fontproperties=FONT_PROP if FONT_PROP else None, fontsize=10)
                 ax.set_xlabel("ç±»åˆ«", fontproperties=FONT_PROP if FONT_PROP else None, fontsize=9)
                 ax.set_ylabel("é¢‘ç‡", fontproperties=FONT_PROP if FONT_PROP else None, fontsize=9)
                 plt.xticks(rotation=45, ha='right', fontsize=8, fontproperties=FONT_PROP if FONT_PROP else None)
                 plt.yticks(fontsize=8)
                 plt.tight_layout()
                 visualizations.append(fig)
            # --- ç»“æŸå¯è§†åŒ– ---

        except Exception as e:
            findings.append({'type': 'warning', 'message': f"åˆ†æåˆ†ç±»ç‰¹å¾ '{col}' æ—¶å‡ºé”™: {e}"})
            if fig: plt.close(fig)


    if high_cardinality_cols:
        findings.append({
            'type': 'warning',
            'message': f"ä»¥ä¸‹åˆ†ç±»ç‰¹å¾åŸºæ•°è¿‡é«˜ (å”¯ä¸€å€¼ > {HIGH_CARDINALITY_THRESHOLD}): {', '.join(high_cardinality_cols)}ã€‚"
                       f" è¿™ä¼šç»™ç‹¬çƒ­ç¼–ç å¸¦æ¥å›°éš¾ï¼ˆç»´åº¦çˆ†ç‚¸ï¼‰ï¼Œå½±å“æ¨¡å‹æ€§èƒ½å’Œè®­ç»ƒæ—¶é—´ã€‚è€ƒè™‘é™åŸºæ•°æ–¹æ³•ï¼šå¦‚åˆå¹¶ç¨€æœ‰ç±»åˆ«ã€ç›®æ ‡ç¼–ç ã€é¢‘æ•°ç¼–ç ã€ç‰¹å¾å“ˆå¸Œï¼Œæˆ–å°†æ­¤ç‰¹å¾è§†ä¸ºæ–‡æœ¬/IDç‰¹å¾å¤„ç†ã€‚"
        })
    if rare_category_cols:
        rare_info = "; ".join([f"{col}: {str(cats[:3])}..." if len(cats) > 3 else f"{col}: {str(cats)}" for col, cats in rare_category_cols.items()])
        findings.append({
            'type': 'warning',
            'message': f"ä»¥ä¸‹åˆ†ç±»ç‰¹å¾åŒ…å«ç¨€æœ‰ç±»åˆ« (é¢‘ç‡ < {RARE_CATEGORY_THRESHOLD*100}%): {rare_info}ã€‚"
                       f" æ¨¡å‹å¯èƒ½éš¾ä»¥ä»ç¨€æœ‰ç±»åˆ«ä¸­å­¦ä¹ ï¼Œè¿™äº›ç±»åˆ«å¯¹æ¨¡å‹æ€§èƒ½è´¡çŒ®æœ‰é™ï¼Œè¿˜å¯èƒ½å¢åŠ è¿‡æ‹Ÿåˆé£é™©ã€‚è€ƒè™‘å°†ç¨€æœ‰ç±»åˆ«åˆå¹¶ä¸º 'å…¶ä»–' ç±»åˆ«ï¼Œæˆ–ä½¿ç”¨ç‰¹å®šå¤„ç†æ–¹æ³•ã€‚"
        })

    if not high_cardinality_cols and not rare_category_cols and categorical_cols:
         findings.append({'type': 'info', 'message': 'åˆ†ç±»ç‰¹å¾çš„åŸºæ•°å’Œé¢‘ç‡åˆ†å¸ƒåœ¨åˆç†èŒƒå›´å†…ã€‚'})

    return findings, visualizations

def check_outliers_summary(X):
    """ä½¿ç”¨ IQR æ–¹æ³•åˆæ­¥æ£€æŸ¥æ•°å€¼ç‰¹å¾ä¸­çš„ç¦»ç¾¤ç‚¹"""
    findings = []
    if X is None or X.empty:
        findings.append({'type': 'error', 'message': 'ç‰¹å¾æ•°æ®Xä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥ç¦»ç¾¤ç‚¹ã€‚'})
        return findings

    numeric_cols = X.select_dtypes(include=np.number).columns
    if numeric_cols.empty:
        findings.append({'type': 'info', 'message': 'æœªå‘ç°æ•°å€¼ç‰¹å¾ï¼Œè·³è¿‡ç¦»ç¾¤ç‚¹åˆæ­¥æ£€æŸ¥ã€‚'})
        return findings

    outlier_cols_details = {}
    for col in numeric_cols:
        try:
            col_data = X[col].dropna()
            if col_data.empty: continue

            Q1 = col_data.quantile(0.25)
            Q3 = col_data.quantile(0.75)
            IQR = Q3 - Q1
            if pd.isna(IQR) or IQR <= 1e-8: continue

            lower_bound = Q1 - IQR_MULTIPLIER * IQR
            upper_bound = Q3 + IQR_MULTIPLIER * IQR
            outliers_mask = (X[col] < lower_bound) | (X[col] > upper_bound)
            num_outliers = outliers_mask.sum()

            if num_outliers > 0:
                percent_outliers = (num_outliers / len(X)) * 100
                if percent_outliers > 1.0:
                    outlier_cols_details[col] = f"{percent_outliers:.1f}%"
        except Exception as e:
             findings.append({'type': 'warning', 'message': f"æ£€æŸ¥ç‰¹å¾ '{col}' çš„ç¦»ç¾¤ç‚¹æ—¶å‡ºé”™: {e}"})

    if outlier_cols_details:
         details_str = "; ".join([f"{col} ({perc})" for col, perc in outlier_cols_details.items()])
         findings.append({
             'type': 'warning',
             'message': f"åˆæ­¥æ£€æµ‹å‘ç°ä»¥ä¸‹æ•°å€¼ç‰¹å¾å¯èƒ½åŒ…å«è¾ƒå¤šç¦»ç¾¤ç‚¹ (åŸºäº >{IQR_MULTIPLIER} å€ IQR): {details_str}ã€‚"
                        f" ç¦»ç¾¤ç‚¹ä¼šæ˜¾è‘—å½±å“å‡å€¼ã€æ–¹å·®ç­‰ç»Ÿè®¡é‡ï¼Œå¯¹çº¿æ€§æ¨¡å‹ã€SVMã€KNN ç­‰ç®—æ³•æ€§èƒ½å½±å“è¾ƒå¤§ï¼Œå¯èƒ½æ‹‰åæ¨¡å‹æ‹Ÿåˆã€‚å»ºè®®ä½¿ç”¨ 'å¼‚å¸¸ç‚¹å‘ç°' åŠŸèƒ½è¯¦ç»†åˆ†æï¼Œå¹¶è€ƒè™‘å¤„ç†ï¼ˆå¦‚ç›–å¸½æ³•ã€ç§»é™¤ã€æ›¿æ¢ä¸ºä¸­ä½æ•°/å‡å€¼ã€åˆ†ç®±ï¼‰æˆ–ä½¿ç”¨å¯¹ç¦»ç¾¤ç‚¹ä¸æ•æ„Ÿçš„é²æ£’æ¨¡å‹ï¼ˆå¦‚æ ‘æ¨¡å‹ï¼‰ã€‚"
         })
    else:
        findings.append({'type': 'info', 'message': 'æ•°å€¼ç‰¹å¾åˆæ­¥æ£€æŸ¥æœªå‘ç°å¤§é‡ç¦»ç¾¤ç‚¹ (åŸºäº IQR)ã€‚'})

    return findings

def check_feature_correlation(X):
    """æ£€æŸ¥æ•°å€¼ç‰¹å¾é—´çš„é«˜ç›¸å…³æ€§ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–"""
    findings = []
    visualizations = []
    if X is None or X.empty:
        findings.append({'type': 'error', 'message': 'ç‰¹å¾æ•°æ®Xä¸ºç©ºï¼Œæ— æ³•æ£€æŸ¥ç›¸å…³æ€§ã€‚'})
        return findings, visualizations

    numeric_X = X.select_dtypes(include=np.number)
    if numeric_X.shape[1] < 2:
        findings.append({'type': 'info', 'message': "æ•°å€¼ç‰¹å¾ä¸è¶³2ä¸ªï¼Œæ— æ³•è®¡ç®—ç›¸å…³æ€§ã€‚"})
        return findings, visualizations

    fig_corr = None # åˆå§‹åŒ– fig
    try:
        corr_matrix = numeric_X.corr()
        upper_triangle_mask = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        upper_triangle_values = corr_matrix.where(upper_triangle_mask)

        high_corr_features = []
        for col in upper_triangle_values.columns:
            correlated_rows = upper_triangle_values.index[upper_triangle_values[col].abs() > HIGH_CORRELATION_THRESHOLD].tolist()
            for row_label in correlated_rows:
                if pd.notna(upper_triangle_values.loc[row_label, col]): # ç¡®ä¿å€¼ä¸æ˜¯ NaN
                    high_corr_features.append((col, row_label, corr_matrix.loc[row_label, col]))

        if high_corr_features:
             details = ", ".join([f"({p[0]}, {p[1]}: {p[2]:.2f})" for p in high_corr_features])
             findings.append({
                'type': 'warning',
                'message': f"å‘ç°ä»¥ä¸‹æ•°å€¼ç‰¹å¾å¯¹ä¹‹é—´å­˜åœ¨é«˜ç›¸å…³æ€§ (ç»å¯¹å€¼ > {HIGH_CORRELATION_THRESHOLD}): {details}ã€‚"
                           f" é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯èƒ½å¼•å…¥å¤šé‡å…±çº¿æ€§é—®é¢˜ï¼Œå½±å“çº¿æ€§æ¨¡å‹ç³»æ•°çš„ç¨³å®šæ€§å’Œè§£é‡Šæ€§ã€‚è€ƒè™‘ç§»é™¤å…¶ä¸­ä¸€ä¸ªç‰¹å¾ã€ä½¿ç”¨PCAè¿›è¡Œé™ç»´ï¼Œæˆ–é€‰ç”¨å¯¹å…±çº¿æ€§ä¸æ•æ„Ÿçš„æ¨¡å‹ï¼ˆå¦‚éšæœºæ£®æ—ã€XGBoostï¼‰ã€‚"
             })
        else:
             findings.append({'type': 'info', 'message': f"æœªå‘ç°æ•°å€¼ç‰¹å¾ä¹‹é—´å­˜åœ¨å¼ºç›¸å…³æ€§ (é˜ˆå€¼ > {HIGH_CORRELATION_THRESHOLD})ã€‚"})

        # --- å¯è§†åŒ–ï¼šç›¸å…³æ€§çƒ­åŠ›å›¾ ---
        max_heatmap_features = 50
        if numeric_X.shape[1] <= max_heatmap_features:
             plot_cols = numeric_X.columns
             plot_corr_matrix = corr_matrix
        else:
             top_var_cols = numeric_X.var().nlargest(max_heatmap_features).index
             plot_cols = top_var_cols
             plot_corr_matrix = numeric_X[plot_cols].corr()
             findings.append({'type': 'info', 'message': f"ç‰¹å¾è¿‡å¤šï¼Œç›¸å…³æ€§çƒ­åŠ›å›¾ä»…æ˜¾ç¤ºæ–¹å·®æœ€å¤§çš„ {max_heatmap_features} ä¸ªç‰¹å¾ã€‚"})

        if not plot_cols.empty and not plot_corr_matrix.empty:
             fig_corr, ax_corr = create_figure_with_safe_dimensions(min(10, len(plot_cols)*0.5 + 1), min(8, len(plot_cols)*0.4 + 1), dpi=80)
             sns.heatmap(plot_corr_matrix, annot=False, cmap='coolwarm', fmt=".2f", linewidths=.5, ax=ax_corr, vmin=-1, vmax=1)
             ax_corr.set_title("ç‰¹å¾ç›¸å…³æ€§çƒ­åŠ›å›¾", fontproperties=FONT_PROP if FONT_PROP else None)
             plt.setp(ax_corr.get_xticklabels(), rotation=45, ha="right", fontproperties=FONT_PROP if FONT_PROP else None, fontsize=7)
             plt.setp(ax_corr.get_yticklabels(), rotation=0, fontproperties=FONT_PROP if FONT_PROP else None, fontsize=7)
             plt.tight_layout()
             visualizations.append(fig_corr)
        # --- ç»“æŸå¯è§†åŒ– ---

    except Exception as e:
        findings.append({'type': 'error', 'message': f"æ£€æŸ¥ç‰¹å¾ç›¸å…³æ€§æ—¶å‡ºé”™: {e}"})
        if fig_corr: plt.close(fig_corr)

    return findings, visualizations

def analyze_target_variable(y, task_type):
    """åˆ†æç›®æ ‡å˜é‡çš„åˆ†å¸ƒ"""
    findings = []
    visualizations = []
    fig_target_reg = None # åˆå§‹åŒ–
    fig_target_cls = None # åˆå§‹åŒ–

    if y is None or y.empty:
        findings.append({'type':'error', 'message':'ç›®æ ‡å˜é‡æ— æ•ˆæˆ–ä¸ºç©ºï¼Œæ— æ³•åˆ†æã€‚'})
        return findings, visualizations

    target_name = y.name if y.name else "ç›®æ ‡å˜é‡"

    try:
        if task_type == 'Regression':
            if not pd.api.types.is_numeric_dtype(y):
                 findings.append({'type':'error', 'message': f"å›å½’ä»»åŠ¡çš„ç›®æ ‡å˜é‡ '{target_name}' ä¸æ˜¯æ•°å€¼ç±»å‹ã€‚"}); return findings, visualizations
            # ç¦»ç¾¤ç‚¹æ£€æŸ¥
            y_clean = y.dropna()
            if not y_clean.empty:
                Q1 = y_clean.quantile(0.25); Q3 = y_clean.quantile(0.75); IQR = Q3 - Q1
                if pd.notna(IQR) and IQR > 1e-8:
                    lower = Q1 - IQR_MULTIPLIER * IQR; upper = Q3 + IQR_MULTIPLIER * IQR
                    outliers_y = y[(y < lower) | (y > upper)]
                    if not outliers_y.empty:
                         perc = (len(outliers_y) / len(y)) * 100
                         if perc > 1.0: findings.append({'type':'warning', 'message': f"ç›®æ ‡å˜é‡ '{target_name}' å‘ç°çº¦ {perc:.1f}% æ½œåœ¨ç¦»ç¾¤ç‚¹(IQR)ã€‚"})
            # å¯è§†åŒ–
            fig_target_reg, axes = plt.subplots(1, 2, figsize=(12, 4), dpi=80)
            sns.histplot(y_clean, kde=True, ax=axes[0]); apply_plot_style(axes[0])
            axes[0].set_title(f"'{target_name}' åˆ†å¸ƒ", fontproperties=FONT_PROP, fontsize=10); axes[0].set_xlabel(target_name, fontproperties=FONT_PROP, fontsize=9); axes[0].set_ylabel("é¢‘ç‡", fontproperties=FONT_PROP, fontsize=9)
            sns.boxplot(y=y_clean, ax=axes[1], color="lightblue"); apply_plot_style(axes[1])
            axes[1].set_title(f"'{target_name}' ç®±çº¿å›¾", fontproperties=FONT_PROP, fontsize=10); axes[1].set_ylabel(target_name, fontproperties=FONT_PROP, fontsize=9); axes[1].tick_params(axis='x', bottom=False, labelbottom=False)
            plt.tight_layout(); visualizations.append(fig_target_reg)
            # ååº¦æ£€æŸ¥
            skewness = y_clean.skew()
            if pd.notna(skewness) and abs(skewness) > HIGH_SKEWNESS_THRESHOLD: findings.append({'type':'warning', 'message': f"å›å½’ç›®æ ‡å˜é‡ '{target_name}' å­˜åœ¨æ˜æ˜¾åæ€ (Skewness: {skewness:.2f})ã€‚è€ƒè™‘å˜æ¢ã€‚"})

        elif task_type == 'Classification':
            if pd.api.types.is_numeric_dtype(y): findings.append({'type':'info', 'message': f"åˆ†ç±»ç›®æ ‡ '{target_name}' æ˜¯æ•°å€¼ç±»å‹ã€‚"})
            elif pd.api.types.is_string_dtype(y) or pd.api.types.is_categorical_dtype(y) or y.dtype == 'object': findings.append({'type':'info', 'message': f"åˆ†ç±»ç›®æ ‡ '{target_name}' æ˜¯æ–‡æœ¬/ç±»åˆ«ç±»å‹ã€‚"})
            else: findings.append({'type':'warning', 'message': f"åˆ†ç±»ç›®æ ‡ '{target_name}' ç±»å‹ ({y.dtype}) æœªçŸ¥ã€‚"})
            # åˆ†ç±»åˆ†å¸ƒå›¾ (ä¾èµ– check_class_balance å‡½æ•°ä¸­è¿”å›çš„ class_counts)
            # æ³¨æ„ï¼šæ­¤å¤„å‡è®¾ check_class_balance å…ˆè¢«è°ƒç”¨ä¸”è¿”å›äº† class_counts
            # è¿™åœ¨ evaluate_data å‡½æ•°çš„ç»“æ„ä¸­æ˜¯ä¿è¯çš„
            class_counts_local = y.value_counts().sort_index() # é‡æ–°è®¡ç®—ä»¥é˜²ä¸‡ä¸€
            if not class_counts_local.empty and len(class_counts_local) > 1:
                 fig_target_cls, ax_target_cls = create_figure_with_safe_dimensions(8, 5, dpi=80)
                 apply_plot_style(ax_target_cls)
                 bars = ax_target_cls.bar(class_counts_local.index.astype(str), class_counts_local.values, color=plt.cm.viridis(np.linspace(0, 1, len(class_counts_local))))
                 ax_target_cls.set_title(f"ç›®æ ‡å˜é‡ '{target_name}' åˆ†å¸ƒ (åˆ†ç±»)", fontproperties=FONT_PROP if FONT_PROP else None)
                 ax_target_cls.set_xlabel("ç±»åˆ«", fontproperties=FONT_PROP if FONT_PROP else None); ax_target_cls.set_ylabel("æ ·æœ¬æ•°é‡", fontproperties=FONT_PROP if FONT_PROP else None)
                 for bar in bars: height = bar.get_height(); ax_target_cls.annotate(f'{int(height)}', xy=(bar.get_x() + bar.get_width() / 2, height), xytext=(0, 3), textcoords="offset points", ha='center', va='bottom', fontsize=8)
                 plt.xticks(rotation=45, ha='right', fontsize=8, fontproperties=FONT_PROP if FONT_PROP else None)
                 plt.tight_layout(); visualizations.append(fig_target_cls)

    except Exception as e:
        findings.append({'type': 'error', 'message': f"åˆ†æç›®æ ‡å˜é‡ '{target_name}' æ—¶å‡ºé”™: {e}"})
        if fig_target_reg: plt.close(fig_target_reg)
        if fig_target_cls: plt.close(fig_target_cls)

    return findings, visualizations

def check_data_leakage_heuristic(X, y, task_type):
    """ä½¿ç”¨å¯å‘å¼æ–¹æ³•æ£€æŸ¥æ½œåœ¨çš„æ•°æ®æ³„éœ²é£é™©"""
    findings = []
    if X is None or X.empty or y is None or y.empty:
        return findings # ä¸èƒ½æ£€æŸ¥

    numeric_X = X.select_dtypes(include=np.number)
    if numeric_X.empty:
        findings.append({'type':'info', 'message':'æœªå‘ç°æ•°å€¼ç‰¹å¾ï¼Œè·³è¿‡åŸºäºç›¸å…³æ€§/AUCçš„æ•°æ®æ³„éœ²æ£€æŸ¥ã€‚'})
        return findings

    suspicious_features = []

    if task_type == 'Regression':
        if pd.api.types.is_numeric_dtype(y):
            try:
                y_clean = y.dropna()
                X_clean = numeric_X.loc[y_clean.index] # å¯¹é½ X
                if not X_clean.empty:
                    correlations = X_clean.corrwith(y_clean).abs().dropna()
                    leaky = correlations[correlations > LEAKAGE_CORR_THRESHOLD]
                    if not leaky.empty:
                        suspicious_features.extend([(f, f"ä¸ç›®æ ‡ç›¸å…³æ€§ {leaky[f]:.3f}") for f in leaky.index])
            except Exception as e: findings.append({'type':'warning', 'message': f'è®¡ç®—å›å½’æ³„éœ²ç›¸å…³æ€§æ—¶å‡ºé”™: {e}'})

    elif task_type == 'Classification':
        try:
            y_clean = y.dropna()
            X_clean = numeric_X.loc[y_clean.index] # å¯¹é½ X
            if X_clean.empty or y_clean.empty: return findings

            if not pd.api.types.is_numeric_dtype(y_clean):
                 le = LabelEncoder()
                 y_encoded = le.fit_transform(y_clean)
            else:
                 y_encoded = y_clean.astype(int)

            n_classes = len(np.unique(y_encoded))
            if n_classes <= 1: return findings # æ— æ³•è®¡ç®— AUC

            for col in X_clean.columns:
                try:
                    X_single = X_clean[[col]].copy()
                    if X_single[col].isnull().any():
                        X_single[col].fillna(X_single[col].median(), inplace=True)
                    if X_single[col].var() < 1e-8: continue # è·³è¿‡ä½æ–¹å·®

                    if len(X_single) < 10 or n_classes > len(y_encoded) // 2: continue

                    X_train, X_test, y_train, y_test = train_test_split(X_single, y_encoded, test_size=0.3, random_state=42, stratify=y_encoded)

                    if len(np.unique(y_train)) > 1 and len(np.unique(y_test)) > 1:
                         model = LogisticRegression(random_state=42, class_weight='balanced', solver='liblinear', max_iter=100) # å¢åŠ  max_iter
                         model.fit(X_train, y_train)
                         if hasattr(model, "predict_proba"):
                             y_prob = model.predict_proba(X_test)
                             if n_classes == 2: auc_val = roc_auc_score(y_test, y_prob[:, 1])
                             else: auc_val = roc_auc_score(y_test, y_prob, multi_class='ovr', average='macro')
                             if auc_val > LEAKAGE_AUC_THRESHOLD: suspicious_features.append((col, f"å•ç‹¬AUC {auc_val:.3f}"))
                         else:
                             y_pred_leak = model.predict(X_test)
                             acc_leak = accuracy_score(y_test, y_pred_leak)
                             if acc_leak > LEAKAGE_AUC_THRESHOLD: suspicious_features.append((col, f"å•ç‹¬å‡†ç¡®ç‡ {acc_leak:.3f}"))
                except ValueError as ve: pass # å¿½ç•¥åˆ†å±‚é”™è¯¯ç­‰
                except Exception as e_inner: findings.append({'type':'error', 'message': f"æ£€æŸ¥ç‰¹å¾'{col}'æ³„éœ²æ—¶å‡ºé”™: {e_inner}"})
        except Exception as e_outer: findings.append({'type':'error', 'message': f"æ£€æŸ¥åˆ†ç±»ä»»åŠ¡æ³„éœ²æ—¶å‡ºé”™: {e_outer}"})

    # æŠ¥å‘Šç»“æœ
    if suspicious_features:
        details_str = "; ".join([f"{f} ({reason})" for f, reason in suspicious_features])
        findings.append({
            'type': 'error',
            'message': f"æ½œåœ¨æ•°æ®æ³„éœ²é£é™©ï¼ä»¥ä¸‹ç‰¹å¾ä¸ç›®æ ‡å˜é‡å…³è”æé«˜: {details_str}ã€‚"
                       f" **å¼ºçƒˆå»ºè®®åœ¨å»ºæ¨¡å‰ç§»é™¤è¿™äº›ç‰¹å¾ï¼**"
        })
    else:
        findings.append({'type': 'info', 'message': 'æœªæ£€æµ‹åˆ°æ˜æ˜¾çš„å•ä¸€ç‰¹å¾æ•°æ®æ³„éœ²é£é™©ã€‚'})

    return findings

def assess_generalization_risk(n_samples, n_features, findings):
    """åŸºäºå·²æœ‰å‘ç°è¯„ä¼°æ¨¡å‹çš„æ³›åŒ–é£é™©"""
    risk_factors = set() # ä½¿ç”¨é›†åˆå»é‡
    recommendations = set()

    # ç»´åº¦ç¾éš¾é£é™©
    if n_samples > 0 and n_features > 0:
        ratio = n_features / n_samples
        if n_features > n_samples: risk_factors.add("é«˜ç»´ç¨€ç–(ç‰¹å¾>æ ·æœ¬)"); recommendations.add("é™ç»´/ç‰¹å¾é€‰æ‹©"); recommendations.add("å¼ºæ­£åˆ™åŒ–"); recommendations.add("è·å–æ›´å¤šæ•°æ®")
        elif ratio > 0.1: risk_factors.add("ç‰¹å¾ç›¸å¯¹è¾ƒå¤š"); recommendations.add("ç‰¹å¾é€‰æ‹©/æ­£åˆ™åŒ–"); recommendations.add("äº¤å‰éªŒè¯")

    # æ ¹æ®å‘ç°æ·»åŠ é£é™©/å»ºè®®
    for finding in findings:
        if finding['type'] in ['warning', 'error']:
            msg = finding['message'].lower()
            if "æ•°æ®é‡è¾ƒå°" in msg: risk_factors.add("æ•°æ®é‡ä¸è¶³"); recommendations.add("è·å–æ›´å¤šæ•°æ®"); recommendations.add("æ•°æ®å¢å¼º"); recommendations.add("ç®€å•æ¨¡å‹"); recommendations.add("äº¤å‰éªŒè¯")
            if "ç±»åˆ«ä¸å¹³è¡¡" in msg: risk_factors.add("ç±»åˆ«ä¸å¹³è¡¡"); recommendations.add("æ•°æ®å¹³è¡¡"); recommendations.add("è°ƒæ•´ç±»åˆ«æƒé‡"); recommendations.add("ä½¿ç”¨ä¸å¹³è¡¡æŒ‡æ ‡")
            if "ç¼ºå¤±å€¼" in msg: risk_factors.add("æ•°æ®ç¼ºå¤±"); recommendations.add("ç¼ºå¤±å€¼å¤„ç†")
            if "æ–¹å·®è¾ƒä½" in msg: risk_factors.add("ä½ä¿¡æ¯é‡ç‰¹å¾"); recommendations.add("ç§»é™¤ä½æ–¹å·®ç‰¹å¾")
            if "é«˜ç›¸å…³æ€§" in msg: risk_factors.add("ç‰¹å¾å…±çº¿æ€§"); recommendations.add("ç§»é™¤ç›¸å…³ç‰¹å¾"); recommendations.add("PCA"); recommendations.add("æ­£åˆ™åŒ–"); recommendations.add("æ ‘æ¨¡å‹")
            if "æ˜æ˜¾åæ€" in msg: risk_factors.add("ç‰¹å¾åˆ†å¸ƒåæ–œ"); recommendations.add("æ•°æ®å˜æ¢"); recommendations.add("ä½¿ç”¨å¯¹åˆ†å¸ƒä¸æ•æ„Ÿæ¨¡å‹")
            if "ç¦»ç¾¤ç‚¹" in msg: risk_factors.add("æ½œåœ¨ç¦»ç¾¤ç‚¹"); recommendations.add("ç¦»ç¾¤ç‚¹å¤„ç†"); recommendations.add("ä½¿ç”¨é²æ£’æ¨¡å‹")
            if "æ•°æ®æ³„éœ²" in msg: risk_factors.add("æ•°æ®æ³„éœ²"); recommendations.add("ç§»é™¤æ³„éœ²ç‰¹å¾ï¼")
            if "åŸºæ•°è¿‡é«˜" in msg: risk_factors.add("é«˜åŸºæ•°åˆ†ç±»ç‰¹å¾"); recommendations.add("åˆé€‚ç¼–ç /é™åŸºæ•°")
            if "ç¨€æœ‰ç±»åˆ«" in msg: risk_factors.add("ç¨€æœ‰åˆ†ç±»ç±»åˆ«"); recommendations.add("åˆå¹¶ç±»åˆ«")

    # æ€»ç»“
    final_findings = []
    if risk_factors:
        final_findings.append({'type': 'warning', 'message': f"æ½œåœ¨æ³›åŒ–é£é™©å› ç´ : {'; '.join(sorted(list(risk_factors)))}ã€‚"})
        final_findings.append({'type': 'recommendation', 'message': f"å»ºè®®æªæ–½: {'; '.join(sorted(list(recommendations)))}ã€‚"})
    else:
        final_findings.append({'type': 'info', 'message': "åˆæ­¥è¯„ä¼°æœªå‘ç°æ˜æ˜¾å½±å“æ³›åŒ–èƒ½åŠ›çš„é£é™©ã€‚"})
    return final_findings


# --- ä¸»è¦è¯„ä¼°é€»è¾‘å‡½æ•° (è°ƒç”¨ä¸Šé¢çš„è¾…åŠ©å‡½æ•°) ---
def evaluate_data(df, target_col, feature_cols, task_type):
    """
    æ‰§è¡Œå„ç§æ•°æ®æ£€æŸ¥å¹¶è¿”å›ç»“æœå’Œå¯è§†åŒ–å›¾è¡¨ã€‚
    """
    all_findings = []
    all_visualizations = []

    # --- 1. è¾“å…¥éªŒè¯ ---
    if df is None or df.empty: all_findings.append({'type': 'error', 'message': 'æ•°æ®ä¸ºç©ºã€‚'}); return all_findings, all_visualizations
    if not target_col or target_col not in df.columns: all_findings.append({'type': 'error', 'message': 'æœªé€‰æœ‰æ•ˆç›®æ ‡åˆ—ã€‚'}); return all_findings, all_visualizations
    if not feature_cols or not all(col in df.columns for col in feature_cols): all_findings.append({'type': 'error', 'message': 'æœªé€‰æœ‰æ•ˆç‰¹å¾åˆ—ã€‚'}); return all_findings, all_visualizations

    # --- 2. æ•°æ®å‡†å¤‡ ---
    try:
        y = df[target_col].copy()
        X = df[feature_cols].copy()
        n_samples_orig, n_features = X.shape
        all_findings.append({'type': 'info', 'message': f"åŸå§‹æ•°æ®ç»´åº¦: {n_samples_orig} æ ·æœ¬, {n_features} ç‰¹å¾ã€‚"})

        # ç‰¹å¾ç±»å‹è½¬æ¢
        for col in X.columns:
             if X[col].dtype == 'object': X[col] = pd.to_numeric(X[col], errors='ignore')

        # ç›®æ ‡å˜é‡å¤„ç†
        if y.dtype == 'object':
            y_numeric = pd.to_numeric(y, errors='coerce')
            if not y_numeric.isnull().all(): y = y_numeric
            elif task_type == 'Regression': all_findings.append({'type': 'error', 'message': f'å›å½’ç›®æ ‡åˆ— {target_col} å«éæ•°å€¼ã€‚'}); return all_findings, all_visualizations

        # ç§»é™¤ç›®æ ‡å˜é‡ä¸º NaN çš„è¡Œ
        nan_indices = y[y.isnull()].index
        if not nan_indices.empty:
            num_nan_y = len(nan_indices)
            all_findings.append({'type': 'warning', 'message': f'ç›®æ ‡åˆ—å‘ç° {num_nan_y} ä¸ªç¼ºå¤±å€¼ï¼Œå·²ç§»é™¤å¯¹åº”æ ·æœ¬ã€‚'})
            X = X.drop(index=nan_indices)
            y = y.drop(index=nan_indices)
            if X.empty: all_findings.append({'type': 'error', 'message': 'ç§»é™¤ç›®æ ‡åˆ—ç¼ºå¤±å€¼åæ•°æ®ä¸ºç©ºã€‚'}); return all_findings, all_visualizations

        n_samples_clean, n_features = X.shape # æ¸…ç†åçš„æ ·æœ¬æ•°

    except KeyError as ke: all_findings.append({'type': 'error', 'message': f'æ•°æ®å‡†å¤‡å‡ºé”™ï¼šæ‰¾ä¸åˆ°åˆ— {ke}ã€‚'}); return all_findings, all_visualizations
    except Exception as e: all_findings.append({'type': 'error', 'message': f'å‡†å¤‡æ•°æ®æ—¶å‡ºé”™: {e}'}); return all_findings, all_visualizations

    if y is None or y.empty: all_findings.append({'type': 'error', 'message': 'ç›®æ ‡åˆ—æ— æ•ˆæˆ–ä¸ºç©ºã€‚'}); return all_findings, all_visualizations

    # --- 3. æ‰§è¡Œæ£€æŸ¥ ---
    all_findings.append({'type': 'header', 'message': 'æ•°æ®è´¨é‡æ£€æŸ¥'})
    all_findings.extend(check_data_size(df))
    all_findings.extend(check_missing_values(X))

    all_findings.append({'type': 'header', 'message': 'ç‰¹å¾ (X) åˆ†æ'})
    all_findings.extend(check_feature_variance(X))
    dist_findings, dist_visualizations = check_feature_distribution(X); all_findings.extend(dist_findings); all_visualizations.extend(dist_visualizations)
    cat_findings, cat_visualizations = check_categorical_features(X); all_findings.extend(cat_findings); all_visualizations.extend(cat_visualizations)
    all_findings.extend(check_outliers_summary(X))
    corr_findings, corr_visualizations = check_feature_correlation(X); all_findings.extend(corr_findings); all_visualizations.extend(corr_visualizations)

    all_findings.append({'type': 'header', 'message': 'ç›®æ ‡å˜é‡ (Y) åˆ†æ'})
    class_counts = None
    if task_type == 'Classification':
        balance_findings, class_counts = check_class_balance(y, task_type); all_findings.extend(balance_findings)
    target_findings, target_visualizations = analyze_target_variable(y, task_type); all_findings.extend(target_findings); all_visualizations.extend(target_visualizations)

    all_findings.append({'type': 'header', 'message': 'æ½œåœ¨æ•°æ®æ³„éœ²æ£€æŸ¥'})
    all_findings.extend(check_data_leakage_heuristic(X, y, task_type))

    all_findings.append({'type': 'header', 'message': 'æ³›åŒ–èƒ½åŠ›è¯„ä¼°ä¸å»ºæ¨¡å»ºè®®'})
    all_findings.extend(assess_generalization_risk(n_samples_clean, n_features, all_findings))

    print(f"è¯„ä¼°å®Œæˆï¼Œå…± {len(all_findings)} æ¡å‘ç°ï¼Œ{len(all_visualizations)} ä¸ªå¯è§†åŒ–å›¾è¡¨ã€‚")
    return all_findings, all_visualizations


# ==============================================================
#               Streamlit UI å‡½æ•° (ç”± app.py è°ƒç”¨)
# ==============================================================
def show_data_evaluator_page():
    """æ˜¾ç¤ºæ•°æ®è¯„ä¼°é¡µé¢çš„ Streamlit UI å…ƒç´ """
    st.title("ğŸ“ æ•°æ®è¯„ä¼°ä¸å»ºæ¨¡å»ºè®®")
    st.markdown("ä¸Šä¼ æ•°æ®ï¼Œé€‰æ‹©ä»»åŠ¡ç±»å‹å’Œç‰¹å¾ï¼Œç³»ç»Ÿå°†åˆ†ææ•°æ®è´¨é‡ã€ç‰¹å¾åˆ†å¸ƒã€ç›®æ ‡å˜é‡ç‰¹æ€§ï¼Œå¹¶è¯„ä¼°å…¶å¯¹æœºå™¨å­¦ä¹ å»ºæ¨¡ï¼ˆåˆ†ç±»/å›å½’ï¼‰çš„é€‚ç”¨æ€§åŠæ½œåœ¨é£é™©ã€‚")
    st.markdown("---")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€ (ç‰¹å®šäºæ­¤æ¨¡å—)
    if 'de_data' not in st.session_state: st.session_state.de_data = None
    if 'de_task_type' not in st.session_state: st.session_state.de_task_type = 'Classification'
    if 'de_target_col' not in st.session_state: st.session_state.de_target_col = None
    if 'de_feature_cols' not in st.session_state: st.session_state.de_feature_cols = []
    if 'de_evaluation_results' not in st.session_state: st.session_state.de_evaluation_results = None
    if 'de_visualizations' not in st.session_state: st.session_state.de_visualizations = []

    # --- 1. æ•°æ®ä¸Šä¼  ---
    st.header("1. ä¸Šä¼ æ•°æ®")
    uploaded_file_eval = st.file_uploader("é€‰æ‹© CSV æˆ– Excel æ–‡ä»¶è¿›è¡Œè¯„ä¼°", type=["csv", "xlsx", "xls"], key="de_uploader")

    if uploaded_file_eval:
        if st.button("åŠ è½½è¯„ä¼°æ•°æ®", key="de_load_btn"):
            with st.spinner("åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®..."):
                try:
                    data = pd.read_csv(uploaded_file_eval) if uploaded_file_eval.name.lower().endswith('.csv') else pd.read_excel(uploaded_file_eval)
                    data.dropna(axis=1, how='all', inplace=True)
                    if data.empty:
                        st.error("ä¸Šä¼ çš„æ–‡ä»¶ä¸ºç©ºæˆ–å¤„ç†åä¸ºç©ºã€‚")
                        st.session_state.de_data = None
                    else:
                        st.session_state.de_data = data
                        st.session_state.de_target_col = None
                        st.session_state.de_feature_cols = []
                        st.session_state.de_evaluation_results = None
                        st.session_state.de_visualizations = []
                        st.success(f"æˆåŠŸåŠ è½½æ–‡ä»¶: {uploaded_file_eval.name} ({data.shape[0]}è¡Œ, {data.shape[1]}åˆ—)")
                except Exception as e:
                    st.error(f"åŠ è½½æ•°æ®å‡ºé”™: {e}")
                    st.session_state.de_data = None

    # --- 2. é…ç½®ä¸è¿è¡Œè¯„ä¼° ---
    if st.session_state.de_data is not None:
        df = st.session_state.de_data
        st.dataframe(df.head())
        st.markdown("---")
        st.header("2. é…ç½®è¯„ä¼°å‚æ•°")

        col1, col2 = st.columns(2)
        with col1:
             st.session_state.de_task_type = st.radio("é€‰æ‹©ç›®æ ‡ä»»åŠ¡ç±»å‹", ['Classification', 'Regression'], key='de_task_radio', horizontal=True, help="é€‰æ‹©æ‚¨æœ€ç»ˆæƒ³ç”¨è¿™ä¸ªæ•°æ®è¿›è¡Œçš„æœºå™¨å­¦ä¹ ä»»åŠ¡ç±»å‹ã€‚")
             all_cols = df.columns.tolist()
             target_options = [None] + all_cols
             current_target_index = 0
             if st.session_state.de_target_col in all_cols:
                  try: current_target_index = target_options.index(st.session_state.de_target_col)
                  except ValueError: st.session_state.de_target_col = None
             st.session_state.de_target_col = st.selectbox("é€‰æ‹©ç›®æ ‡å˜é‡ (Y)", target_options, index=current_target_index, key='de_target_select', help="æ‚¨æƒ³è¦é¢„æµ‹æˆ–åˆ†ç±»çš„åˆ—ã€‚")

        with col2:
             if st.session_state.de_target_col:
                 feature_options = [col for col in all_cols if col != st.session_state.de_target_col]
                 default_features = [col for col in st.session_state.de_feature_cols if col in feature_options]
                 if not default_features and feature_options:
                      default_features = feature_options
                 st.session_state.de_feature_cols = st.multiselect("é€‰æ‹©ç‰¹å¾å˜é‡ (X)", feature_options, default=default_features, key='de_feature_select', help="ç”¨äºé¢„æµ‹ç›®æ ‡å˜é‡çš„åˆ—ã€‚")
             else:
                 st.multiselect("é€‰æ‹©ç‰¹å¾å˜é‡ (X)", [], disabled=True, key='de_feature_select')
                 st.warning("è¯·å…ˆé€‰æ‹©ç›®æ ‡å˜é‡")

        st.markdown("---")
        # è¯„ä¼°æŒ‰é’®
        can_evaluate = st.session_state.de_target_col and st.session_state.de_feature_cols
        if st.button("ğŸ“ˆ å¼€å§‹æ•°æ®è¯„ä¼°", key="de_run_btn", disabled=not can_evaluate, type="primary", use_container_width=True):
             if can_evaluate:
                 st.session_state.de_evaluation_results = None
                 st.session_state.de_visualizations = []
                 with st.spinner("æ­£åœ¨æ‰§è¡Œæ•°æ®è¯„ä¼°åˆ†æ..."):
                    time.sleep(0.5)
                    try:
                        findings, visualizations = evaluate_data(
                             df,
                             st.session_state.de_target_col,
                             st.session_state.de_feature_cols,
                             st.session_state.de_task_type
                        )
                        st.session_state.de_evaluation_results = findings
                        st.session_state.de_visualizations = visualizations
                        st.success("æ•°æ®è¯„ä¼°å®Œæˆï¼ç»“æœå¦‚ä¸‹æ‰€ç¤ºã€‚")
                    except Exception as eval_e:
                         st.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {eval_e}")
                         st.code(traceback.format_exc())

    # --- 3. æ˜¾ç¤ºè¯„ä¼°ç»“æœ ---
    if st.session_state.de_evaluation_results:
        st.markdown("---")
        st.header("3. è¯„ä¼°ç»“æœä¸å»ºè®®")

        current_expander = None # ç”¨äºè¿½è¸ªå½“å‰æ´»åŠ¨çš„ expander

        # ä½¿ç”¨ Expander åˆ†ç»„æ˜¾ç¤ºç»“æœ
        for finding in st.session_state.de_evaluation_results:
            msg_type = finding.get('type', 'info')
            message = finding.get('message', '')

            if msg_type == 'header':
                # åˆ›å»ºæ–°çš„ Expander
                current_expander = st.expander(f"**{message}**", expanded=True)
            elif current_expander: # å¦‚æœå½“å‰æœ‰æ´»åŠ¨çš„ expander
                 # åœ¨å½“å‰ expander å†…æ˜¾ç¤ºå†…å®¹
                 with current_expander:
                     # --- ä¿®æ­£ SyntaxError çš„åœ°æ–¹ ---
                     message_html = message.replace('\n', '<br>')  # å…ˆè¿›è¡Œæ›¿æ¢æ“ä½œ
                     if msg_type == 'info':
                         st.markdown(f"<div class='eval-finding info'>â„¹ï¸ {message_html}</div>",
                                     unsafe_allow_html=True)  # ä½¿ç”¨æ–°å˜é‡
                     elif msg_type == 'warning':
                         st.markdown(f"<div class='eval-finding warning'>âš ï¸ **è­¦å‘Š:** {message_html}</div>",
                                     unsafe_allow_html=True)  # ä½¿ç”¨æ–°å˜é‡
                     elif msg_type == 'error':
                         st.markdown(f"<div class='eval-finding error'>âŒ **é”™è¯¯:** {message_html}</div>",
                                     unsafe_allow_html=True)  # ä½¿ç”¨æ–°å˜é‡
                     elif msg_type == 'recommendation':
                         st.markdown(f"<div class='eval-finding recommendation'>ğŸ’¡ **å»ºè®®:** {message_html}</div>",
                                     unsafe_allow_html=True)  # ä½¿ç”¨æ–°å˜é‡
                     else:
                         st.markdown(f"â¡ï¸ {message_html}")  # ä½¿ç”¨æ–°å˜é‡ (å¦‚æœé»˜è®¤ä¹Ÿéœ€è¦æ¢è¡Œ)
                     # --- ç»“æŸä¿®æ­£ ---
            else: # å¤‡ç”¨ï¼šå¦‚æœæ¶ˆæ¯åœ¨ä»»ä½• header ä¹‹å‰å‡ºç° (ç†è®ºä¸Šä¸åº”å‘ç”Ÿï¼Œå› ä¸ºç¬¬ä¸€ä¸ª finding åº”è¯¥æ˜¯ header)
                  if msg_type == 'info': st.info(f"â„¹ï¸ {message}")
                  elif msg_type == 'warning': st.warning(f"âš ï¸ **è­¦å‘Š:** {message}")
                  elif msg_type == 'error': st.error(f"âŒ **é”™è¯¯:** {message}")
                  elif msg_type == 'recommendation': st.success(f"ğŸ’¡ **å»ºè®®:** {message}")
                  else: st.markdown(f"â¡ï¸ {message}")


        # æ˜¾ç¤ºæ‰€æœ‰å¯è§†åŒ–å›¾è¡¨
        if st.session_state.de_visualizations:
             st.markdown("---")
             st.header("ç›¸å…³å¯è§†åŒ–å›¾è¡¨")
             num_viz = len(st.session_state.de_visualizations)
             cols_per_row = 2
             viz_cols = st.columns(cols_per_row)
             for i, fig in enumerate(st.session_state.de_visualizations):
                  col_index = i % cols_per_row
                  with viz_cols[col_index]:
                       try:
                            plot_title = f"å›¾è¡¨ {i+1}"
                            if fig.axes and fig.axes[0].get_title(): plot_title = fig.axes[0].get_title()
                            # st.write(plot_title) # Optional: display title above plot
                            st.pyplot(fig)
                            plt.close(fig)
                       except Exception as viz_e:
                            st.warning(f"æ˜¾ç¤ºå›¾è¡¨ {i+1} ('{plot_title}') æ—¶å‡ºé”™: {viz_e}")
                            try: plt.close(fig)
                            except: pass

# --- å¯é€‰: ç”¨äºç‹¬ç«‹æµ‹è¯• ---
# if __name__ == "__main__":
#     st.set_page_config(layout="wide", page_title="æ•°æ®è¯„ä¼°æ¨¡å—æµ‹è¯•")
#     show_data_evaluator_page()